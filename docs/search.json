[
  {
    "objectID": "Textbook/TestingTesting.html",
    "href": "Textbook/TestingTesting.html",
    "title": "Testing…Testing…1…2…3",
    "section": "",
    "text": "Introduction\nThis type of file is called a “markdown” file. Markdown is like Microsoft Word but much more powerful. This file is made specific for R, and has a file type .Rmd meaning “R Markdown”.\nYou will become very familiar with these files throughout the semester. For now, it’s only necessary to download this file, save it in a sensible folder on your computer or OneDrive, and “run” it.\nClicking on the “knit” button above (depending on your computer setup it may show up shows up “Render”) will create an .html document that should open up in your default browser.\nAs you see, we can make section headers and type regular text. But the power of .Rmd files is that we can also code inside these documents and present our output directly within the document.\nClick “Knit” or “Render” to test to see if your software is set up.\nWhen coding, we have to tell the computer when we’re writing text and when we expect it to compile code. Below is an example of a “code chunk” that creates a made up graph.\nYou do not have to understand this right now. We’re only testing that R and RStudio are set up correctly.\n\n\nCode\nx &lt;- seq(0,10, length = 100)\ny &lt;- 2+exp(x)\n\nplot(x,y, type = \"l\", lwd=2, col=\"darkblue\", main = \"Exponential Function\")"
  },
  {
    "objectID": "Textbook/Spread.html",
    "href": "Textbook/Spread.html",
    "title": "Describing Quantitative Data (Spread)",
    "section": "",
    "text": "By the end of this lesson, you should be able to:\n\nCalculate a percentile from data\nInterpret a percentile\nCalculate the standard deviation from data\nInterpret the standard deviation\nCalculate the five-number summary using software\nInterpret the five-number summary\nCreate a box plot using software\nDetermine the five-number summary visually from a box plot"
  },
  {
    "objectID": "Textbook/Spread.html#lesson-outcomes",
    "href": "Textbook/Spread.html#lesson-outcomes",
    "title": "Describing Quantitative Data (Spread)",
    "section": "",
    "text": "By the end of this lesson, you should be able to:\n\nCalculate a percentile from data\nInterpret a percentile\nCalculate the standard deviation from data\nInterpret the standard deviation\nCalculate the five-number summary using software\nInterpret the five-number summary\nCreate a box plot using software\nDetermine the five-number summary visually from a box plot"
  },
  {
    "objectID": "Textbook/Spread.html#spread-of-a-distribution",
    "href": "Textbook/Spread.html#spread-of-a-distribution",
    "title": "Describing Quantitative Data (Spread)",
    "section": "Spread of a Distribution",
    "text": "Spread of a Distribution\nIn the previous lesson, we introduced two important characteristics of a distribution: the shape and the center. In this section, you will discover ways to summarize the spread of a distribution of data. The spread of a distribution of data describes how far the observations tend to be from each other. There are many ways to describe the spread of a distribution, but one of the most popular measurements of spread is called the “standard deviation.” \n\nStandard Deviation and Variance\nThis activity introduces two measures of spread: the standard deviation and the variance.\n\nBird Flu Fever \n\nAvian Influenza A H5N1, commonly called the bird flu, is a deadly illness that is currently only passed to humans from infected birds. This illness is particularly dangerous because at some point it is likely to mutate to allow human-to-human transmission. Health officials worldwide are preparing for the possibility of a bird flu pandemic.\n\nDr. K. Y. Yuen led a team of researchers who reported the body temperatures of people admitted to Chinese hospitals with confirmed cases of Avian Influenza. Their research team collected data on the body temperature at the time that people with the bird flu were admitted to the hospital. In the article, they reported on two groups of people, those with relatively uncomplicated cases of the bird flu and those with severe cases.\n\nThe table below presents the data representative of the body temperatures for the two groups of bird flu patients:\n\n\n\nBody Temperature\nCase Type\n\n\n\n\n38.1\nSimple\n\n\n38.3\nSimple\n\n\n38.4\nSimple\n\n\n39.5\nSimple\n\n\n39.7\nSimple\n\n\n39.1\nSevere\n\n\n39.5\nSevere\n\n\n38.9\nSevere\n\n\n39.2\nSevere\n\n\n39.9\nSevere\n\n\n39.7\nSevere\n\n\n39.0\nSevere\n\n\n\n\nLet us focus on the relatively uncomplicated cases. Creating a histogram of such a small dataset does not provide much benefit. With only a handful of values, there is not much shape to the distribution.\nWe can, however, use numerical summaries to give an indication of the center of the distribution.\n\nAnswer the following questions:\n\n\n\nWhat is the median of the body temperatures for the simple cases? \n\n\n\nSolution\n\n\nThe median body temperature for the simple cases is 38.4 degrees Centigrade.\n\n\n\nWhat is the mean of the body temperatures for the simple cases? \n\n\n\nSolution\n\n\nThe mean body temperature for the simple cases is 38.8 degrees Centigrade.\n\n\n\n\nWe will use these data to investigate some measures of the spread in a data set.\nThere is relatively little difference in the temperatures of the uncomplicated patients. The lowest is \\(38.1 ^\\circ \\text{C}\\), while the highest temperature is \\(39.7 ^\\circ \\text{C}\\).\nThe standard deviation is a measure of the spread in the distribution. If the data tend to be close together, then the standard deviation is relatively small. If the data tend to be more spread out, then the standard deviation is relatively large.\nThe standard deviation of the body temperatures is \\(0.742 ^\\circ \\text{C}\\). This number contains information from all the patients. If the patients’ temperatures had been more diverse, the standard deviation would be larger. If the patients’ temperatures were more uniform (i.e. closer together), then the standard deviation would have been smaller. If all the patients somehow had the same temperature, then the standard deviation would be zero.\nWe are working with a sample. To be explicit, we call \\(0.742 ^\\circ \\text{C}\\) the sample standard deviation. The symbol for the sample standard deviation is \\(s\\). This is a statistic. The parameter representing the population standard deviation is \\(\\sigma\\) (pronounced /SIG-ma/). In practice, we rarely know the value of the population standard deviation, so we use the sample standard deviation \\(s\\) as an approximation for the unknown population standard deviation \\(\\sigma\\).\nAt this point, you probably do not have much intuition regarding the standard deviation. We will use this statistic frequently. By the end of the semester you can expect to become very comfortable with this idea. For now, all you need to know is that if two variables are measured on the same scale, the variable with values that are further apart will have the larger standard deviation.\n\nR Instructions\n\n\n  To calculate the sample standard deviation in R, follow these steps:   \n\ndata &lt;- c(38.1,38.3,38.4,39.5,39.7,39.1,39.5,38.9,39.2,39.9,39.7,39.0)\n\nsd(data)\n\n[1] 0.5930788\n\n\n\n\n\n\nRounding: As a general rule, when reporting your answers in this class, round to three decimal places unless otherwise specified.\n\n \n\nCalculating the Standard Deviation by Hand\nHow is the standard deviation computed? Where does this “magic” number come from? How does one number include the information about the spread of all the points?\nIt is a little tedious to compute the standard deviation by hand. You will usually compute standard deviation with a computer. However, the process is very instructive and will help you understand conceptually what the statistic represents. As you work through the following steps, please remember the goal is to find a measure of the spread in a data set. We want one number that describes how spread out the data are.\nFirst, observe the number line below, where each x represents the temperature of a patient with a relatively uncomplicated case of bird flu. As mentioned earlier, there is not a huge spread in the temperatures.\n\nOn your sketch of the number line, we draw a vertical line at 38.8 degrees, the sample mean. Now, draw horizontal lines from the mean to each of your \\(\\times\\)’s. These horizontal line segments represent the spread of the data about the mean. Your plot should look something like this:\n\nThe length of each of the line segments represents how far each observation is from the mean. If the data are close together, these lines will be fairly short. If the distribution has a large spread, the line segments will be longer. The standard deviation is a measure of how long these lines are, as a whole.\nThe distance between the mean and an observation is referred to as a deviation. In other words, deviations are the lengths of the line segments drawn in the image above.\n\\[\n\\begin{array}{1cl}\n\\text{Deviation} & = & \\text{Value} - \\text{Mean} \\\\\n\\text{Deviation} & = & x - \\bar x\n\\end{array}\n\\]\nIf the observed value is greater than the mean, the deviation is positive. If the value is less than the mean, the deviation is negative.\nThe standard deviation is a complicated sort of average of the deviations. Making a table like the one below will help you keep track of your calculations. Please participate fully in this exercise. Writing your answers at each step and developing a table as instructed will greatly enhance the learning experience. By following these steps, you will be able to compute the standard deviation by hand, and more importantly, understand what it is telling you.\nStep 01: The first step in computing the standard deviation by hand is to create a table, like the following. Enter the observed data in the first column.\n\n\n\n\n\n\nObservation (\\(x\\))\n\n\n\n\n\n\nDeviation from the Mean (\\(x-\\bar x\\))\n\n\n\n\n\n\n\n\n\n\n\\(38.1\\)\n\n\n\n\n\\(38.1-38.8=-0.7\\)\n\n\n\n\n\n\n\\(38.3\\)\n\n\n\n\n\n\n\n\n\\(38.4\\)\n\n\n\n\n\n\n\n\n\\(39.5\\)\n\n\n\n\n\n\n\n\n\\(39.7\\)\n\n\n\n\n\n\n\n\n\\(\\bar x = 38.8\\)\n\n\n\n\n\n\n\n\nStep 02: The second column of the table contains the deviations from the mean. Complete column 2 of the table above.\nCheck Results for Step 2\n\n\n\n\n\n\nObservation (\\(x\\))\n\n\n\n\nDeviation from the Mean (\\(x-\\bar x\\))\n\n\n\n\n\n\n\n\n\\(38.1\\)\n\n\n\n\n\\(38.1-38.8=-0.7\\)\n\n\n\n\n\n\n\\(38.3\\)\n\n\n\n\n\\(38.3-38.8=-0.5\\)\n\n\n\n\n\n\n\\(38.4\\)\n\n\n\n\n\\(38.4-38.8=-0.4\\)\n\n\n\n\n\n\n\\(39.5\\)\n\n\n\n\n\\(39.5-38.8=0.7\\)\n\n\n\n\n\n\n\\(39.7\\)\n\n\n\n\n\\(39.7-38.8=0.9\\)\n\n\n\n\n\n\n\\(\\bar x = 38.8\\)\n\n\n\n\n\n\n\n\n\n\nAnswer the following questions:\n\n\n\nHow could we use this table to find the “typical” distance from each point to the mean? Think carefully about this, and then write down your answer before continuing.\n\n\n\nSolution\n\n\nYou may have suggested that we compute the mean of these values. This seems like a good idea. If we compute the mean, it will tell us the average deviation from the mean. \n\n\n9b. Compute the mean of Column 2. What do you get?\n\n\nSolution\n\n\nYou should have found that the mean of the deviations is zero. This is true for every data set. If you add up the deviations from the mean, the positive values will cancel with the negative values. The sum of the deviations from the mean will be zero, so the mean also must equal zero.\nThe good news is that you can use this fact to check if you are on the right track. If the deviations from the mean do not add up to zero, then you have made a mistake in the calculations. The bad news is that the deviations always add up to 0, making it look like the distance from the data to the mean is 0. Nonsense!\nThe mean of the deviations from the mean cannot be used to find a measure of the spread in a data set, but it does provide a guidepost that shows we are on the right track. We must find another way to estimate the spread of a data set.\n\n\n\nWe need a way to work with the negative deviations from the mean, so they do not cancel with the positive ones. What could we do? (Choose one of the four options below.)\n\n\n\nOption 1: Take the absolute value of the deviations\n\n\nThis is an excellent suggestion. This is probably one of the first things statisticians used to estimate the spread in the data.\nIf we take the absolute value of the deviations, then all the values are positive. By taking the mean of these numbers, we do get a measure of spread. This quantity is called the mean absolute deviation (MAD).\nThere is good news and bad news. The good news is, you discovered a way to estimate the spread in a data set. (In fact, the MAD is used as one estimate of the volatility of stocks.) The bad news is that the MAD does not have good theoretical properties. A proof of this claim requires calculus, and so will not be discussed here. For most applications, there is a better choice. Please select another option.\n\n\n\n\nOption 2: Square the deviations\n\n\nIf we square the deviations from the mean, the values that were negative will become positive. This leads to an estimator of the spread that has excellent theoretical properties. This is the best of the four options. You will apply this idea in Step 03.\n\n\n\n\nOption 3: Delete the negative deviations\n\n\nSorry, you can’t make your troubles go away by deleting things you don’t like. Please try again.\n\n\n\n\nOption 4: Do something entirely different\n\n\nYou probably have an ingenious idea. Surprisingly enough, there is a right answer to the question. Please choose a different option.\n\n\n\nPlease do not go on to Step 03 until you have finished this exploration.\n\n\n\n\n“Piled Higher and Deeper” by Jorge Cham  \n\nStep 03: Add a third column to your table. To get the values in this column, square the deviations from the mean that you found in Column 2.\n\n\n\n\n\nObservation \\(x\\)\n\n\n\n\nDeviation from the Mean \\(x-\\bar x\\)\n\n\n\n\nSquared Deviation from the Mean \\(\\left(x-\\bar x\\right)^2\\)\n\n\n\n\n\n\n\n\n\\(38.1\\)\n\n\n\n\n\\(38.1-38.8=-0.7\\)\n\n\n\n\n\n\n\n\n\\(38.3\\)\n\n\n\n\n\\(38.3-38.8=-0.5\\)\n\n\n\n\n\n\n\n\n\\(38.4\\)\n\n\n\n\n\\(38.4-38.8=-0.4\\)\n\n\n\n\n\n\n\n\n\\(39.5\\)\n\n\n\n\n\\(39.5-38.8=0.7\\)\n\n\n\n\n\n\n\n\n\\(39.7\\)\n\n\n\n\n\\(39.7-38.8=0.9\\)\n\n\n\n\n\n\n\n\n\\(\\bar x = 38.8\\)\n\n\n\n\nSum \\(=0\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservation \\(x\\)\n\n\n\n\nDeviation from the Mean \\(x-\\bar x\\)\n\n\n\n\nSquared Deviation from the Mean \\(\\left(x-\\bar x\\right)^2\\)\n\n\n\n\n\n\n\n\n\\(38.1\\)\n\n\n\n\n\\(38.1-38.8=-0.7\\)\n\n\n\n\n\\((-0.7)^2=0.49\\)\n\n\n\n\n\n\n\\(38.3\\)\n\n\n\n\n\\(38.3-38.8=-0.5\\)\n\n\n\n\n\\((-0.5)^2=0.25\\)\n\n\n\n\n\n\n\\(38.4\\)\n\n\n\n\n\\(38.4-38.8=-0.4\\)\n\n\n\n\n\\((-0.4)^2=0.16\\)\n\n\n\n\n\n\n\\(9.5\\)\n\n\n\n\n\\(39.5-38.8=0.7\\)\n\n\n\n\n\\((0.7)^2=0.49\\)\n\n\n\n\n\n\n\\(39.7\\)\n\n\n\n\n\\(39.7-38.8=0.9\\)\n\n\n\n\n\\((0.9)^2=0.81\\)\n\n\n\n\n\n\n\\(\\bar x = 38.8\\)\n\n\n\n\nSum \\(=0\\)\n\n\n\n\n\n\n\n\nStep 04: Now, add up the squared deviations from the mean.\n\n\n\n\n\nObservation \\(x\\)\n\n\n\n\nDeviation from the Mean \\(x-\\bar x\\)\n\n\n\n\nSquared Deviation from the Mean \\(\\left(x-\\bar x\\right)^2\\)\n\n\n\n\n\n\n\n\n\\(38.1\\)\n\n\n\n\n\\(38.1-38.8=-0.7\\)\n\n\n\n\n\\((-0.7)^2=0.49\\)\n\n\n\n\n\n\n\\(38.3\\)\n\n\n\n\n\\(38.3-38.8=-0.5\\)\n\n\n\n\n\\((-0.5)^2=0.25\\)\n\n\n\n\n\n\n\\(38.4\\)\n\n\n\n\n\\(38.4-38.8=-0.4\\)\n\n\n\n\n\\((-0.4)^2=0.16\\)\n\n\n\n\n\n\n\\(39.5\\)\n\n\n\n\n\\(39.5-38.8=0.7\\)\n\n\n\n\n\\((0.7)^2=0.49\\)\n\n\n\n\n\n\n\\(39.7\\)\n\n\n\n\n\\(39.7-38.8=0.9\\)\n\n\n\n\n\\((0.9)^2=0.81\\)\n\n\n\n\n\n\n\\(\\bar x = 38.8\\)\n\n\n\n\nSum \\(=0\\)\n\n\n\n\nSum \\(=2.20\\)\n\n\n\n\n\nThe sum of the squared deviations is 2.20. \n\nAnswer the following questions:\n\n\n\nSuppose that the researchers had collected body temperature data on 500 bird flu patients instead of 5. What would happen to the sum of the squared deviations, if the distribution of the data is the same for the 500 patients as the 5 patients?\n\n\n\nSolution\n\n\nWe would expect the sum of the squared deviations to be a lot larger than it is now. We would be adding squared deviations for 500 observations instead of 5. So, the sum of the squared deviations would be about 100 times larger.\n\nRemember, we are trying to find a measure of the spread of a data set. Our final measure should not be dependent on the sample size. We need to do something else.\n\n\n\nPlease do not go on until you have finished this exercise.\n\n\n\nStep 05: Recall that an average is adding a bunch of things up and dividing by the number of things. Consider taking the average of the squared deviations by adding them up and dividing by the number of deviations.\nUnfortunately, this is what is technically known as a “biased” estimate. We don’t get into what that means in this class, but to correct for the bias, we divide by \\(n-1\\) instead.\nThe number you computed in Step 05 is called the sample variance. It is a measure of the spread in a data set. It has very nice theoretical properties. The variance plays an important role in Statistics. We denote the sample variance by the symbol \\(s^2\\).\nIt can be shown that the sample variance is an unbiased estimator of the true population variance (which is denoted \\(\\sigma^2\\).) This means that the sample variance can be considered a reasonable estimator of the population variance. If the sample size is large, this estimator tends to be very good.\n\n\nResults from Step 5\n\nThe sum of the squared deviations is the sum of the values in Column 3. This sum equals 2.20. We divide the sum of Column 3 (\\(2.20\\)) by \\(n-1=5-1=4\\) to get the sample variance, \\(s^2\\):\n\\[s^2=\\frac{sum}{n-1}=\\frac{2.20}{5-1}=0.55\\]\nThis is the sample variance.\n\n\n\n\n\nObservation \\(x\\)\n\n\n\n\nDeviation from the Mean \\(x-\\bar x\\)\n\n\n\n\nSquared Deviation from the Mean \\(\\left(x-\\bar x\\right)^2\\)\n\n\n\n\n\n\n\n\n\\(38.1\\)\n\n\n\n\n\\(38.1-38.8=-0.7\\)\n\n\n\n\n\\((-0.7)^2=0.49\\)\n\n\n\n\n\n\n\\(38.3\\)\n\n\n\n\n\\(38.3-38.8=-0.5\\)\n\n\n\n\n\\((-0.5)^2=0.25\\)\n\n\n\n\n\n\n\\(38.4\\)\n\n\n\n\n\\(38.4-38.8=-0.4\\)\n\n\n\n\n\\((-0.4)^2=0.16\\)\n\n\n\n\n\n\n\\(39.5\\)\n\n\n\n\n\\(39.5-38.8=0.7\\)\n\n\n\n\n\\((0.7)^2=0.49\\)\n\n\n\n\n\n\n\\(39.7\\)\n\n\n\n\n\\(39.7-38.8=0.9\\)\n\n\n\n\n\\((0.9)^2=0.81\\)\n\n\n\n\n\n\n\\(\\bar x = 38.8\\)\n\n\n\n\nSum \\(=0\\)\n\n\n\n\nSum \\(=2.20\\)\n\n\n\n\n\n\nVariance:\n\n\n\n\n\\(\\displaystyle{s^2=\\frac{sum}{n-1}=\\frac{2.20}{5-1}=0.55}\\)\n\n\n\n\n\n\n\n\n\n\nAnswer the following questions:\n\n\n\nThe temperature data for the bird flu patients are in degrees Centigrade. What are the units of the variance?\n\n\n\nSolution\n\n\nThe data in Column 1 of the table is in degrees Centigrade. The mean also is in degrees Centigrade.\nTo get the numbers in Column 2, we subtracted the mean from each of the values in Column 1.\nWe squared the values in Column 2 to get Column 3. The units for this column are degrees Centigrade squared.\n\nThe sum of the numbers in Column 3 will also be in units of degrees Centigrade squared.\nWhen we divided that sum by \\(n-1\\), we obtained the sample variance. The sample variance has units of degrees Centigrade squared. This is not easily interpretable. It would be much easier to think about it if our measure of spread was in the same units as the data.\n\n\n\nWhat operation can we do to the variance to get a quantity with units degrees Centigrade?\n\n\n\nSolution\n\n\nIf we take the square root of the variance, we get a quantity that has units of degrees Centigrade. This quantity is the standard deviation.\n\n\n\n\nStep 06: Take the square root of the sample variance to get the sample standard deviation.\nThe sample standard deviation is defined as the square root of the sample variance.\n\\[\\text{Sample Standard Deviation} = s = \\sqrt{ s^2 } = \\sqrt{\\strut\\text{Sample Variance}}\\]\nThe standard deviation has the same units as the original observations. We use the standard deviation heavily in statistics.\nThe sample standard deviation (\\(s\\)) is an estimate of the true population standard deviation (\\(\\sigma\\)).\n\nAnswer the following questions:\n\n\n\nWhat is the sample standard deviation, \\(s\\), of the temperatures of the five patients with relatively uncomplicated cases of the bird flu?\n\n\n\nSolution\n\n\nThe sum of the squared deviations is the sum of the values in Column 3. This sum equals 2.20. We divide the sum of Column 3 (\\(2.20\\)) by \\(n-1=5-1=4\\) to get the sample variance, \\(s^2\\):\n\n\n\\(s^2=\\frac{sum}{n-1}=\\frac{2.20}{5-1}=0.55\\)\n\n\nThis is the sample variance.\n\n\n\n\n\n\nObservation \\(x\\)\n\n\n\n\nDeviation from the Mean \\(x-\\bar x\\)\n\n\n\n\nSquared Deviation from the Mean \\(\\left(x-\\bar x\\right)^2\\)\n\n\n\n\n\n\n\n\n\\(38.1\\)\n\n\n\n\n\\(38.1-38.8=-0.7\\)\n\n\n\n\n\\((-0.7)^2=0.49\\)\n\n\n\n\n\n\n\\(38.3\\)\n\n\n\n\n\\(38.3-38.8=-0.5\\)\n\n\n\n\n\\((-0.5)^2=0.25\\)\n\n\n\n\n\n\n\\(38.4\\)\n\n\n\n\n\\(38.4-38.8=-0.4\\)\n\n\n\n\n\\((-0.4)^2=0.16\\)\n\n\n\n\n\n\n\\(39.5\\)\n\n\n\n\n\\(39.5-38.8=0.7\\)\n\n\n\n\n\\((0.7)^2=0.49\\)\n\n\n\n\n\n\n\\(39.7\\)\n\n\n\n\n\\(39.7-38.8=0.9\\)\n\n\n\n\n\\((0.9)^2=0.81\\)\n\n\n\n\n\n\n\\(\\bar x = 38.8\\)\n\n\n\n\nSum \\(=0\\)\n\n\n\n\nSum \\(=2.20\\)\n\n\n\n\n\n\nVariance:\n\n\n\n\n\\(\\displaystyle{s^2=\\frac{sum}{n-1}=\\frac{2.20}{5-1}=0.55}\\)\n\n\n\n\n\n\n\n\nStandard Deviation:\n\n\n\n\n\\(\\displaystyle{s = \\sqrt{s^2}=\\sqrt{0.55} \\approx 0.742}\\)\n\n\n\n\n\n\n\n\nThe sample standard deviation is \\(s = 0.742\\) degrees Centigrade.\nTake a few minutes to verify that you can recreate this table on your own.\n\n\n\n\n\n\nSummary\n\n  Standard Deviation\n  The standard deviation is one number that describes the spread in a set of data. If the data points are close together the standard deviation will be smaller than if they are spread out.\n  At this point, it may be difficult to understand the meaning and usefulness of the standard deviation. For now, it is enough for you to recognize the following points:\n\nThe standard deviation is a measure of how spread out the data are.\nIf the standard deviation is large, then the data are very spread out.\nIf the standard deviation is zero, then all the values are the identical–there is no spread in the data.\nThe standard deviation cannot be negative. \n\n\nVariance\nThe variance is the square of the standard deviation. The sample variance is denoted by the symbol \\(s^2\\). You found the sample standard deviation for patient temperatures of uncomplicated cases of bird in the bird above is \\(s = 0.74162\\). So, the sample variance for this data set is \\(s^2 = 0.74162^2 = 0.550\\). Be aware, if you had squared the rounded value of \\(s^2 = 0.742\\) in the calculation, you would have gotten an answer of 0.551 instead. This would be considered incorrect! \n\nRounding: Use unrounded values in interim calculations. Rounding too early in the process can lead to wrong answers.\n\n \n\nR Instructions\n\n\n  To calculate the sample variance in R:   \n\ndata &lt;- c(38.1,38.3,38.4,39.5,39.7,39.1,39.5,38.9,39.2,39.9,39.7,39.0)\n\nvar(data)\n\n[1] 0.3517424\n\n\n\n\n\nThe standard deviation and variance are two commonly used measures of the spread in a data set. Why is there more than one measure of the spread? The standard deviation and the variance each have their own pros and cons.\nThe variance has excellent theoretical properties. It is an unbiased estimator of the true population variance. That means that if many, many samples of \\(n\\) observations were drawn, the variances computed for all the samples would be centered nicely around the true population variance, \\(\\sigma^2\\). Because of these benefits, the variance is regularly used in higher-level statistics applications. One drawback of the variance is that the units for the variance are the square of the units for the original data. In the bird flu example, the body temperatures were measured in degrees Centigrade. So, the variance will have units of degrees Centigrade squared \\((^\\circ \\text{C})^2\\). What does degrees Centigrade squared mean? How do you interpret this? It doesn’t make any sense. This is one of the major drawbacks of the sample variance.\nBecause we take the square root of the variance to get the standard deviation, the standard deviation is in the same units as the original data. This is a great advantage, and is one of the reasons that the standard deviation is commonly used to describe the spread of data.\n\nAnswer the following questions:\n\n\nEnter the patient temperature data for the severe cases of bird flu into R Then use R to calculate the numerical summaries you have learned so far. As a reminder, the temperatures of patients with a severe case of bird flu are:\n\n39.1, 39.5, 38.9, 39.2, 39.9, 39.7, 39\n\n\nWhat is the mean, median, standard deviation and variance of the sample?\n\n\n\nSolution\n\n\nbird_flu &lt;- c(39.1, 39.5, 38.9, 39.2, 39.9, 39.7, 39)\n\nmean(bird_flu)\n\n[1] 39.32857\n\nmedian(bird_flu)\n\n[1] 39.2\n\nsd(bird_flu)\n\n[1] 0.377334\n\nvar(bird_flu)\n\n[1] 0.142381\n\n\n\n\nFor the next two questions, consider the histograms below comparing weight (in kilograms) of men (top histogram) to elephant seals (bottom histogram).\n\n\nWeight of Men Compared to Weight of Seals \n\n\nBased on the histograms, who has a greater sample mean weight, men or elephant seals?\n\n\n\nSolution\n\n\nThe mean is a measure of the center of a distribution. The mean weight of the men is less than the mean weight of the seals. We can see this because the bulk of the data in the histogram for the men’s weight is to the left of the seals’. The center of the distribution of elephant seals is about 195 kg. The center of the distribution of men’s weight is located below 100 kg on the number line.\n\n\n\n\nBased on the histograms, do the weights of men or elephant seals have a larger sample standard deviation?\n\n\n\nSolution\n\n\nStandard deviation is a measure of spread. You will note that the weights of the seals are more spread out than the weights of the men. Therefore, we conclude that the sample standard deviation of elephant seal weights is larger than the sample standard deviation of men’s weights.\n\n\n \n\n\nReview of Parameters and Statistics\nWe have now learned some statistics that can be used to estimate population parameters. For example, we use \\(\\bar x\\) to estimate the population mean \\(\\mu\\). The sample statistics \\(s\\) estimates the true population standard deviation \\(\\sigma\\). The following table summarizes what we have done so far:\n\n\n\n\n\n\n\n\nSample Statistic\n\n\n\n\nPopulation Parameter\n\n\n\n\n\n\n\n\nMean\n\n\n\n\n\\(\\bar x\\)\n\n\n\n\n\\(\\mu\\)\n\n\n\n\n\n\nStandard Deviation\n\n\n\n\n\\(s\\)\n\n\n\n\n\\(\\sigma\\)\n\n\n\n\n\n\nVariance\n\n\n\n\n\\(s^2\\)\n\n\n\n\n\\(\\sigma^2\\)\n\n\n\n\n\n\n\\(\\vdots\\)\n\n\n\n\n\\(\\vdots\\)\n\n\n\n\n\\(\\vdots\\)\n\n\n\n\n\n\nUnless otherwise specified, we will always use Rto find the sample variance and sample mean. In each case, the sample statistic estimates the population parameter. The ellipses \\(\\vdots\\) in this table hint that we will add rows in the future.\n\nOptional Reading: Formulas for \\(s\\) and \\(s^2\\) (Hidden)\n\n\nClick here if you love math\n\nFormulas\nFor those who like formulas, the equation for the sample variance and sample standard deviation are given here.\nSample variance:\n\\[\\displaystyle{ s^2=\\frac{\\sum\\limits_{i=1}^n (x_i-\\bar x)^2}{n-1} }\\]\nSample standard deviation:\n\\[\\displaystyle{ s=\\sqrt{s^2}=\\sqrt{\\frac{\\sum\\limits_{i=1}^n (x_i-\\bar x)^2}{n-1}} }\\]\nwhere \\(x_i\\) is the \\(i^{th}\\) observed data value, and \\(i=1, 2, \\ldots, n\\).\nUnless otherwise specified, we will always use Excel to find the sample variance and sample mean.\nWhy do we divide by \\(n-1\\)?\nWhen computing the standard deviation or the variance, we are finding a value that describes the spread of data values. It is a measure of how far the data are from the mean. Since we do not know the true mean (\\(\\mu\\),) we use the sample mean (\\(\\bar x\\),) to estimate it. Typically, the data will be closer to \\(\\bar x\\) than to \\(\\mu\\), since \\(\\bar x\\) was computed using the data. To compensate for this, we divide by \\(n-1\\) rather than \\(n\\) when we find the “average” of the squared deviations from the mean. It turns out, that subtracting 1 from \\(n\\) inflates this average by the precise amount needed to compensate for the use of \\(\\bar x\\) as an estimate for \\(\\mu\\). As a result, the sample variance (\\(s^2\\)) is a good estimator of the population variance (\\(\\sigma^2\\).)\n\n\nNeither the standard deviation nor the variance is resistant to outliers. This means that when there are outliers in the data set, the standard deviation and the variance become artificially large. It is worth noting that the mean is also not resistant. When there are outliers, the mean will be “pulled” in the direction of the outliers.\nThe mean and standard deviation are used to describe the center and spread when the distribution of the data is symmetric and bell-shaped. If data are not symmetric and bell-shaped, we typically use the five-number summary (discussed below) to describe the spread, because this summary is resistant."
  },
  {
    "objectID": "Textbook/Spread.html#additional-tools-to-describe-the-data",
    "href": "Textbook/Spread.html#additional-tools-to-describe-the-data",
    "title": "Describing Quantitative Data (Spread)",
    "section": "Additional Tools to Describe the Data",
    "text": "Additional Tools to Describe the Data\nRecall the five steps of the Statistical Process (and the mnemonic “Daniel Can Discern More Truth).\n\n\n\n\n\n\nStep 1:\n\n\n\n\nDaniel\n\n\n\n\nDesign the study\n\n\n\n\n\n\nStep 2:\n\n\n\n\nCan\n\n\n\n\nCollect data\n\n\n\n\n\n\nStep 3:\n\n\n\n\nDiscern\n\n\n\n\nDescribe the data\n\n\n\n\n\n\nStep 4:\n\n\n\n\nMore\n\n\n\n\nMake inferences\n\n\n\n\n\n\nStep 5:\n\n\n\n\nTruth\n\n\n\n\nTake action\n\n\n\n\n\n\n\nStep 3 of this process is “Describe the data.” You have already learned about the mean, median, mode, standard deviation, variance and histograms. These can be good ways to describe the data. The following information on percentiles, quartiles, 5-number summaries, and boxplots will help you learn other common ways to describe data, especially if the data are skewed or contain outliers.\n\nFor symmetric, bell-shaped data, the mean and standard deviation provide a good description of the center and shape of the distribution. The mean and standard deviation are not sufficient to describe a distribution that is skewed or has outliers. An outlier is any observation that is very far from the others. The mean is pulled in the direction of the outlier. Also, the standard deviation is inflated by points that are very far from the mean.\nNow, you have probably had some experience with percentiles in the past especially when you received a score on a standardized test such as the ACT. Even though percentiles are commonly used, they are generally misunderstood. Before examining the wrong site/wrong patient data, let’s review percentiles. Even if you think you understand percentiles, please study this section carefully.\n\n\nPercentiles and Quartiles\nImagine a very long street with houses on one side. The houses increase in value from left to right. At the left end of the street is a small cardboard box with a leaky roof. Next door is a slightly larger cardboard box that does not leak. The houses eventually get larger and more valuable. The rightmost house on the street is a huge mansion.\n\nAnswer the following question:\n\n\n\nThere are 100 homes with increasing property values. How many fences are needed to separate the 100 properties?\n\n\n\nSolution\n\n\nIn order to separate the 100 homes, 99 fences are required.\n\n\n\n\nThe home values are representative of data. If we have a list of data, sorted in increasing order, and we want to divide it into 100 equal groups, we only need 99 dividers (like fences) to divide up the data. The first divider is as large or larger than 1% of the data. The second divider is as large or larger than 2% of the data, and so on. The last divider, the 99th, is the value that is as large or larger than 99% of the data. These dividers are called percentiles. A percentile is a number such that a specified percentage of the data are at or below this number. For example, the 99th percentile is a number such that 99% of the data are at or below this value. As another example, half (50%) of the data lie at or below the 50th percentile. The word percent means \\(\\div 100\\). This can help you remember that the percentiles divide the data into 100 equal groups.\nQuartiles are special percentiles. The word quartile is from the Latin quartus, which means “fourth.” The quartiles divide the data into four equal groups. The quartiles correspond to specific percentiles. The first quartile, Q1, is the 25th percentile. The second quartile, Q2, is the same as the 50th percentile or the median. The third quartile, Q3, is equivalent to the 75th percentile.\n\nAnswer the following questions:\n\n\n\nHow many quartiles are there?\n\n\n\nSolution\n\n\nThere are 3 quartiles! To divide the data into 100 equal groups, we needed 99 percentiles. To divide the data into 4 equal groups, we need 3 quartiles.\n\n \n\n\n\nWrong Site/Wrong Patient Lawsuits\nPercentiles can be used to describe the center and spread of any distribution and are particularly useful when the distribution is skewed or has outliers. To explore this issue, you will use software to calculate percentiles of data on costs incurred by hospitals due to certain lawsuits. The lawsuits in question were about surgeries performed on the wrong patient, or on the right patient but the wrong part of the patient’s body (the wrong site).\nBut first, we need to learn how to load data into R.\nR has many built-in toolboxes. R also has a vast array of toolboxes beyond the built-in ones that we must first install. This is like going to the Home Depot to buy a specialized toolbox and then storing it in your garage. We only have to “buy” it once.\nTo install a library, we use the install.packages(\"\") command, where we specify the library we want in the quotes inside the parentheses.\nrio is a toolbox that is very useful for loading data into R. If you haven’t already done so, install the rio library.\ninstall.packages('rio')\nWhile you only have to install libraries once, you have to load them every time you want to use one. It’s like going to the garage to get the toolbox you need for the job.\nNow let’s load the data and calculate some percentiles!\n\nR Instructions\n\n\n\nOpen R and load the rio library:\n\n\nlibrary(rio)\n\n\nUse the import() function to load the dataset:\n\n\nwrong_site &lt;- import(\"https://github.com/byuistats/Math221D_Course/raw/main/Data/WrongSiteWrongPatient.xlsx\")\n\n  To calculate percentiles and quartiles in R, do the following\n\nDatasets loaded into R may have many columns of information. To specify which column in the dataset should be used for analysis we use the $. For example, if we wanted only to look at the Wrong_Patient clumn in the wrong_site dataset:\n\n\n\n  [1]  250000  106900   62307  192800   20769    2680    4300   30819   23214\n [10]   26099       0   50000   66600  175000   10384   42900   52928       0\n [19]    8200    2500    6900  126300     900    7700  140000   76000   50000\n [28]  354530    5359    4300   12000   16749   35600    9045   21900    2010\n [37]   22444   50000   85000   40370   39863       0   36100   49000   48908\n [46]   19800   32200    3400       0   75000   21774    2600   30000    7300\n [55]  176940   55000    9500   55272    4690   75000   34168   83700    1005\n [64]   17419   34800   14739       0       0    1000     325   41538  108200\n [73]   63224   15000       0    3900   65657   50000  109205    3900   10000\n [82]    9900   87096   12090       0    1000       0   74701    3900   18000\n [91]       0   33499    1250       0   29813   11724  141363    3685   35508\n[100]    2500   12060    5695   50582   82071   55400       0  104400     500\n[109]       0   25000   10000   85000   25000       0   24100    3900 1250000\n[118]   15074     550    7195  101800   11600    1000    4020   19764   25794\n[127]     900   10000   35200   94100       0   16909  128400   60967   50000\n[136]   50000   84751   46800  130308   43800   49242   22800   15500   11054\n[145]     400   10000  104790   13064    6400  100000   17084   16300   11000\n[154]   12500       0    1200       0  200000    3900    3015  172200   25000\n[163]   27468  250000   21104   12500   30000   59000   46227     500  131000\n[172]    2345    6000       0     670    9714      NA      NA      NA      NA\n[181]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n[190]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n[199]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n[208]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n[217]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n[226]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n[235]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n[244]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n[253]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n[262]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n[271]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n[280]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n[289]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n[298]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n[307]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n[316]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n[325]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n[334]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n[343]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n[352]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n[361]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n[370]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n[379]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n[388]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n[397]      NA      NA      NA      NA      NA      NA      NA      NA      NA\n[406]      NA      NA      NA      NA      NA      NA\n\n\n\nUse R’s quantile() function. This functions requires two inputs separated by a comma: the data and the desired percentile input as a decimal.\nTo calculate the 25th percentile for the costs of surgery done on the Wrong Site:\n\n\nquantile(wrong_site$Wrong_Site, .25, na.rm=TRUE)\n\n  25% \n29496 \n\n# Note: the na.rm=TRUE removes the missing values from the dataset\n\n\n\n\nThe first quartile (\\(Q_1\\)) or 25th percentile (calculated in R) of the wrong-site data is: $29,496. (This result is illustrated in the figure below.) This means that 25 percent of the time hospitals lost a wrong-site lawsuit, they had to pay $29,496 or less. The 25th percentile can be written symbolically as: P25 = $29,496. Other percentiles can be written the same way. The 99th percentile can be written as P99.\n\nPercentiles (Calculated in Excel)\n\n\n\n\n \n \n\n\n\n\n1st percentile\n0\n\n\n2nd percentile\n0\n\n\n3rd percentile\n0\n\n\n…\n…\n\n\n24th percentile\n28633.4\n\n\n25th percentile\n29496\n\n\n26th percentile\n31067\n\n\n\n\n\nAnswer the following questions:\n\n\n\nWhat is the 13th percentile of the wrong site data?\n\n\n\nSolution\n\n\n$6343.40\n\n\nquantile(wrong_site$Wrong_Site, .13, na.rm=TRUE)\n\n   13% \n6343.4 \n\n\n\n\nHow would you interpret the 13th percentile (assuming the 13th percentile is $6343.40)?\n\n\n\n100 of the lawsuits cost more than 13%.\n\n\n13% of the lawsuits cost the hospital over $6343.40.\n\n\nIn 13% of the wrong-site lawsuits, hospitals had to pay $6343.40 or less.\n\n\nFor 13% of the wrong-site lawsuits, the hospitals had to pay $6343.40 to the patient.\n\n\n\n\nSolution\n\n\nCorrect Answer: C\n\n\n\n\nFind P90.\n\n\n\nSolution\n\n\n$149,963.00\n\n\nquantile(wrong_site$Wrong_Site, .9, na.rm=TRUE)\n\n   90% \n149963 \n\n\n\n\n\nThe quartiles divide a sorted list of data into four equal groups. So, each group contains 25% of the data. The first quartile is the value that is greater than or equal to 25% of the data. What is another name for this number?\n\n\n\nSolution\n\n\nThe 25th percentile.\n\n\n\n\nWhat is the value of the third quartile?\n\n\n\nSolution\n\n\n$124,280.00\n\n\nquantile(wrong_site$Wrong_Site, .75, na.rm=TRUE)\n\n   75% \n124280 \n\n\n\n\n\nHalf of the wrong-site lawsuits judgments were less than or equal to what value?\n\n\n\nSolution\n\n\n$68,552.00\n\n\nquantile(wrong_site$Wrong_Site, .5, na.rm=TRUE)\n\n  50% \n68552 \n\n#Or\nmedian(wrong_site$Wrong_Site)\n\n[1] 68552\n\n\n\n\n\nThe median is the middle observation in a sorted list of data. What percentile is always equal to the median?\n\n\n\nSolution\n\n\nThe 50th percentile\n\n\n \n\n\n\nThe Five-Number Summary\nAnother way to summarize data is with the five-number summary. The five-number summary is comprised of the minimum, the first quartile, the second quartile (or median), the third quartile, and the maximum.\nThere is a very easy way to get the Five-Number Summary along with the mean and standard deviation. The favstats() function in the mosaic library gives us all of our favorite statistics.\nAs before, we will have to install the mosaic library once, then load it when we want to use it.\n\nR Instructions\n\n\n  To find the values for a five-number summary in R, do the following\n\nInstall the mosaic library (Only Once):\n\ninstall.packages(\"mosaic\")\n\nLoad the Library:\n\n\nlibrary(mosaic)\n\n\nInput the data into the favstats() function:\n\n\nfavstats(wrong_site$Wrong_Site)\n\n min    Q1 median     Q3    max     mean       sd   n missing\n   0 29496  68552 124280 780575 80041.24 71403.83 411       0\n\n\n\n\n\n\nAnswer the following questions:\n\n\n\nGive the five-number summary for the Wrong Site data.\n\n\nSolution\n\n\n\\[\\displaystyle{\\$0,~~\\$29,496;~~\\$68,552;~~\\$124,280;~~\\$780,575}\\]\n\n\n \n\n\n\n\n\nSome students mistakenly include the mean in the five-number summary. The third value in the five-number summary is the median.\n\n \n\n\nBoxplots\nA boxplot is a graphical representation of the five-number summary. Unlike the mean or standard deviation, a boxplot is resistant to outliers. That means that it won’t be “pulled” one way or the other by extraordinarily large or small values in the data as will a mean, for instance. We will illustrate the process of making a boxplot using the wrong-site data.\nFollow the steps below to learn how a boxplot relates to the five-number summary. Learning what each part of the boxplot represents will enable you to interpret the plot correctly.\nStep 01: To draw a boxplot, start with a number line.\n\nStep 02: Draw a vertical line segment above each of the quartiles.\n\nStep 03: Connect the tops and bottoms of the line segments, making a box.\n\nStep 04: Make a smaller mark above the values corresponding to the minimum and the maximum.\n\nStep 05: Draw a line from the left side of the box to the minimum, and draw another line from the right side of the box the maximum.\n\nStep 06: These last two lines look like whiskers, so this is sometimes called a box-and-whisker plot.\n\n\n\nR Instructions\n\n\nTo create a boxplot in Excel, do the following\n\nLoad the data file. For this example, open the file WrongSiteWrongPatient.xlsx.\n\n\nwrong_site &lt;-  import(\"https://github.com/byuistats/Math221D_Course/raw/main/Data/WrongSiteWrongPatient.xlsx\")\n\n\nUse the boxplot() function to get a boxplot:\n\n\nboxplot(wrong_site$Wrong_Site)\n\n\n\n\n\n\n\n# We can make it a little nicer by adding labels to the x and y axes and adding a title as follows:\n\nboxplot(wrong_site$Wrong_Site, xlab=\"Wrong Site\", ylab=\"Cost in $\", main=\"Boxplot of Costs of Operating on the Wrong Site\")\n\n\n\n\n\n\n\n\n\n\n\n\nAnswer the following questions:\n\n\n\nCreate a histogram of the wrong-patient lawsuit data, located in column B of the file WrongSiteWrongPatient.xlsx. What is the shape of the wrong-patient data?\n\n\nSkewed left\nSymmetric\nSkewed right\nMulti-modal\nUniform\n\n\n\nSolution\n\nTo create the histogram, use the histogram() function on the data:\n\nhistogram(wrong_site$Wrong_Patient)\n\n\n\n\n\n\n\n\nFrom the histogram we clearly see most values bunched near the left and gradually fewer values as we move to the right along the number line, so the correct answer is ‘c. Skewed right’."
  },
  {
    "objectID": "Textbook/Spread.html#summary-1",
    "href": "Textbook/Spread.html#summary-1",
    "title": "Describing Quantitative Data (Spread)",
    "section": "Summary",
    "text": "Summary\n\nRemember…\n\n\n\nA percentile is calculated in R using quantile(data, 0.#) where the 0.# is the percentile written as a decimal number. So the 20th percentile would be written as 0.2.\nA percentile is a number such that a specified percentage of the data are at or below this number. For example, if say 80% of college students were shorter than (or equal to) 70 inches tall in height, then the 80th percentile of heights of college students would be 70 inches.\nStandard deviation is calculated in R for a sample of data using sd(data).\nThe standard deviation is a number that describes how spread out the data typically are from the mean of that data. A larger standard deviation means the data are more spread out from their mean than data with a smaller standard deviation. The standard deviation is never negative. A standard deviation of zero implies all values in the data set are exactly the same.\nTo compute any of the five-number summary values in R, use the R function favstats(data) which also includes the mean and standard deviation.\nThe five-number summary consists of (1) the minimum value in the data, (2) the first quartile (25th percentile) of the data, (3) the median of the data (50th percentile), (4) the third quartile (75th percentile) of the data, and (5) the maximum value occurring in the data.\nTo create a boxplot in R, use the boxplot(data) or for multiple columns boxplot(data1, data2, names=c(\"Name of Column 1\", \"Name of Column 2)).\nBoxplots are a visualization of the five-number summary of a data set."
  },
  {
    "objectID": "Textbook/Intro_to_Probability.html",
    "href": "Textbook/Intro_to_Probability.html",
    "title": "Intro to Probability",
    "section": "",
    "text": "Probability is a way of numerically quantifying how likely an event is to happen or not happen. The following historical account demonstrates this idea and shows how fractions (like 1/2 or 3/4) or percentages (like 50% or 75%) can be used to represent probabilities.\n\n\n\nOn August 3, 1492, Columbus set sail from Spain for his intended destination: the Indies (Caso, Adolph 1990). He was on the Santa Maria, which had a crew of approximately 41 men (“Cristobal colon” 1991; “Christopher Columbus”). Several other men were aboard the Nina and the Pinta (“Cristobal colon” 1991). On October 12, he landed on an island in the Bahamas he called San Salvador.\nThe return trip was not without challenges. The Santa Maria ran aground on Christmas Day, 1492, and was abandoned on the island we now call Hispaniola (home to Haiti and the Dominican Republic). Following this incident, Columbus sailed for Spain. Severe storms made the journey difficult. A particularly bad storm on February 14, 1493 made the crew fear for their lives. By morning, the storm was even worse!\nRecognizing his dependence upon God, Columbus ordered that a pilgrimage should be made to a particular shrine upon their safe arrival in Spain. He decided that they would use random chance to determine who would make the pilgrimage. They took one chick pea for each man on board. A knife was used to mark one of the chick peas with a cross. The chick peas were placed in a hat and shaken up. Each man was to draw a chick pea, and the one who had the cross would make the pilgrimage.\n“The first who put in his hand was [Columbus,] and he drew out the bean with a cross, so the lot fell on him; and he was bound to go on the pilgrimage and fulfil the vow” (Caso, Adolph 1990).\n\nAnswer the following questions:\n\n\n\nRemember, there were 41 men aboard his ship. What is the probability that Columbus would draw out the marked chick pea? Express your answer as a fraction, and then convert it to a decimal.\n\n\n\nShow/Hide Solution\n\nThere is only one marked chick pea in the hat, out of 41 chick peas total. Out of is expressed arithmetically by division. The probability is \\(\\frac{1}{41} = 0.0244\\). (Note: this is about 2%.)\n\n\nBased on your answer to the previous question, how likely is it that Columbus would draw out the marked chick pea?\n\n\n\nShow/Hide Solution\n\n\nThere is only about a 2% chance that Columbus will draw out the marked Chick Pea. This is not very likely.\n\n\n\n\nA Second Drawing\nColumbus’ promise to make the pilgrimage did not stop the storm. It was determined that there should be a pilgrimage to another site they held sacred. Again, chick peas representing each member of the crew were placed in a hat and shaken up. The lot fell on a sailor…named Pedro de Villa (Caso, Adolph 1990).\n\nAnswer the following questions:\n\n\n\nWhat is the probability that Columbus would not draw out the marked chick pea? Express your answer as a fraction, and then covert it to a decimal?\n\n\n\nShow/Hide Solution\n\n\nThere are 40 other men on board plus Columbus. So, the probability that Columbus would not draw out the marked chick pea is: \\(\\frac{40}{41} = 0.9756\\). (Note: this is almost 98%.)\n\n\n\nBased on your answer to the previous question, how likely is it that Columbus would not draw out the marked chick pea?\n\n\n\nShow/Hide Solution\n\n\nIt is very likely that Columbus would not draw out the marked chick pea. This result is not surprising.\n\n\n\nIn this second drawing, either Columbus would draw out the marked chick pea, or he would not. Add the probability that Columbus would draw out the marked chick pea and the probability that he would not draw out the marked chick pea. What is the value of this sum?\n\n\n\nShow/Hide Solution\n\n\nThe sum of the probabilities is 1:\n\n\\[\n\\frac{1}{41} + \\frac{40}{41} = \\frac{41}{41} = 1\n\\]\n\n\n\nAdditional Drawings\nAfter the drawing in which Pedro de Villa was chosen to make a pilgrimage, two additional drawings were held. In both cases, Columbus drew out the marked chick pea (Caso, Adolph 1990). In all, Christopher Columbus drew the marked chick pea in three of the four drawings. It can be shown that the probability that this would occur due to chance is very small: 0.0000566.\nBonus material. Read only if you are interested.\nThis calculation is more involved than the calculations you will be required to make in this course this semester. But if you are still interested, read on.\nIn each individual drawing, there was a 1/41 chance of Columbus getting the marked chick pea. Similarly, there was a 40/41 chance of not getting it. Since there were four drawings total, and the goal is to measure the probability of “three of those drawings” resulting in Columbus getting the marked chick pea, it becomes important to think about all of the orders in which Columbus could have gotten 3 out of 4.\n\n\n\n\n\n\n\n\n\n\nPossible Outcome\nFirst Drawing\nSecond Drawing\nThird Drawing\nFourth Drawing\n\n\n\n\nWhat actually happened…\nGot it.\nDidn’t get it.\nGot it.\nGot it.\n\n\nBut he could have…\nGot it.\nGot it.\nDidn’t get it.\nGot it.\n\n\nOr he could have…\nGot it.\nGot it.\nGot it.\nDidn’t get it.\n\n\nOr he could have…\nDidn’t get it.\nGot it.\nGot it.\nGot it.\n\n\n\nIn each of the above cases, notice that Columbus would have gotten the marked chick pea a total of 3 out of 4 times. So, this tells us there are four diffent ways to get the chick pea 3 out of 4 times.\nThe probability of what actually happened to Columbus in the order in which it happened would be computed by multiplying the individual probabilities of each drawing together.\n\\[\n  \\frac{1}{41} \\cdot \\frac{40}{41} \\cdot \\frac{1}{41} \\cdot \\frac{1}{41} \\approx 0.00001415548\n\\]\nBut then, we must also add to this the other “possible” scenarios that would also lead to getting the chick pea 3 out of 4 times, but as shown below, because multiplication is commutative (the order doesn’t matter) these “different” situations result in the same probability as the first.\n\\[\n  \\frac{1}{41} \\cdot \\frac{1}{41} \\cdot \\frac{40}{41} \\cdot \\frac{1}{41} \\approx 0.00001415548\n\\]\n\\[\n  \\frac{1}{41} \\cdot \\frac{1}{41} \\cdot \\frac{1}{41} \\cdot \\frac{40}{41} \\approx 0.00001415548\n\\]\n\\[\n  \\frac{40}{41} \\cdot \\frac{1}{41} \\cdot \\frac{1}{41} \\cdot \\frac{1}{41} \\approx 0.00001415548\n\\]\nThus, all that is needed is to multiply the first probability of roughly 0.00001415548 by 4 to get \\(0.00001415548 \\cdot 4 = 0.00005662192\\).\nEnd of Bonus Material."
  },
  {
    "objectID": "Textbook/Intro_to_Probability.html#probability",
    "href": "Textbook/Intro_to_Probability.html#probability",
    "title": "Intro to Probability",
    "section": "",
    "text": "Probability is a way of numerically quantifying how likely an event is to happen or not happen. The following historical account demonstrates this idea and shows how fractions (like 1/2 or 3/4) or percentages (like 50% or 75%) can be used to represent probabilities.\n\n\n\nOn August 3, 1492, Columbus set sail from Spain for his intended destination: the Indies (Caso, Adolph 1990). He was on the Santa Maria, which had a crew of approximately 41 men (“Cristobal colon” 1991; “Christopher Columbus”). Several other men were aboard the Nina and the Pinta (“Cristobal colon” 1991). On October 12, he landed on an island in the Bahamas he called San Salvador.\nThe return trip was not without challenges. The Santa Maria ran aground on Christmas Day, 1492, and was abandoned on the island we now call Hispaniola (home to Haiti and the Dominican Republic). Following this incident, Columbus sailed for Spain. Severe storms made the journey difficult. A particularly bad storm on February 14, 1493 made the crew fear for their lives. By morning, the storm was even worse!\nRecognizing his dependence upon God, Columbus ordered that a pilgrimage should be made to a particular shrine upon their safe arrival in Spain. He decided that they would use random chance to determine who would make the pilgrimage. They took one chick pea for each man on board. A knife was used to mark one of the chick peas with a cross. The chick peas were placed in a hat and shaken up. Each man was to draw a chick pea, and the one who had the cross would make the pilgrimage.\n“The first who put in his hand was [Columbus,] and he drew out the bean with a cross, so the lot fell on him; and he was bound to go on the pilgrimage and fulfil the vow” (Caso, Adolph 1990).\n\nAnswer the following questions:\n\n\n\nRemember, there were 41 men aboard his ship. What is the probability that Columbus would draw out the marked chick pea? Express your answer as a fraction, and then convert it to a decimal.\n\n\n\nShow/Hide Solution\n\nThere is only one marked chick pea in the hat, out of 41 chick peas total. Out of is expressed arithmetically by division. The probability is \\(\\frac{1}{41} = 0.0244\\). (Note: this is about 2%.)\n\n\nBased on your answer to the previous question, how likely is it that Columbus would draw out the marked chick pea?\n\n\n\nShow/Hide Solution\n\n\nThere is only about a 2% chance that Columbus will draw out the marked Chick Pea. This is not very likely.\n\n\n\n\nA Second Drawing\nColumbus’ promise to make the pilgrimage did not stop the storm. It was determined that there should be a pilgrimage to another site they held sacred. Again, chick peas representing each member of the crew were placed in a hat and shaken up. The lot fell on a sailor…named Pedro de Villa (Caso, Adolph 1990).\n\nAnswer the following questions:\n\n\n\nWhat is the probability that Columbus would not draw out the marked chick pea? Express your answer as a fraction, and then covert it to a decimal?\n\n\n\nShow/Hide Solution\n\n\nThere are 40 other men on board plus Columbus. So, the probability that Columbus would not draw out the marked chick pea is: \\(\\frac{40}{41} = 0.9756\\). (Note: this is almost 98%.)\n\n\n\nBased on your answer to the previous question, how likely is it that Columbus would not draw out the marked chick pea?\n\n\n\nShow/Hide Solution\n\n\nIt is very likely that Columbus would not draw out the marked chick pea. This result is not surprising.\n\n\n\nIn this second drawing, either Columbus would draw out the marked chick pea, or he would not. Add the probability that Columbus would draw out the marked chick pea and the probability that he would not draw out the marked chick pea. What is the value of this sum?\n\n\n\nShow/Hide Solution\n\n\nThe sum of the probabilities is 1:\n\n\\[\n\\frac{1}{41} + \\frac{40}{41} = \\frac{41}{41} = 1\n\\]\n\n\n\nAdditional Drawings\nAfter the drawing in which Pedro de Villa was chosen to make a pilgrimage, two additional drawings were held. In both cases, Columbus drew out the marked chick pea (Caso, Adolph 1990). In all, Christopher Columbus drew the marked chick pea in three of the four drawings. It can be shown that the probability that this would occur due to chance is very small: 0.0000566.\nBonus material. Read only if you are interested.\nThis calculation is more involved than the calculations you will be required to make in this course this semester. But if you are still interested, read on.\nIn each individual drawing, there was a 1/41 chance of Columbus getting the marked chick pea. Similarly, there was a 40/41 chance of not getting it. Since there were four drawings total, and the goal is to measure the probability of “three of those drawings” resulting in Columbus getting the marked chick pea, it becomes important to think about all of the orders in which Columbus could have gotten 3 out of 4.\n\n\n\n\n\n\n\n\n\n\nPossible Outcome\nFirst Drawing\nSecond Drawing\nThird Drawing\nFourth Drawing\n\n\n\n\nWhat actually happened…\nGot it.\nDidn’t get it.\nGot it.\nGot it.\n\n\nBut he could have…\nGot it.\nGot it.\nDidn’t get it.\nGot it.\n\n\nOr he could have…\nGot it.\nGot it.\nGot it.\nDidn’t get it.\n\n\nOr he could have…\nDidn’t get it.\nGot it.\nGot it.\nGot it.\n\n\n\nIn each of the above cases, notice that Columbus would have gotten the marked chick pea a total of 3 out of 4 times. So, this tells us there are four diffent ways to get the chick pea 3 out of 4 times.\nThe probability of what actually happened to Columbus in the order in which it happened would be computed by multiplying the individual probabilities of each drawing together.\n\\[\n  \\frac{1}{41} \\cdot \\frac{40}{41} \\cdot \\frac{1}{41} \\cdot \\frac{1}{41} \\approx 0.00001415548\n\\]\nBut then, we must also add to this the other “possible” scenarios that would also lead to getting the chick pea 3 out of 4 times, but as shown below, because multiplication is commutative (the order doesn’t matter) these “different” situations result in the same probability as the first.\n\\[\n  \\frac{1}{41} \\cdot \\frac{1}{41} \\cdot \\frac{40}{41} \\cdot \\frac{1}{41} \\approx 0.00001415548\n\\]\n\\[\n  \\frac{1}{41} \\cdot \\frac{1}{41} \\cdot \\frac{1}{41} \\cdot \\frac{40}{41} \\approx 0.00001415548\n\\]\n\\[\n  \\frac{40}{41} \\cdot \\frac{1}{41} \\cdot \\frac{1}{41} \\cdot \\frac{1}{41} \\approx 0.00001415548\n\\]\nThus, all that is needed is to multiply the first probability of roughly 0.00001415548 by 4 to get \\(0.00001415548 \\cdot 4 = 0.00005662192\\).\nEnd of Bonus Material."
  },
  {
    "objectID": "Textbook/Intro_to_Probability.html#conclusion",
    "href": "Textbook/Intro_to_Probability.html#conclusion",
    "title": "Intro to Probability",
    "section": "Conclusion",
    "text": "Conclusion\nAs with all the classes you take at BYU-Idaho, it is up to you to decide what you want to get out of this class. If you choose to approach the things you study in class with an open mind, if you prepare diligently and work hard to complete all the learning activities, and if you humbly seek the Lord’s help to understand the intellectual and spiritual truths discussed in this course and in other courses, you will have an outstanding educational experience that will be a blessing to you throughout your life. May you enjoy the journey this semester into statistics!"
  },
  {
    "objectID": "Textbook/Intro_to_Probability.html#summary",
    "href": "Textbook/Intro_to_Probability.html#summary",
    "title": "Intro to Probability",
    "section": "Summary",
    "text": "Summary\n\nRemember…\n\n\n\nIn this class you will use the online textbook that has been written for you by your statistics teachers. All of the assignments and quizzes, available in I-Learn, will be based on the readings, so study it well. Most weeks will cover two lessons.\nYou have successfully located the online textbook. Ensure you have also located the course in I-Learn and can access the quizzes and assignments that are there.\nEnsure you have located the contact information for your instructor in the I-Learn course. Recording the contact information of peers from class would also be a wise idea.\nThis course uses MS Excel for all statistical analysis. Check that you have access to the software on your computer. If not, see I-Learn for details on how to obtain it through the University for free.\nBy doing the work, staying on schedule, and living the Honor Code you will succeed in this class.\nThe three rules of probability are:\n\nA probability is a number between 0 and 1. \\[0 \\leq P(X) \\leq 1\\]\nIf you list all the outcomes of a probability experiment (such as rolling a die) the probability that one of these outcomes will occur is 1. In other words, the sum of the probabilities in any probability is 1. \\[\\sum P(X) = 1\\]\nThe probability that an outcome will not occur is 1 minus the probability that it will occur. \\[P(\\text{not}~X) = 1 - P(X)\\]"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "BYU-Idaho Math 221: Introduction to Statistics in R",
    "section": "",
    "text": "This course is intended to familiarize students with foundational concepts and vocabulary in statistics and introduce basic data wrangling and visualization in R.\nThose who diligently work through these materials will be well prepared for their next Statistics or Data Science course.\nThis book is designed to be a digital workbook and can be edited locally on your computer. Completed Class Notes and Assignments will be submitted in Canvas as web-page reports.\n\n\nThe digital textbook is a great reference for all topics that will be covered in class. Each lesson consists of background readings, example code, and an opportunity to practice.\nThe Textbook tab provides chapters detailing critical statistical concepts.\nClass Notes are more hands-on, interactive activities that reinforce and expand on the key topics in the Textbook.\nRegular homework assignments can be found in the “Homework” tab.\nApplication Activities are designed to give you a more real-world experience dealing with data.\nThe section labeled “R Help” provides resources to help with R programming.\n\n\nThis book was specifically designed for the Math 221, Intro to Stats In R. After introducing some foundational principles of Statistics, the recommended sequence:\n\nCourse Introduction, including R installation\nSummarizing Data - Shape and Center\nSummarizing Data - Measures of Spread\nProbability\nNormal Distribution\nDistribution of Sample Means\nProbability Calculations for Means\nIntroduction to Hypothesis Testing\nIntroduction to Confidence Intervals\nInference for a Mean\nInference for Dependent Samples\nInference for Independent Samples\nANOVA\nRegression\nDistribution of \\(\\hat{p}\\)\nInference for 1 Proportion\nInference for 2 Proportions\nChi Square Test for Independence"
  },
  {
    "objectID": "index.html#a-customizable-textbook",
    "href": "index.html#a-customizable-textbook",
    "title": "BYU-Idaho Math 221: Introduction to Statistics in R",
    "section": "",
    "text": "This course is intended to familiarize students with foundational concepts and vocabulary in statistics and introduce basic data wrangling and visualization in R.\nThose who diligently work through these materials will be well prepared for their next Statistics or Data Science course.\nThis book is designed to be a digital workbook and can be edited locally on your computer. Completed Class Notes and Assignments will be submitted in Canvas as web-page reports.\n\n\nThe digital textbook is a great reference for all topics that will be covered in class. Each lesson consists of background readings, example code, and an opportunity to practice.\nThe Textbook tab provides chapters detailing critical statistical concepts.\nClass Notes are more hands-on, interactive activities that reinforce and expand on the key topics in the Textbook.\nRegular homework assignments can be found in the “Homework” tab.\nApplication Activities are designed to give you a more real-world experience dealing with data.\nThe section labeled “R Help” provides resources to help with R programming.\n\n\nThis book was specifically designed for the Math 221, Intro to Stats In R. After introducing some foundational principles of Statistics, the recommended sequence:\n\nCourse Introduction, including R installation\nSummarizing Data - Shape and Center\nSummarizing Data - Measures of Spread\nProbability\nNormal Distribution\nDistribution of Sample Means\nProbability Calculations for Means\nIntroduction to Hypothesis Testing\nIntroduction to Confidence Intervals\nInference for a Mean\nInference for Dependent Samples\nInference for Independent Samples\nANOVA\nRegression\nDistribution of \\(\\hat{p}\\)\nInference for 1 Proportion\nInference for 2 Proportions\nChi Square Test for Independence"
  },
  {
    "objectID": "index.html#book-scope",
    "href": "index.html#book-scope",
    "title": "BYU-Idaho Math 221: Introduction to Statistics in R",
    "section": "Book Scope",
    "text": "Book Scope\nThis is an introductory book intended to familiarize students with foundational concepts and vocabulary in statistics and introduce basic data wrangling and visualization in R.\nThose who diligently work through these materials should be well prepared for their next statistics course whether in Psychology or Fish and Wildlife Management."
  },
  {
    "objectID": "Homework/Exploring_New_Data_with_Tidyverse.html",
    "href": "Homework/Exploring_New_Data_with_Tidyverse.html",
    "title": "Into The Tidyverse",
    "section": "",
    "text": "Statistics is only as interesting as the research questions we create. However, we cannot hope to answer those questions appropriately without going through the trouble of wrangling data.\nThe more time we spend digging into the data and noticing irregularities, the more likely we are to make better research questions and more appropriate conclusions. It doesn’t matter how sophisticated our analysis becomes if it’s based on bad data.\nIn this activity, you will explore a survey about happiness and related factors. By reviewing the data at a high level and then drilling into specific variables, you will gain a better idea about what this survey contains, generate interesting research questions and, with clean data, make better-informed answers to those questions."
  },
  {
    "objectID": "Homework/Exploring_New_Data_with_Tidyverse.html#lying",
    "href": "Homework/Exploring_New_Data_with_Tidyverse.html#lying",
    "title": "Into The Tidyverse",
    "section": "Lying",
    "text": "Lying\nRepeat the above analysis comparing how peoples’ happiness scores depend on their attitudes about lying. http://127.0.0.1:41855/graphics/e18505de-f3d7-47b3-b868-2c638e36eb4c.png\n\nCreate a new dataset called lying that excludes the #N/A values in Lying and the Happiness scores outliers, and includes only “Happiness_score” and “Lying”:\n\n\nsteve &lt;- happiness %&gt;%\n  filter(Happiness_score &lt;= 15, \n         Happiness_score &gt;= 0, \n         Lying != \"#N/A\"\n  ) %&gt;%\n  select(Happiness_score, Lying)\n\n#View(steve)\n\n\nCreate a side-by-side boxplot and summary statistics (using favstats()) table for each attitude about Lying:\n\n\nboxplot(steve$Happiness_score ~ steve$Lying)\n\n\n\n\n\n\n\nfavstats(steve$Happiness_score ~ steve$Lying)\n\n                    steve$Lying min Q1 median Q3 max     mean       sd   n\n1         everytime it suits me   5  9     10 11  15 10.05839 2.175284 137\n2                         never   4  9     11 12  15 10.80000 2.138090  50\n3 only to avoid hurting someone   3  9     11 12  15 10.54444 1.997179 270\n4                     sometimes   3 10     11 12  15 10.71639 1.764933 543\n  missing\n1       0\n2       0\n3       0\n4       0"
  },
  {
    "objectID": "Class_Notes/Tidyverse_Fundamentals.html",
    "href": "Class_Notes/Tidyverse_Fundamentals.html",
    "title": "Introducing the Tidyverse - Part 1",
    "section": "",
    "text": "In statistics classes, you are typically provided simple, clean datasets to load and analyze with ease. This is a terrible disservice to anyone who will deal with data outside of the classroom.\nAnyone who works with data will have to do some data wrangling. Data wrangling is an appropriate description of cleaning, sorting, filtering, summarizing, transforming, and a whole host of other activities to make data usable for a specific purpose.\nIn this document, we introduce a moderately messy dataset and demonstrate basic programming commands to help us get data ready for analysis or visualization."
  },
  {
    "objectID": "Class_Notes/Tidyverse_Fundamentals.html#additional-resources",
    "href": "Class_Notes/Tidyverse_Fundamentals.html#additional-resources",
    "title": "Introducing the Tidyverse - Part 1",
    "section": "Additional Resources",
    "text": "Additional Resources\nBelow are 2 great resources for digging a little deeper into data manipulation in R.\nTidyverse Cheat Sheet\nR for Data Science\nNext, we will explain a few programming fundamentals that will help make"
  },
  {
    "objectID": "Class_Notes/Tidyverse_Fundamentals.html#logical-operators",
    "href": "Class_Notes/Tidyverse_Fundamentals.html#logical-operators",
    "title": "Introducing the Tidyverse - Part 1",
    "section": "Logical Operators",
    "text": "Logical Operators\nLogical operators are used extensively in computer programming to determine if a certain condition is met. They always return a “True” or a “False”, but we can treat them like a 0 for false and 1 for true.\nWe can tell a computer to determine a conditional statement (typically “less than”, “greater than” or “not equal to”) for specific variables, and it will return a TRUE if the statement is true and FALSE if not.\n\nQuantitative Variables\nLet’s examine the height_cm column in the survey data.\n\nfavstats(survey$Height_cm)\n\n  min  Q1 median      Q3 max     mean       sd   n missing\n 1.68 161    170 178.125 999 169.2412 53.54382 312       0\n\nhist(survey$Height_cm)\n\n\n\n\n\n\n\n\nThe maximum is 999 cm, which is around 33 Feet! We know this is not a possible value.\nIt is very unlikely that a high school student is taller than 7 feet. We can use a logical operator to see which students are taller than 7 feet (213.36 cm):\n\nsurvey$Height_cm &gt; 213.36\n\n  [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [13] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [25] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [37] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [49] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [61] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [73] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE\n [85] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [97] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[109] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[121] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[133] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[145] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[157] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[169] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[181] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[193] FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[205] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[217] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[229] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[241] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[253] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[265] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[277] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[289] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[301] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n\n# To illustrate, the below code puts the Survey column, Height_cm, along with the TRUE/FALSE logical\ndata.frame(Height_cm =survey$Height_cm, logical = survey$Height_cm &gt; 213.36)[73:84,]\n\n   Height_cm logical\n73     162.0   FALSE\n74     172.0   FALSE\n75     160.0   FALSE\n76     175.0   FALSE\n77     182.8   FALSE\n78     153.0   FALSE\n79     184.0   FALSE\n80     170.0   FALSE\n81     150.0   FALSE\n82     177.8   FALSE\n83     999.0    TRUE\n84     172.7   FALSE\n\n\nWhat does the above code return?\nA list of TRUE and FALSE for every line of the data. It is as long as the number of rows in the dataset.\n\n\nCategorical Variables\nWe can also use logical operators for categorical data. For example, if we wanted to see how many people are ambidextrous, we can run the following:\n\n# What are unique values in of the respondents?\n\nunique(survey$Handed)\n\n[1] \"Left-Handed\"  \"Right-Handed\" \"Ambidextrous\"\n\n# Use a logical operator to get TRUE and FALSE for students who responded \"Ambidextrous\" on the survey question about handedness\n\nsum(survey$Handed == \"Ambidextrous\")\n\n[1] 9\n\ndata.frame(Handed = survey$Handed, logical = survey$Handed == 'Ambidextrous')\n\n          Handed logical\n1    Left-Handed   FALSE\n2   Right-Handed   FALSE\n3   Right-Handed   FALSE\n4   Right-Handed   FALSE\n5    Left-Handed   FALSE\n6   Right-Handed   FALSE\n7   Ambidextrous    TRUE\n8   Right-Handed   FALSE\n9   Right-Handed   FALSE\n10  Right-Handed   FALSE\n11  Right-Handed   FALSE\n12   Left-Handed   FALSE\n13  Right-Handed   FALSE\n14  Right-Handed   FALSE\n15  Right-Handed   FALSE\n16  Right-Handed   FALSE\n17  Right-Handed   FALSE\n18  Right-Handed   FALSE\n19  Right-Handed   FALSE\n20   Left-Handed   FALSE\n21  Right-Handed   FALSE\n22   Left-Handed   FALSE\n23  Right-Handed   FALSE\n24  Right-Handed   FALSE\n25  Right-Handed   FALSE\n26  Right-Handed   FALSE\n27  Right-Handed   FALSE\n28  Ambidextrous    TRUE\n29  Right-Handed   FALSE\n30  Right-Handed   FALSE\n31  Right-Handed   FALSE\n32  Right-Handed   FALSE\n33  Right-Handed   FALSE\n34  Right-Handed   FALSE\n35   Left-Handed   FALSE\n36  Right-Handed   FALSE\n37  Right-Handed   FALSE\n38   Left-Handed   FALSE\n39  Right-Handed   FALSE\n40  Right-Handed   FALSE\n41  Right-Handed   FALSE\n42  Right-Handed   FALSE\n43  Right-Handed   FALSE\n44  Right-Handed   FALSE\n45  Right-Handed   FALSE\n46  Right-Handed   FALSE\n47  Right-Handed   FALSE\n48  Ambidextrous    TRUE\n49  Right-Handed   FALSE\n50  Right-Handed   FALSE\n51  Right-Handed   FALSE\n52  Right-Handed   FALSE\n53  Right-Handed   FALSE\n54  Right-Handed   FALSE\n55  Right-Handed   FALSE\n56   Left-Handed   FALSE\n57  Right-Handed   FALSE\n58  Right-Handed   FALSE\n59  Right-Handed   FALSE\n60  Right-Handed   FALSE\n61  Right-Handed   FALSE\n62  Right-Handed   FALSE\n63  Right-Handed   FALSE\n64  Right-Handed   FALSE\n65  Right-Handed   FALSE\n66  Right-Handed   FALSE\n67  Right-Handed   FALSE\n68  Right-Handed   FALSE\n69  Right-Handed   FALSE\n70  Right-Handed   FALSE\n71  Right-Handed   FALSE\n72  Right-Handed   FALSE\n73  Right-Handed   FALSE\n74   Left-Handed   FALSE\n75   Left-Handed   FALSE\n76  Right-Handed   FALSE\n77  Right-Handed   FALSE\n78  Right-Handed   FALSE\n79  Right-Handed   FALSE\n80   Left-Handed   FALSE\n81  Right-Handed   FALSE\n82   Left-Handed   FALSE\n83  Ambidextrous    TRUE\n84  Right-Handed   FALSE\n85  Right-Handed   FALSE\n86   Left-Handed   FALSE\n87  Right-Handed   FALSE\n88  Right-Handed   FALSE\n89   Left-Handed   FALSE\n90  Right-Handed   FALSE\n91   Left-Handed   FALSE\n92  Right-Handed   FALSE\n93  Right-Handed   FALSE\n94  Right-Handed   FALSE\n95  Right-Handed   FALSE\n96  Right-Handed   FALSE\n97  Right-Handed   FALSE\n98  Right-Handed   FALSE\n99  Right-Handed   FALSE\n100 Right-Handed   FALSE\n101 Right-Handed   FALSE\n102 Right-Handed   FALSE\n103 Right-Handed   FALSE\n104 Right-Handed   FALSE\n105 Right-Handed   FALSE\n106 Right-Handed   FALSE\n107 Right-Handed   FALSE\n108 Right-Handed   FALSE\n109 Right-Handed   FALSE\n110 Right-Handed   FALSE\n111 Right-Handed   FALSE\n112 Right-Handed   FALSE\n113 Right-Handed   FALSE\n114 Right-Handed   FALSE\n115 Right-Handed   FALSE\n116 Right-Handed   FALSE\n117 Right-Handed   FALSE\n118 Right-Handed   FALSE\n119 Right-Handed   FALSE\n120 Right-Handed   FALSE\n121 Right-Handed   FALSE\n122 Right-Handed   FALSE\n123 Right-Handed   FALSE\n124 Right-Handed   FALSE\n125 Right-Handed   FALSE\n126 Right-Handed   FALSE\n127 Right-Handed   FALSE\n128 Right-Handed   FALSE\n129 Right-Handed   FALSE\n130 Right-Handed   FALSE\n131  Left-Handed   FALSE\n132 Right-Handed   FALSE\n133 Right-Handed   FALSE\n134 Right-Handed   FALSE\n135 Right-Handed   FALSE\n136 Right-Handed   FALSE\n137 Right-Handed   FALSE\n138 Right-Handed   FALSE\n139 Right-Handed   FALSE\n140 Right-Handed   FALSE\n141 Right-Handed   FALSE\n142 Right-Handed   FALSE\n143 Ambidextrous    TRUE\n144 Right-Handed   FALSE\n145 Right-Handed   FALSE\n146 Right-Handed   FALSE\n147 Right-Handed   FALSE\n148 Right-Handed   FALSE\n149  Left-Handed   FALSE\n150 Right-Handed   FALSE\n151 Right-Handed   FALSE\n152 Right-Handed   FALSE\n153 Right-Handed   FALSE\n154 Right-Handed   FALSE\n155 Right-Handed   FALSE\n156 Right-Handed   FALSE\n157 Right-Handed   FALSE\n158 Right-Handed   FALSE\n159 Right-Handed   FALSE\n160 Right-Handed   FALSE\n161 Right-Handed   FALSE\n162 Right-Handed   FALSE\n163  Left-Handed   FALSE\n164 Right-Handed   FALSE\n165 Right-Handed   FALSE\n166 Right-Handed   FALSE\n167  Left-Handed   FALSE\n168 Right-Handed   FALSE\n169  Left-Handed   FALSE\n170  Left-Handed   FALSE\n171 Right-Handed   FALSE\n172 Right-Handed   FALSE\n173 Right-Handed   FALSE\n174 Right-Handed   FALSE\n175 Right-Handed   FALSE\n176 Right-Handed   FALSE\n177 Right-Handed   FALSE\n178 Right-Handed   FALSE\n179 Right-Handed   FALSE\n180 Right-Handed   FALSE\n181 Right-Handed   FALSE\n182 Right-Handed   FALSE\n183 Right-Handed   FALSE\n184 Right-Handed   FALSE\n185 Ambidextrous    TRUE\n186 Right-Handed   FALSE\n187 Right-Handed   FALSE\n188 Right-Handed   FALSE\n189 Right-Handed   FALSE\n190 Right-Handed   FALSE\n191 Right-Handed   FALSE\n192 Right-Handed   FALSE\n193 Right-Handed   FALSE\n194 Right-Handed   FALSE\n195 Right-Handed   FALSE\n196 Right-Handed   FALSE\n197 Right-Handed   FALSE\n198 Right-Handed   FALSE\n199 Right-Handed   FALSE\n200 Right-Handed   FALSE\n201 Right-Handed   FALSE\n202 Right-Handed   FALSE\n203 Right-Handed   FALSE\n204 Right-Handed   FALSE\n205 Right-Handed   FALSE\n206  Left-Handed   FALSE\n207 Ambidextrous    TRUE\n208 Right-Handed   FALSE\n209 Ambidextrous    TRUE\n210 Right-Handed   FALSE\n211 Right-Handed   FALSE\n212 Right-Handed   FALSE\n213 Right-Handed   FALSE\n214 Right-Handed   FALSE\n215 Right-Handed   FALSE\n216 Right-Handed   FALSE\n217 Right-Handed   FALSE\n218 Right-Handed   FALSE\n219 Right-Handed   FALSE\n220 Right-Handed   FALSE\n221 Right-Handed   FALSE\n222 Right-Handed   FALSE\n223 Right-Handed   FALSE\n224  Left-Handed   FALSE\n225  Left-Handed   FALSE\n226 Right-Handed   FALSE\n227 Right-Handed   FALSE\n228 Right-Handed   FALSE\n229 Right-Handed   FALSE\n230 Right-Handed   FALSE\n231 Right-Handed   FALSE\n232 Right-Handed   FALSE\n233 Right-Handed   FALSE\n234 Right-Handed   FALSE\n235 Right-Handed   FALSE\n236  Left-Handed   FALSE\n237 Right-Handed   FALSE\n238  Left-Handed   FALSE\n239 Right-Handed   FALSE\n240  Left-Handed   FALSE\n241 Right-Handed   FALSE\n242 Right-Handed   FALSE\n243 Right-Handed   FALSE\n244  Left-Handed   FALSE\n245 Right-Handed   FALSE\n246 Right-Handed   FALSE\n247 Right-Handed   FALSE\n248 Right-Handed   FALSE\n249 Right-Handed   FALSE\n250 Right-Handed   FALSE\n251  Left-Handed   FALSE\n252 Right-Handed   FALSE\n253 Right-Handed   FALSE\n254 Right-Handed   FALSE\n255 Right-Handed   FALSE\n256 Right-Handed   FALSE\n257 Right-Handed   FALSE\n258 Right-Handed   FALSE\n259 Right-Handed   FALSE\n260 Right-Handed   FALSE\n261 Right-Handed   FALSE\n262 Right-Handed   FALSE\n263 Right-Handed   FALSE\n264 Right-Handed   FALSE\n265 Right-Handed   FALSE\n266 Right-Handed   FALSE\n267 Right-Handed   FALSE\n268 Right-Handed   FALSE\n269 Right-Handed   FALSE\n270 Right-Handed   FALSE\n271 Right-Handed   FALSE\n272 Right-Handed   FALSE\n273 Right-Handed   FALSE\n274 Right-Handed   FALSE\n275 Right-Handed   FALSE\n276  Left-Handed   FALSE\n277 Right-Handed   FALSE\n278 Right-Handed   FALSE\n279 Right-Handed   FALSE\n280  Left-Handed   FALSE\n281 Right-Handed   FALSE\n282 Right-Handed   FALSE\n283 Right-Handed   FALSE\n284 Right-Handed   FALSE\n285  Left-Handed   FALSE\n286 Right-Handed   FALSE\n287 Right-Handed   FALSE\n288 Ambidextrous    TRUE\n289  Left-Handed   FALSE\n290 Right-Handed   FALSE\n291  Left-Handed   FALSE\n292 Right-Handed   FALSE\n293 Right-Handed   FALSE\n294 Right-Handed   FALSE\n295 Right-Handed   FALSE\n296 Right-Handed   FALSE\n297 Right-Handed   FALSE\n298 Right-Handed   FALSE\n299  Left-Handed   FALSE\n300 Right-Handed   FALSE\n301 Right-Handed   FALSE\n302 Right-Handed   FALSE\n303 Right-Handed   FALSE\n304 Right-Handed   FALSE\n305 Right-Handed   FALSE\n306 Right-Handed   FALSE\n307  Left-Handed   FALSE\n308 Right-Handed   FALSE\n309 Right-Handed   FALSE\n310 Right-Handed   FALSE\n311 Right-Handed   FALSE\n312 Right-Handed   FALSE\n\n\nWe will show you why this is useful when we introduce tidyverse functions."
  },
  {
    "objectID": "Class_Notes/Tidyverse_Fundamentals.html#ceci-nest-pas-une-pipe",
    "href": "Class_Notes/Tidyverse_Fundamentals.html#ceci-nest-pas-une-pipe",
    "title": "Introducing the Tidyverse - Part 1",
    "section": "Ceci n’est pas une pipe",
    "text": "Ceci n’est pas une pipe\nThe tidyverse organizes actions to data sequentially. We separate steps by what is called a “pipe” which is programmed %&gt;%.\nHINT: The shortkey for adding a “pipe” is ctrl+shift+m for Windows, and cmd+shift+m on Mac. Learn this because we use them a lot!"
  },
  {
    "objectID": "Class_Notes/Tidyverse_Fundamentals.html#removing-rows---filter",
    "href": "Class_Notes/Tidyverse_Fundamentals.html#removing-rows---filter",
    "title": "Introducing the Tidyverse - Part 1",
    "section": "Removing Rows - filter()",
    "text": "Removing Rows - filter()\nLogical operators are useful when removing rows from a dataset. The most common logical operators used to filter rows are:\n\n&lt; and &lt;= means “less than” and “less than or equal to” respectively\n&gt; and &gt;= means “greater than” and “greater than or equal to” respectively\n== means “equal to” (NOTE: we use double equals because in most computer languages, a single = is an assignment operator. This avoids ambiguity)\n!= means “not equal to”; this one is useful if you want to eliminate one level of a variable\n%in% is useful for defining a list of levels that you want to include\n\nWe typically begin with the raw dataset, then “pipe” that dataset into a sequence of functions using the “pipe” operator, %&gt;%.\nLet’s begin by filtering out rows we think have legitimate heights:\n\nsurvey %&gt;% \n  filter(Height_cm &lt; 214)\n\n# A tibble: 310 × 60\n   Country Region DataYear ClassGrade Gender Ageyears Handed       Height_cm\n   &lt;chr&gt;   &lt;chr&gt;     &lt;int&gt;      &lt;int&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;            &lt;dbl&gt;\n 1 USA     FL         2022         12 Male         18 Left-Handed        182\n 2 USA     IN         2022         12 Male         17 Right-Handed       190\n 3 USA     GA         2022         12 Female       17 Right-Handed       172\n 4 USA     NC         2022         11 Female       15 Right-Handed       163\n 5 USA     CO         2022         12 Female       17 Left-Handed         51\n 6 USA     MO         2022         11 Male         17 Right-Handed       181\n 7 USA     NC         2022         11 Male         17 Ambidextrous       175\n 8 USA     SC         2022         11 Female       18 Right-Handed       160\n 9 USA     WA         2022         11 Female       16 Right-Handed       156\n10 USA     WA         2022         12 Female       17 Right-Handed       169\n# ℹ 300 more rows\n# ℹ 52 more variables: Footlength_cm &lt;dbl&gt;, Armspan_cm &lt;dbl&gt;,\n#   Languages_spoken &lt;dbl&gt;, Travel_to_School &lt;chr&gt;,\n#   Travel_time_to_School &lt;int&gt;, Reaction_time &lt;dbl&gt;,\n#   Score_in_memory_game &lt;dbl&gt;, Favourite_physical_activity &lt;chr&gt;,\n#   Imprtance_reducing_pllutin &lt;int&gt;, Imprtance_recycling_rubbish &lt;int&gt;,\n#   Imprtance_cnserving_water &lt;int&gt;, Imprtance_saving_energy &lt;int&gt;, …\n\n\nThe above code will return a new dataset without the outliers.\nHow many rows does the original dataset have?\nHow many rows does the filtered dataset have?\nSuppose for some reason, we only want to include right- or left-handed people (excluding ambidextrous). We can add multiple conditions in the filter() function separated by a comma:\n\nunique(survey$Handed)\n\n[1] \"Left-Handed\"  \"Right-Handed\" \"Ambidextrous\"\n\nsurvey %&gt;%\n  filter(Height_cm &lt; 214,\n         Handed != \"Ambidextrous\")\n\n# A tibble: 302 × 60\n   Country Region DataYear ClassGrade Gender Ageyears Handed       Height_cm\n   &lt;chr&gt;   &lt;chr&gt;     &lt;int&gt;      &lt;int&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;            &lt;dbl&gt;\n 1 USA     FL         2022         12 Male         18 Left-Handed        182\n 2 USA     IN         2022         12 Male         17 Right-Handed       190\n 3 USA     GA         2022         12 Female       17 Right-Handed       172\n 4 USA     NC         2022         11 Female       15 Right-Handed       163\n 5 USA     CO         2022         12 Female       17 Left-Handed         51\n 6 USA     MO         2022         11 Male         17 Right-Handed       181\n 7 USA     SC         2022         11 Female       18 Right-Handed       160\n 8 USA     WA         2022         11 Female       16 Right-Handed       156\n 9 USA     WA         2022         12 Female       17 Right-Handed       169\n10 USA     WA         2022         11 Male         18 Right-Handed       160\n# ℹ 292 more rows\n# ℹ 52 more variables: Footlength_cm &lt;dbl&gt;, Armspan_cm &lt;dbl&gt;,\n#   Languages_spoken &lt;dbl&gt;, Travel_to_School &lt;chr&gt;,\n#   Travel_time_to_School &lt;int&gt;, Reaction_time &lt;dbl&gt;,\n#   Score_in_memory_game &lt;dbl&gt;, Favourite_physical_activity &lt;chr&gt;,\n#   Imprtance_reducing_pllutin &lt;int&gt;, Imprtance_recycling_rubbish &lt;int&gt;,\n#   Imprtance_cnserving_water &lt;int&gt;, Imprtance_saving_energy &lt;int&gt;, …\n\n# Equivalently we can use %in% instead of the !=\n\nsurvey %&gt;%\n  filter(Height_cm &lt; 214,\n         Handed %in% c(\"Left-Handed\", \"Right-Handed\"),\n         Region %in% c(\"MO\", \"FL\"))\n\n# A tibble: 27 × 60\n   Country Region DataYear ClassGrade Gender Ageyears Handed       Height_cm\n   &lt;chr&gt;   &lt;chr&gt;     &lt;int&gt;      &lt;int&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;            &lt;dbl&gt;\n 1 USA     FL         2022         12 Male         18 Left-Handed       182 \n 2 USA     MO         2022         11 Male         17 Right-Handed      181 \n 3 USA     MO         2022         11 Female       17 Right-Handed      151 \n 4 USA     FL         2022         12 Female       18 Right-Handed      170 \n 5 USA     FL         2022         12 Male         18 Right-Handed      180 \n 6 USA     FL         2022         12 Female       18 Right-Handed      173.\n 7 USA     FL         2022         12 Male         18 Right-Handed      175 \n 8 USA     FL         2022         12 Male         17 Right-Handed      180 \n 9 USA     MO         2022         11 Male         17 Right-Handed      164 \n10 USA     FL         2022         12 Female       17 Right-Handed      166 \n# ℹ 17 more rows\n# ℹ 52 more variables: Footlength_cm &lt;dbl&gt;, Armspan_cm &lt;dbl&gt;,\n#   Languages_spoken &lt;dbl&gt;, Travel_to_School &lt;chr&gt;,\n#   Travel_time_to_School &lt;int&gt;, Reaction_time &lt;dbl&gt;,\n#   Score_in_memory_game &lt;dbl&gt;, Favourite_physical_activity &lt;chr&gt;,\n#   Imprtance_reducing_pllutin &lt;int&gt;, Imprtance_recycling_rubbish &lt;int&gt;,\n#   Imprtance_cnserving_water &lt;int&gt;, Imprtance_saving_energy &lt;int&gt;, …\n\nsurvey$Handed %in% c(\"Left-Handed\", \"Right-Handed\")\n\n  [1]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE\n [13]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [25]  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [37]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE\n [49]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [61]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [73]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE\n [85]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [97]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[109]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[121]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[133]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE\n[145]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[157]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[169]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[181]  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[193]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[205]  TRUE  TRUE FALSE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[217]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[229]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[241]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[253]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[265]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[277]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE\n[289]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[301]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n\n\nHow many rows does our latest dataset have?"
  },
  {
    "objectID": "Class_Notes/Tidyverse_Fundamentals.html#adding-columns---mutate",
    "href": "Class_Notes/Tidyverse_Fundamentals.html#adding-columns---mutate",
    "title": "Introducing the Tidyverse - Part 1",
    "section": "Adding Columns - mutate()",
    "text": "Adding Columns - mutate()\nThe mutate() statement is used to add new columns to a dataset.\nTo create a new column, “pipe” the previous steps into the mutate() statement. Inside the parentheses, give the new column a name and set it equal to what you want that column to be.\nEXAMPLE: Create a column of the ratio of Height to armspan called, ht_to_span, by using a mutate() statement:\n\nclean &lt;- survey %&gt;%\n  filter(Height_cm &lt; 214,\n         Handed != \"Ambidextrous\") %&gt;%\n  mutate(ht_to_span = Height_cm / Armspan_cm,\n         ht_in = Height_cm / 2.54) %&gt;%\n  select(Handed, ht_to_span, ht_in)\n\nView(clean)\n\nNotice we no longer have to use $ to access specific columns! The tidyverse lives up to its name!"
  },
  {
    "objectID": "Class_Notes/Tidyverse_Fundamentals.html#selecting-columns---select",
    "href": "Class_Notes/Tidyverse_Fundamentals.html#selecting-columns---select",
    "title": "Introducing the Tidyverse - Part 1",
    "section": "Selecting Columns - select()",
    "text": "Selecting Columns - select()\nThere are now over 60 columns in this dataset. Suppose we are only interested in reaction times and height-to-armspan ratio as they related to handedness. To tidy up the data even further, select only the columns we are interested in (Handed, Reaction_time, and ht_to_span):\n\nsurvey %&gt;%\n  filter(Height_cm &lt; 214,\n         Handed != \"Ambidextrous\") %&gt;%\n  mutate(ht_to_span = Height_cm / Armspan_cm) %&gt;%\n  select(Handed, Reaction_time, ht_to_span)\n\n# A tibble: 302 × 3\n   Handed       Reaction_time ht_to_span\n   &lt;chr&gt;                &lt;dbl&gt;      &lt;dbl&gt;\n 1 Left-Handed          0.349      1.28 \n 2 Right-Handed         0.358      0.990\n 3 Right-Handed         0.447      1.03 \n 4 Right-Handed         0.438      1.02 \n 5 Left-Handed          0.542      0.981\n 6 Right-Handed         0.428      0.968\n 7 Right-Handed         0.427      1.01 \n 8 Right-Handed         0.412      1.10 \n 9 Right-Handed         0.346      1.04 \n10 Right-Handed         0.391      1    \n# ℹ 292 more rows\n\n\nSee how much we can do in just a few short, sequential lines of code? Let’s name out clean dataset, clean, and create a boxplot of reaction times comparing left and right handed students:\n\nclean &lt;- survey %&gt;%\n  filter(Height_cm &lt; 214,\n         Handed != \"Ambidextrous\") %&gt;%\n  mutate(ht_to_span = Height_cm / Armspan_cm) %&gt;%\n  select(Handed, Reaction_time, ht_to_span)\n\nboxplot(clean$Reaction_time ~ clean$Handed)\n\n# Modify the code to remove outliers in Reaction_time and remake the boxplot\n\nclean &lt;- survey %&gt;%\n  filter(Height_cm &lt; 214,\n         Handed != \"Ambidextrous\") %&gt;%\n  mutate(ht_to_span = Height_cm / Armspan_cm) %&gt;%\n  select(Handed, Reaction_time, ht_to_span)\n\nboxplot(clean$Reaction_time ~ clean$Handed)"
  },
  {
    "objectID": "Class_Notes/Tidyverse_Fundamentals.html#summarising-data---group_by-summarise",
    "href": "Class_Notes/Tidyverse_Fundamentals.html#summarising-data---group_by-summarise",
    "title": "Introducing the Tidyverse - Part 1",
    "section": "Summarising Data - group_by() + summarise()",
    "text": "Summarising Data - group_by() + summarise()\nThe above data might be adequate for a visualization or analysis, but we can calculate summary statistics tables like we did with favstats() using the tidyverse.\nThe summarise() (or equivalently, summarize()) function is like the mutate statement. We create a name for the new column and set it equal to what we want.\nLet’s name the new dataset, clean, and see how to make summaries using tidyverse.\n\nboxplot(survey$Reaction_time)\n\n\n\n\n\n\n\nclean &lt;- survey %&gt;%\n  filter(Height_cm &lt; 214, \n         Handed != \"Ambidextrous\", \n         Reaction_time &lt; 1,\n         Armspan_cm &gt; 0,\n         ClassGrade == 12) %&gt;%\n  mutate(ht_to_span = Height_cm / Armspan_cm) %&gt;%\n  select(Handed, Reaction_time, ht_to_span)\n\nboxplot(clean$Reaction_time ~ clean$Handed)\n\n\n\n\n\n\n\nclean %&gt;%\n  summarise(\n    mn_react_time = mean(Reaction_time, na.rm=TRUE),\n    med_react_time = median(Reaction_time, na.rm=TRUE),\n    mn_ratio = mean(ht_to_span, na.rm=TRUE)\n  )\n\n# A tibble: 1 × 3\n  mn_react_time med_react_time mn_ratio\n          &lt;dbl&gt;          &lt;dbl&gt;    &lt;dbl&gt;\n1         0.397          0.366     1.27\n\n\nNotice that the mn_ratio is Inf.\nWhy might that be the case?\nModify the code chunk to exclude rows where arm span is 0:\n\nclean &lt;- survey %&gt;%\n  filter(Height_cm &lt; 214,\n         Handed != \"Ambidextrous\",\n         Reaction_time &lt; 1) %&gt;%\n  mutate(ht_to_span = Height_cm / Armspan_cm) %&gt;%\n  select(Handed, Reaction_time, ht_to_span)\n\n\n\nclean %&gt;%\n  summarise(\n    `Mean Reaction Time` = mean(Reaction_time, na.rm=TRUE),\n    med_react_time = median(Reaction_time, na.rm=TRUE),\n    mn_ratio = mean(ht_to_span, na.rm=TRUE),\n    min_react_time = min(Reaction_time, na.rm=TRUE)\n  ) %&gt;% knitr::kable()\n\n\n\n\nMean Reaction Time\nmed_react_time\nmn_ratio\nmin_react_time\n\n\n\n\n0.4205068\n0.388\nInf\n0.067\n\n\n\n\nfavstats(clean$Reaction_time ~ clean$Handed)\n\n  clean$Handed   min      Q1 median      Q3   max      mean        sd   n\n1  Left-Handed 0.274 0.34825 0.4415 0.53850 0.895 0.4741471 0.1691585  34\n2 Right-Handed 0.067 0.33825 0.3845 0.44775 0.995 0.4134380 0.1302318 258\n  missing\n1       0\n2       0\n\nfavstats(clean$ht_to_span)\n\n    min        Q1   median       Q3 max mean  sd   n missing\n 0.0168 0.9841579 1.006519 1.052681 Inf  Inf NaN 292       0\n\n\nIf we want to get means for separate groups, we can add a group_by() statement to tell which variable(s) we want to group by:\n\nclean &lt;- survey %&gt;%\n  filter(Height_cm &lt; 214,\n         Handed != \"Ambidextrous\",\n         Reaction_time &lt; 1,\n         Armspan_cm &gt; 0) %&gt;%\n  mutate(ht_to_span = Height_cm / Armspan_cm) %&gt;%\n  select(Handed, Reaction_time, ht_to_span)\n\n\n\nclean %&gt;%\n  group_by(Handed) %&gt;%\n  summarise(\n    mn_react_time = mean(Reaction_time, na.rm=TRUE),\n    med_react_time = median(Reaction_time, na.rm=TRUE),\n    mn_ratio = mean(ht_to_span, na.rm=TRUE),\n    max_react = max(Reaction_time),\n    count = n()\n  ) %&gt;%\n  knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\nHanded\nmn_react_time\nmed_react_time\nmn_ratio\nmax_react\ncount\n\n\n\n\nLeft-Handed\n0.4741471\n0.4415\n1.266468\n0.895\n34\n\n\nRight-Handed\n0.4133891\n0.3840\n1.260190\n0.995\n257\n\n\n\n\n\nThe n() is very useful for counting up the number of observations in each group.\nIf we were only interested in the summary statistics table, we can do everything in one series of steps:\n\nsummary_stats_table &lt;-  survey %&gt;%\n  filter(\n    Height_cm &lt; 214,\n    Handed != \"Ambidextrous\",\n    Reaction_time &lt; 1,\n    Armspan_cm &gt; 0) %&gt;%\n  mutate(ht_to_span = Height_cm / Armspan_cm) %&gt;%\n  select(Handed, Reaction_time, ht_to_span) %&gt;%\n  group_by(Handed) %&gt;%\n  summarise(\n    mn_react_time = mean(Reaction_time, na.rm=TRUE),\n    med_react_time = median(Reaction_time, na.rm=TRUE),\n    mn_ratio = mean(ht_to_span, na.rm=TRUE),\n    max_react = max(Reaction_time),\n    count = n()\n  )\n\nsummary_stats_table\n\n# A tibble: 2 × 6\n  Handed       mn_react_time med_react_time mn_ratio max_react count\n  &lt;chr&gt;                &lt;dbl&gt;          &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;int&gt;\n1 Left-Handed          0.474          0.442     1.27     0.895    34\n2 Right-Handed         0.413          0.384     1.26     0.995   257"
  },
  {
    "objectID": "Class_Notes/Paired_ttest.html",
    "href": "Class_Notes/Paired_ttest.html",
    "title": "Paired T-test Class Notes",
    "section": "",
    "text": "A matched pairs design is used in statistics to compare 2 treatments or conditions measured on subjects that are logically connected in a meaningful way. In the simplest case, measurements are collected on the same subject as in a before-and-after evaluation.\nMatched pairs designs are often called “dependent samples” because knowing who or what is in the first treatment group determines who will be in the second. In the case of a before-and-after situation, if you are selected to be in the “pre” group, then you will also be in the “post” group. But pairs are not always the same subjects.\n\n\n\nYou want to study the difference in salaries between husbands and wives. If one spouse is selected for the study it automatically determines that the other spouse will be in the other group.\nAn ACT preparation course gives you a test before you take the course and after to see if the course improved test score\nA weight loss program takes your weight at the beginning and after the 12 weeks in the program to see if the program reduced weight.\nComparing prices of a specific set of items between Walmart and Dierbergs. Because we are comparing the same items, we can take the difference between prices for each item.\n\n\n\n\nAs with the one-sample t-test, we have to make sure that the either the pairs are normally distributed or we have a large enough sample size.\nWith smaller sample sizes, create a qqPlot() using the car library to check for normality.\n\n\n\nWhen we perform a matched pairs analysis, we will be doing a 1-sample t-test based on the differences between the connected observations. One challenge is that we can define the difference either way: before - after or after - before.\nA good convention for defining differences so that a negative number means “loss” and a positive number means “gain”.\nFor example, if we believe our weight loss program reduces weight, then defining post_weight - pre_weight should give a negative number, meaning weight lost during the program.\nIf you believe Walmart is cheaper than Dierbergs, defining the difference Dierbergs - Walmart gives a positive number, meaning how much you can save, on average, for shopping at Walmart.\nMathematically, it doesn’t matter which way we define the difference as long as we keep track of what a negative number and a postive number mean. This will define which alternative hypothesis we use."
  },
  {
    "objectID": "Class_Notes/Paired_ttest.html#examples",
    "href": "Class_Notes/Paired_ttest.html#examples",
    "title": "Paired T-test Class Notes",
    "section": "",
    "text": "You want to study the difference in salaries between husbands and wives. If one spouse is selected for the study it automatically determines that the other spouse will be in the other group.\nAn ACT preparation course gives you a test before you take the course and after to see if the course improved test score\nA weight loss program takes your weight at the beginning and after the 12 weeks in the program to see if the program reduced weight.\nComparing prices of a specific set of items between Walmart and Dierbergs. Because we are comparing the same items, we can take the difference between prices for each item."
  },
  {
    "objectID": "Class_Notes/Paired_ttest.html#requirements-for-a-matched-pairs-analysis",
    "href": "Class_Notes/Paired_ttest.html#requirements-for-a-matched-pairs-analysis",
    "title": "Paired T-test Class Notes",
    "section": "",
    "text": "As with the one-sample t-test, we have to make sure that the either the pairs are normally distributed or we have a large enough sample size.\nWith smaller sample sizes, create a qqPlot() using the car library to check for normality."
  },
  {
    "objectID": "Class_Notes/Paired_ttest.html#watchouts",
    "href": "Class_Notes/Paired_ttest.html#watchouts",
    "title": "Paired T-test Class Notes",
    "section": "",
    "text": "When we perform a matched pairs analysis, we will be doing a 1-sample t-test based on the differences between the connected observations. One challenge is that we can define the difference either way: before - after or after - before.\nA good convention for defining differences so that a negative number means “loss” and a positive number means “gain”.\nFor example, if we believe our weight loss program reduces weight, then defining post_weight - pre_weight should give a negative number, meaning weight lost during the program.\nIf you believe Walmart is cheaper than Dierbergs, defining the difference Dierbergs - Walmart gives a positive number, meaning how much you can save, on average, for shopping at Walmart.\nMathematically, it doesn’t matter which way we define the difference as long as we keep track of what a negative number and a postive number mean. This will define which alternative hypothesis we use."
  },
  {
    "objectID": "Class_Notes/Paired_ttest.html#step-1-read-in-data",
    "href": "Class_Notes/Paired_ttest.html#step-1-read-in-data",
    "title": "Paired T-test Class Notes",
    "section": "Step 1: Read in Data",
    "text": "Step 1: Read in Data\n\n# Load Libraries\n\nlibrary(tidyverse)\nlibrary(mosaic)\nlibrary(rio)\nlibrary(car)\n\n\n# Load Data\nweight_loss &lt;- import(\"https://byuistats.github.io/M221R/Data/weight_loss.xlsx\")"
  },
  {
    "objectID": "Class_Notes/Paired_ttest.html#step-2-explore-the-data-and-generate-hypotheses",
    "href": "Class_Notes/Paired_ttest.html#step-2-explore-the-data-and-generate-hypotheses",
    "title": "Paired T-test Class Notes",
    "section": "Step 2: Explore the Data and Generate Hypotheses",
    "text": "Step 2: Explore the Data and Generate Hypotheses\nCreate histograms summary statistics for the pre and post weight measurements:\n\nView(weight_loss)\n\n# Pre-weight histogram\nhistogram(weight_loss$pre)\n\n\n\n\n\n\n\nfavstats(weight_loss$pre)\n\n  min    Q1 median   Q3  max mean       sd  n missing\n 60.7 72.75     77 80.7 99.6 76.9 10.20671 27       0\n\n# Post-weight histogram\nhistogram(weight_loss$post)\n\n\n\n\n\n\n\nfavstats(weight_loss$post)\n\n  min    Q1 median   Q3 max    mean       sd  n missing\n 53.3 63.45   69.9 75.2  97 70.0963 10.63273 27       0"
  },
  {
    "objectID": "Class_Notes/Paired_ttest.html#step-3-prepare-the-data-for-analysis",
    "href": "Class_Notes/Paired_ttest.html#step-3-prepare-the-data-for-analysis",
    "title": "Paired T-test Class Notes",
    "section": "Step 3: Prepare the data for analysis",
    "text": "Step 3: Prepare the data for analysis\nDecide how you’re going to define the difference (post - pre or pre - post).\nQ: What does a negative number mean based on your definition?\n\n# Decide which column to subtract from the other\ndiff &lt;- weight_loss$post - weight_loss$pre\n\nCreate a histogram and a qqPlot of the differences to determine if you will be able to trust the statsitical analyses:\n\n# histogram of the differences\nhistogram(diff)\n\n\n\n\n\n\n\nfavstats(diff)\n\n   min   Q1 median   Q3  max      mean       sd  n missing\n -13.6 -8.9   -6.7 -4.2 -1.6 -6.803704 3.172051 27       0\n\n# Check if the differences are normally distributed:\nqqPlot(diff)\n\n\n\n\n\n\n\n\n[1] 14 18"
  },
  {
    "objectID": "Class_Notes/Paired_ttest.html#step-4-perform-the-appropriate-analysis",
    "href": "Class_Notes/Paired_ttest.html#step-4-perform-the-appropriate-analysis",
    "title": "Paired T-test Class Notes",
    "section": "Step 4: Perform the appropriate analysis",
    "text": "Step 4: Perform the appropriate analysis\n\nHypothesis Test\nState your null and alternative hypotheses.\n\\[ H_0: \\mu_{differences} = 0\\]\n\\[H_a:  \\mu_{differences} &lt; 0\\]\n\\[ \\alpha = 0.01\\]\nQuestions to consider:\n\nHow did you define your difference?\nBased on your decision for the difference, what does a negative number mean? a positive number?\nAre you expecting the difference to be greater than, less than, or not equal to 0?\n\nPerform a t-test of the differences:\n\n# Hypothesis t.test()\nt.test(diff, mu = 0, alternative = \"less\")\n\n\n    One Sample t-test\n\ndata:  diff\nt = -11.145, df = 26, p-value = 1.059e-11\nalternative hypothesis: true mean is less than 0\n95 percent confidence interval:\n     -Inf -5.76249\nsample estimates:\nmean of x \n-6.803704 \n\n\nState your conclusion in context of the research question:\nBecause P-value &lt; 0.01 we reject the null hypothesis. We have SUFFICIENT evidence to suggest that participation in the weight loss program led to weight loss.\n\n\nConfidence Interval\n\n# Confidence interval using t.test()\nt.test(diff, conf.level = .99)$conf.int\n\n[1] -8.500002 -5.107405\nattr(,\"conf.level\")\n[1] 0.99\n\n\nI am 99% confident that the true average weight loss during the program was between -8.5 and -5.11 kilograms."
  },
  {
    "objectID": "Class_Notes/Paired_ttest.html#step-1-read-in-data-1",
    "href": "Class_Notes/Paired_ttest.html#step-1-read-in-data-1",
    "title": "Paired T-test Class Notes",
    "section": "Step 1: Read in Data",
    "text": "Step 1: Read in Data\n\ncholesterol &lt;- import(\"https://byuistats.github.io/M221R/Data/quiz/R/cholesterol.csv\")"
  },
  {
    "objectID": "Class_Notes/Paired_ttest.html#step-2-check-data",
    "href": "Class_Notes/Paired_ttest.html#step-2-check-data",
    "title": "Paired T-test Class Notes",
    "section": "Step 2: Check Data",
    "text": "Step 2: Check Data\nReview the data to see if the variables look correct. Calculate summary statistics and histograms for chol_day2 and chol_day4."
  },
  {
    "objectID": "Class_Notes/Paired_ttest.html#step-3-prepare-data-for-analysis",
    "href": "Class_Notes/Paired_ttest.html#step-3-prepare-data-for-analysis",
    "title": "Paired T-test Class Notes",
    "section": "Step 3: Prepare Data for Analysis",
    "text": "Step 3: Prepare Data for Analysis\nDecide how to define your difference. What does a negative number indicate? a positive number?\nCreate a histogram and summary statistics of the differences:\nBecause we \\(n &lt; 30\\) we need to check the qqPlot() to assess the normality of the differences."
  },
  {
    "objectID": "Class_Notes/Paired_ttest.html#step-4-perform-the-appropriate-analysis-1",
    "href": "Class_Notes/Paired_ttest.html#step-4-perform-the-appropriate-analysis-1",
    "title": "Paired T-test Class Notes",
    "section": "Step 4: Perform the appropriate analysis",
    "text": "Step 4: Perform the appropriate analysis\n\nHypothesis Test\nState your null and alternative hypotheses.\nWhat is your confidence level, ($= $)?\nPerform a matched pairs t-test for the difference in cholesterol at day 2 and day 4:\nState your conclusion:\n\n\nConfidence Interval\nCalculate a confidence interval for the average difference.\nState your conclusions and interpret the confidence interval in context of the research question."
  },
  {
    "objectID": "Class_Notes/Normal_Distribution.html",
    "href": "Class_Notes/Normal_Distribution.html",
    "title": "The Normal Distribution",
    "section": "",
    "text": "In your reading, you learned about the normal distribution which is a probability model that can calculate probabilities for certain types of events that follow a normal distribution.\nFor example, scores on the ACT exam are normally distributed with a mean of 21 and a standard deviation of 5. Let’s say we really wanted to get into Stanford which requires a 33 or higher on the ACT. We can calculate the probability of a random test taker getting into Stanford (if we’re basing it on test score alone.)\nWe can first calculate a Z-score, which is the number of standard deviations a score is away from the mean.\nI always recommend drawing a picture first. Without doing any math, you can usually make a pretty good guess about what the Z-score will be.\nWe can calculate a Z-score using the formula $Z = $ where \\(\\mu\\) is the mean of the normal distribution and \\(\\sigma\\) is the standard deviation.\nThere is a function in R that calculates the probabilities for these values.\n\n# To calculate a z-score, you can use R like a calculator:\n\nx &lt;- 33\nmu &lt;- 21\nstdev &lt;- 5\nz &lt;- (x-mu) / stdev\n\nz\n\n[1] 2.4\n\n# By default, pnorm() gives us the area to the LEFT of our observation\n\nprob_less_than_x &lt;- pnorm(x, mean = mu, sd = stdev)\nprob_greater_than_x &lt;- 1 - prob_less_than_x\n\nprob_less_than_x\n\n[1] 0.9918025\n\nprob_greater_than_x\n\n[1] 0.008197536\n\n# Z is what we call the Standard Normal Distribution. It has a mean of 0 and a SD of 1.  No matter what our original distribution is, if we subtract the mean and divide by the SD we get a distribution with a mean of 0 and an SD = 1.  \n\n# If X is normally distributed, then (X-Mu)/SD is normally distributed with a mean of 0 and SD = 1.  So we can use:\n\npnorm(z) ## Left Tail\n\n[1] 0.9918025\n\n1-pnorm(z) ## Right Tail\n\n[1] 0.008197536"
  },
  {
    "objectID": "Class_Notes/Normal_Distribution.html#iq-scores",
    "href": "Class_Notes/Normal_Distribution.html#iq-scores",
    "title": "The Normal Distribution",
    "section": "IQ Scores",
    "text": "IQ Scores\nIf IQ is normally distributed with a mean of 100 and a Standard Deviation of 11, what’s the probability of a randomly selected person having an IQ GREATER than 127?\nWhat about the probability of a randomly selected person having an IQ LESS than 85?\n\nx &lt;- \nmu &lt;- \nstdev &lt;- \n\nz &lt;- (x-mu) / stdev\n  \npnorm(z)  # LESS THAN\n\n[1] 0.9918025\n\n1-pnorm(z) # GREATER THAN\n\n[1] 0.008197536"
  },
  {
    "objectID": "Class_Notes/Normal_Distribution.html#brisket-competition",
    "href": "Class_Notes/Normal_Distribution.html#brisket-competition",
    "title": "The Normal Distribution",
    "section": "Brisket Competition",
    "text": "Brisket Competition\nCompetition was tight at a Saint Louis BBQ competition. Brisket scores were normally distributed with an average score of 5.941 with a standard deviation of 0.04.\nWhat’s the probability of getting a score GREATER than 6?\nWhat’s the probability of getting a score LESS than 4?\n\nx &lt;- \nmu &lt;- \nstdev &lt;- \n\nz &lt;- (x-mu) / stdev\n  \npnorm(z)  # LESS THAN\n\n[1] 0.5\n\n1-pnorm(z) # GREATER THAN\n\n[1] 0.5"
  },
  {
    "objectID": "Class_Notes/Normal_Distribution.html#prbability-of-a-false-start",
    "href": "Class_Notes/Normal_Distribution.html#prbability-of-a-false-start",
    "title": "The Normal Distribution",
    "section": "Prbability of a “False Start”",
    "text": "Prbability of a “False Start”\n“At high level meets, the time between the gun and first kick against the starting block is measured electronically, via sensors built in the gun and the blocks. A reaction time less than 0.1 s is considered a false start. The 0.2-second interval accounts for the sum of the time it takes for the sound of the starter’s pistol to reach the runners’ ears, and the time they take to react to it.” (https://en.wikipedia.org/wiki/100_metres) Let’s suppose that reaction times are normally distributed with a mean of 0.2 seconds and a standard deviation of 0.03.\nWhat’s the probability of a false start? (that is a reaction time LESS than 0.1)\n\nx &lt;- \nmu &lt;- \nstdev &lt;- \n\nz &lt;- (x-mu) / stdev\n  \npnorm(z)  # LESS THAN\n\n[1] NaN\n\n1-pnorm(z) # GREATER THAN\n\n[1] NaN"
  },
  {
    "objectID": "Class_Notes/Installing_R.html",
    "href": "Class_Notes/Installing_R.html",
    "title": "Installing R",
    "section": "",
    "text": "In this course, we will use R and RStudio to perform necessary calculations. This will allow us to focus on the statistical principles.\nIf you have not already installed R and RStudio on your computer, please follow these instructions to do so.\n\nInstall R: Install the latest version of R link\nInstall RStudio: (Mac OS X | PC)\n\nIf you are using a Chromebook or other “web browsing only” computer that will not allow you to install software, then set up an account at RStudio Cloud instead of installing R and RStudio as shown here. Use your BYU-I\nJust accept all of the default options when installing.\n\n\nTo install the statistical analysis program RStudio you will first need to install a piece of software called R. Funny name, right? (There was originally a software called “S” for statistics, and then “R” was invented later on. Part of the reason they used “R” was to claim that “R” was a “leap ahead” of “S.”)\nInstall the R Software by clicking:\n\nMac OS X M1-3 Chip (Most Common)\nMac OS X Intel Chip\nPC\n\nOnce that download finishes, open the resulting file.\nClick “Continue” or “Okay” or “Accept” for all of the several various windows that will appear.\nNow that R is properly installed on your computer, we need to install RStudio. RStudio is an app that runs R inside of it and provides you with many other tools that go way beyond what R can do. This is why R must be installed first, so that RStudio can use it. You will never need to open R yourself. Just use RStudio. But without R, RStudio won’t work properly.\nInstall the RStudio app by clicking here: (Mac OS X | Windows).\nOnce the RStudio installer downloads, open the resulting file.\nAgain, work through the installation process, agreeing with all the defaults and terms of conditions.\nOnce the installation finishes you can use your computer’s search bar to search for “RStudio” in your apps."
  },
  {
    "objectID": "Class_Notes/Installing_R.html#detailed-instructions",
    "href": "Class_Notes/Installing_R.html#detailed-instructions",
    "title": "Installing R",
    "section": "",
    "text": "To install the statistical analysis program RStudio you will first need to install a piece of software called R. Funny name, right? (There was originally a software called “S” for statistics, and then “R” was invented later on. Part of the reason they used “R” was to claim that “R” was a “leap ahead” of “S.”)\nInstall the R Software by clicking:\n\nMac OS X M1-3 Chip (Most Common)\nMac OS X Intel Chip\nPC\n\nOnce that download finishes, open the resulting file.\nClick “Continue” or “Okay” or “Accept” for all of the several various windows that will appear.\nNow that R is properly installed on your computer, we need to install RStudio. RStudio is an app that runs R inside of it and provides you with many other tools that go way beyond what R can do. This is why R must be installed first, so that RStudio can use it. You will never need to open R yourself. Just use RStudio. But without R, RStudio won’t work properly.\nInstall the RStudio app by clicking here: (Mac OS X | Windows).\nOnce the RStudio installer downloads, open the resulting file.\nAgain, work through the installation process, agreeing with all the defaults and terms of conditions.\nOnce the installation finishes you can use your computer’s search bar to search for “RStudio” in your apps."
  },
  {
    "objectID": "Class_Notes/Installing_R.html#mac-processing-chip",
    "href": "Class_Notes/Installing_R.html#mac-processing-chip",
    "title": "Installing R",
    "section": "Mac Processing Chip",
    "text": "Mac Processing Chip\nFor Macs, Which version of R-Studio you download depends on which processing chip you have. If you followed the instructions above and R-Studio opens but gives you a big error, you need to download the other version of R linked above."
  },
  {
    "objectID": "Class_Notes/Independent_2_sample_ttest.html",
    "href": "Class_Notes/Independent_2_sample_ttest.html",
    "title": "Independent Two-sample T-test",
    "section": "",
    "text": "In many situations we would like to compare averages from different populations. In these situations, we take 2 random samples from each population and perform statistical tests to see determine if the populations are significantly different. Because these two groups of individuals are sampled independently, we call this analysis Independent 2-Sample t-test.\nAlternatively, in many experimental designs, participants are randomly assigned into a treatment and a control group. The randomization process ensures that there is no association between participants in either group. They are independent.\nWhen 2 random samples are taken from 2 separate populations, or when a group of people are randomly assigned into treatment groups, the samples are independent.\nThis differs from the dependent t-test. Samples are dependent when knowing who or what is in one group determines who or what is in the second group.\nSome examples include:\n\nComparing salaries of men and women (randomly sample men and women separately)\nTesting a new medication compared to a placebo (participants randomly assigned to treatment groups)\nComparing average GPA of Math majors and Economics majors (randomly select from each population)"
  },
  {
    "objectID": "Class_Notes/Independent_2_sample_ttest.html#independent-t-test-in-r",
    "href": "Class_Notes/Independent_2_sample_ttest.html#independent-t-test-in-r",
    "title": "Independent Two-sample T-test",
    "section": "Independent t-test in R",
    "text": "Independent t-test in R\nA 2-sample independent t-test in R requires a slight modification to the 1-sample and dependent t-tests already performed. The syntax should also look familiar.\nRecall that when we created a boxplot() or did favstats() for one set of data it looked like:\n\nboxplot(data$response_variable)\nfavstats(data$response_variable)\n\nwith data$response_variable corresponding to our quantitative variable of interest.\nWhen we wanted to break the analysis down by a grouping factor we used the ~ notation to add a group variable:\nboxplot(data$response_variable ~ data$grouping_variable)\nfavstats(data$response_variable ~ data$grouping_variable)\nWe use the exact same modification for a t-test with 2 groups:\nt.test(data$response_variable ~ data$grouping_variable, alterntive = \"greater\")\nRecall that the t-test() function uses mu=0 as a default, we do not need to specify it in the function because that is the null value when comparing 2 groups.\nNOTE: In R, group 1 and 2 are determined alphabetically according to the labels in the dataset."
  },
  {
    "objectID": "Class_Notes/Independent_2_sample_ttest.html#confidence-intervals",
    "href": "Class_Notes/Independent_2_sample_ttest.html#confidence-intervals",
    "title": "Independent Two-sample T-test",
    "section": "Confidence Intervals",
    "text": "Confidence Intervals\nRecall that confidence intervals are necessarily two-sided. So the code for a 99% confidence interval looks like:\nt.test(data$response_variable ~ data$grouping_variable, conf.level = .99)$conf.int\nWe interpret a confidence interval for the difference of means as follows:\n\nI am 99% confident that the true difference of the means is between [lower limit] and [upper limit].\n\nWe can usually do better within the context of a research question:\n\nClass 1 did, on average, between 3.21 and 5.67 percent better than class 2 on the last exam.\n\nStore A outperforms Store B by between $27,022 and $36,977 on average"
  },
  {
    "objectID": "Class_Notes/Independent_2_sample_ttest.html#step-1-read-in-data",
    "href": "Class_Notes/Independent_2_sample_ttest.html#step-1-read-in-data",
    "title": "Independent Two-sample T-test",
    "section": "Step 1: Read in Data",
    "text": "Step 1: Read in Data\n\n# Load Libraries\n\nlibrary(tidyverse)\nlibrary(mosaic)\nlibrary(rio)\nlibrary(car)\n\n\n# Load Data\n\nfifa_heart_attacks &lt;- import(\"https://byuistats.github.io/M221R/Data/fifa_heart_attacks.xlsx\")"
  },
  {
    "objectID": "Class_Notes/Independent_2_sample_ttest.html#step-2-review-data",
    "href": "Class_Notes/Independent_2_sample_ttest.html#step-2-review-data",
    "title": "Independent Two-sample T-test",
    "section": "Step 2: Review Data",
    "text": "Step 2: Review Data\nLook at the data.\nCreate summary statistics tables of the number of heart attacks for each group.\nCreate a side-by-side boxplot for the during the World Cup and the Control.\nDo you notice any outliers or data that may need to be omitted for analysis?\nCheck to see if the means from both groups are normally distributed:\n\nIs n &gt; 30 for both groups?\nCreate a qqPlot()\n\n\nqqPlot(fifa_heart_attacks$heart_attacks, groups = fifa_heart_attacks$time_period)\n\n\n\n\n\n\n\n\nCan we trust that the central limit theorem applies?"
  },
  {
    "objectID": "Class_Notes/Independent_2_sample_ttest.html#step-3-prepare-data-for-analysis",
    "href": "Class_Notes/Independent_2_sample_ttest.html#step-3-prepare-data-for-analysis",
    "title": "Independent Two-sample T-test",
    "section": "Step 3: Prepare Data for Analysis",
    "text": "Step 3: Prepare Data for Analysis\nThese data look ready for analysis."
  },
  {
    "objectID": "Class_Notes/Independent_2_sample_ttest.html#step-4-perform-the-appropriate-analysis",
    "href": "Class_Notes/Independent_2_sample_ttest.html#step-4-perform-the-appropriate-analysis",
    "title": "Independent Two-sample T-test",
    "section": "Step 4: Perform the appropriate analysis",
    "text": "Step 4: Perform the appropriate analysis\n\nHypothesis Test\nAre the individuals in each group dependent or independent of each other?\nWrite out your null and alternative hypotheses.\nHo: Ha:\nWhich group is considered group 1 and which is group 2 in R?\nCheck the alphabetical order:\n\nunique(fifa_heart_attacks$time_period)\n\n[1] \"Control\"   \"World Cup\"\n\n\nPerform the appropriate t-test.\nWhat is your test statistic?\nWhat is your p-value?\nState your conclusion:\n\n\n97% Confidence interval\nCalculate the 97% confidence interval for the difference of the means.\nIn context of the research question, interpret the confidence interval."
  },
  {
    "objectID": "Class_Notes/Independent_2_sample_ttest.html#new-zealand-rugby",
    "href": "Class_Notes/Independent_2_sample_ttest.html#new-zealand-rugby",
    "title": "Independent Two-sample T-test",
    "section": "New Zealand Rugby",
    "text": "New Zealand Rugby\nRugby is a popular sport in the United Kingdom, France, Australia, New Zealand and South Africa. It is gaining popularity in the US, Canada, Japan and parts of Europe. Some of the rules of the game have recently been changed to make play more exciting. In a study to examine the effects of the rule changes, Hollings and Triggs (1993) collected data on some recent games.\nTypically, a game consists of bursts of activity that terminate when points are scored, if the ball is moved out of the field of play or if a violation of the rules occurs. In 1992, the investigators gathered data on ten international matches which involved the New Zealand national team, the All Blacks. The first five games were the last international games played under the old rules, and the second set of five were the first internationals played under the new rules.\nFor each of the ten games, the data give the successive times (in seconds) of each passage of play in that game.\nYou will investigate whether the mean duration of the passages has dropped under the new rules.\nUse a level of significance of 0.01.\n\nLoad the Data\n\nrugby &lt;- import(\"https://byuistats.github.io/M221R/Data/quiz/R/nz_rugby.csv\")\n\n\n\nExplore the Data\nCreate a side-by-side boxplot for the amount of reported passage of play before and after the rule changes.\nAdd a title and change the colors of the boxes.\nWhat do you observe?\nCreate a table of summary statistics of play time for before and after the rule change. (favstats()):\n\n\nPerform the Appropriate Analysis\n\nHypothesis Test\nState your null and alternative hypotheses:\nNOTE: The default for R is to set group order alphabetically. This means Group 1 = NewRules\nCompare the the time per play under the new and old rules:\n\nqqPlot(rugby$time, groups = rugby$period)\n\n\n\n\n\n\n\n\nDo the data for each group appear normally distributed?\nWhy is it OK to continue with the analysis?\nPerform a t-test.\nWhat is the value of the test statistic?\nHow many degrees of freedom for this test?\nWhat is the p-value?\nWhat do you conclude?\n\n\nConfidence Interval\nCreate a confidence interval for the difference of the average Importance Score between both groups:"
  },
  {
    "objectID": "Class_Notes/Independent_2_sample_ttest.html#step-1-read-in-the-data",
    "href": "Class_Notes/Independent_2_sample_ttest.html#step-1-read-in-the-data",
    "title": "Independent Two-sample T-test",
    "section": "Step 1: Read in the data",
    "text": "Step 1: Read in the data\n\ncopd &lt;- import(\"https://byuistats.github.io/M221R/Data/copd_rehab.xlsx\") %&gt;% pivot_longer(cols=c(\"community\", \"hospital\"), names_to = \"Treatment\", values_to = \"Steps\") %&gt;% select(Treatment, Steps) %&gt;% arrange(Treatment)"
  },
  {
    "objectID": "Class_Notes/Independent_2_sample_ttest.html#step-2-review-the-data",
    "href": "Class_Notes/Independent_2_sample_ttest.html#step-2-review-the-data",
    "title": "Independent Two-sample T-test",
    "section": "Step 2: Review the data",
    "text": "Step 2: Review the data\nCreate side-by-side boxplots and summary statistics for the community and hospital groups:\nCheck to see if the means are expected to be normally distributed.\nCan trust the CLT for our test statistic and P-value?"
  },
  {
    "objectID": "Class_Notes/Independent_2_sample_ttest.html#step-3-prepare-data-for-analysis-1",
    "href": "Class_Notes/Independent_2_sample_ttest.html#step-3-prepare-data-for-analysis-1",
    "title": "Independent Two-sample T-test",
    "section": "Step 3: Prepare Data for Analysis",
    "text": "Step 3: Prepare Data for Analysis\nThe data cleansing has been performed for you. You’re welcome."
  },
  {
    "objectID": "Class_Notes/Independent_2_sample_ttest.html#step-4-perform-the-appropriate-analysis-1",
    "href": "Class_Notes/Independent_2_sample_ttest.html#step-4-perform-the-appropriate-analysis-1",
    "title": "Independent Two-sample T-test",
    "section": "Step 4: Perform the appropriate analysis",
    "text": "Step 4: Perform the appropriate analysis\n\nHypothesis Test\nState your null and alternative hypotheses.\nHo:\nHa:\nWhich group is considered group 1 in this data?\nRun the appropriate t-test.\n\n#t.test()\n\nState your conclusion about the hypothesis test.\n\n\nConfidence Interval\nCreate a 95% confidence interval for the difference between the means\nInterpret the 95% confidence interval for the mean difference between the community-based and hospital-based groups."
  },
  {
    "objectID": "Class_Notes/CLT_Normal_Prob_for_Means.html",
    "href": "Class_Notes/CLT_Normal_Prob_for_Means.html",
    "title": "Normal Probability Calculations for Means",
    "section": "",
    "text": "The Central Limit Theorem states that for a large enough sample size (\\(n&gt;30\\)) the distribution of sample means is approximately normal with mean, \\(\\mu_{\\bar{x}} = \\mu\\) and \\(\\sigma_{\\bar{x}} = \\frac{\\sigma}{\\sqrt{n}}\\) regardless of the distribution of the population.\nWe can assume the distribution of sample means is approximately normal if:\n\nThe population is normally distributed\nn &gt; 30\n\nDon’t forget, that if the population is normally distributed, so is the distribution of sample means regardless of sample size."
  },
  {
    "objectID": "Class_Notes/CLT_Normal_Prob_for_Means.html#gpas",
    "href": "Class_Notes/CLT_Normal_Prob_for_Means.html#gpas",
    "title": "Normal Probability Calculations for Means",
    "section": "GPA’s",
    "text": "GPA’s\nSuppose the mean GPA of BYU-Idaho students is 3.5 and the standard deviation is 0.7. It is well known that this distribution is left-skewed. A random sample of n = 45 students will be drawn.\nQuestion 1: What is the mean of the distribution of the sample means (sampling distribution) for all possible samples of size 45 that could be drawn from the parent population of GPAs?\nQuestion 2: What is the standard deviation of the distribution of the sample means (sampling distribution) for all possible samples of size 45 that could be drawn from the parent population?\nQuestion 3: What is the probability that the mean GPA for 45 randomly selected BYU-Idaho students will be less than 3.3?\nQuestion 4: What is the shape of the distribution of sample means, \\(\\bar{x}\\), 45 students are selected in each of the possible random samples?"
  },
  {
    "objectID": "Class_Notes/CLT_Normal_Prob_for_Means.html#gre-scores",
    "href": "Class_Notes/CLT_Normal_Prob_for_Means.html#gre-scores",
    "title": "Normal Probability Calculations for Means",
    "section": "GRE Scores",
    "text": "GRE Scores\nScores on the quantitative portion of the GRE are approximately normally distributed with mean, \\(\\mu=150.8\\), and standard deviation \\(\\sigma = 8.8\\).\nQuestion 1: Dianne earned a score of 160 on the quantitative portion of the GRE. What is the z-score corresponding to Dianne’s score?\nQuestion 2: What is the probability that a randomly selected student will score above 160 on the quantitative portion of the GRE?\nQuestion 3: What is the probability that a randomly selected group of 18 students will have an average less than 160 on the quantitative portion of the GRE?\nQuestion 4: What is the probability that a randomly selected group of 18 students will have an average between 145 and 160 on the quantitative portion of the GRE?"
  },
  {
    "objectID": "Class_Notes/Bivariate_Data_Intro.html",
    "href": "Class_Notes/Bivariate_Data_Intro.html",
    "title": "Introducing the Bivariate Data",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(rio)\n\ngeyser &lt;- import('https://byuistats.github.io/BYUI_M221_Book/Data/OldFaithful.xlsx')\nnames(geyser)\n\n[1] \"Duration\" \"Wait\"     \"Source\"  \n\nmath &lt;- import('https://byuistats.github.io/BYUI_M221_Book/Data/MathSelfEfficacy.xlsx')\nnames(math)\n\n[1] \"Gender\"               \"Score\"                \"ConfidenceRatingMean\"\n[4] \"Comments\""
  },
  {
    "objectID": "Class_Notes/Bivariate_Data_Intro.html#scatter-plot",
    "href": "Class_Notes/Bivariate_Data_Intro.html#scatter-plot",
    "title": "Introducing the Bivariate Data",
    "section": "Scatter plot",
    "text": "Scatter plot\nMake a scatter plot showing the relationship between wait time and the duration of the next eruption.\nWhich variable is the Explanatory variable? Which is the Response?\n\nplot(Duration ~ Wait, data = geyser)\n\n\n\n\n\n\n\n\nBefore looking at the Correlation Coefficient, r, describe in words the direction and strength of the relationship. Does it look linear?"
  },
  {
    "objectID": "Class_Notes/Bivariate_Data_Intro.html#correlation-coefficient",
    "href": "Class_Notes/Bivariate_Data_Intro.html#correlation-coefficient",
    "title": "Introducing the Bivariate Data",
    "section": "Correlation Coefficient",
    "text": "Correlation Coefficient\n\ncor(Duration ~ Wait, data = geyser)\n\n[1] 0.9008112"
  },
  {
    "objectID": "Class_Notes/Bivariate_Data_Intro.html#scatter-plot-1",
    "href": "Class_Notes/Bivariate_Data_Intro.html#scatter-plot-1",
    "title": "Introducing the Bivariate Data",
    "section": "Scatter plot",
    "text": "Scatter plot\nMake a scatter plot showing the relationship between the self reported confidence rating and their test score.\nWhich variable is the Explanatory variable? Which is the Response?\n\nplot(Score ~ ConfidenceRatingMean, data = math)\n\n\n\n\n\n\n\n\nBefore looking at the Correlation Coefficient, r, describe in words the direction and strength of the relationship. Does it look linear?"
  },
  {
    "objectID": "Class_Notes/Bivariate_Data_Intro.html#correlation-coefficient-1",
    "href": "Class_Notes/Bivariate_Data_Intro.html#correlation-coefficient-1",
    "title": "Introducing the Bivariate Data",
    "section": "Correlation Coefficient",
    "text": "Correlation Coefficient\n\ncor(Score ~ ConfidenceRatingMean, data = math)\n\n[1] 0.7278648"
  },
  {
    "objectID": "Class_Notes/ANOVA_Intro.html",
    "href": "Class_Notes/ANOVA_Intro.html",
    "title": "Analysis of Variance (ANOVA)",
    "section": "",
    "text": "When we want to compare 3 or more groups, the math get’s more complicated. Analysis of Variance (ANOVA) compares how spread out the means are relative to the average within group variation. The formula for the new test statistic, F, is messy, but we can get a sense for what it’s doing visually.\n\nThe test statistic is the ratio of the variation between groups and the average variation within groups. The further spread out the sample means are relative to the noise within the groups, the more significant the result.\n\n\nRecall that the shape of the t-distribution depended on how much data was in the sample. The t-distribution was fatter tailed than the standard normal distribution when n was small. The F-statistic also changes shape. Its shape depends on how many data points are in the sample and how many groups we are comparing. This means the F-distribution has 2 sets of degrees of freedom.\nThe numerator, or between groups degrees of freedom is \\(df_{between}=k-1\\), where k is the number of groups you are comparing.\nThe denominator, or within groups degrees of freedom is \\(df_{within}=n-k\\) where n is the total number of data points and k is the number of groups.\nWe can get these degrees of freedom directly from the R output.\nUnlike the t-distribution, the F-distribution is not centered around zero and can never be negative. F is the ratio of 2 positive numbers and is, therefore, always positive.\nTo summarize:\n\nF is always positive because it is the ratio of 2 positive numbers\nF is always right skewed\nF changes shape depending on the number of groups (numerator degrees of freedom) and the number of total data points (denominator degrees of freedom)\n\n\nThe P-value for an F-statistic is always one-tailed. The probability of observing a test statistic, F, if the null hypothesis is true can be visualized:\n\nIn practice, the computer calculates the test statistic, P-value, and degrees of freedom and we interpret the output as with other statistical tests.\n\n\n\nThe null and alternative hypotheses are always the same for an ANOVA:\n\\[H_o: \\mu_1 = \\mu_2 = ...\\mu_k\\] where k is the number of groups in the data.\n\\[H_a: \\text{at least one } \\mu_k \\text{ is different}\\] Note: The F-test does not tell us which group is different or how many are different from each other. The F-test only tells us that something is different.\n\n\n\nJust as with other statistical tests we’ve done so far, the F-test has certain requirements we must check to validate our P-values. Because of the way we calculate F, we are less concerned with the normality of the individual groups as we are with the variation within the groups.\nWhat to check:\n\nAre the standard deviations of each group “equal”?\nAre the residuals normally distributed?\n\n\n\nTo check the first requirement we can compare the biggest standard deviation to the smallest. If the ratio of the biggest to the smallest is less than 2, we conclude that the population standard deviations are “equal”.\nNOTE: Intuitively, this means that if the biggest standard deviation is more than twice as big as the smallest, then we might have cause for concern.\nThis can be checked using the standard deviations from the favstats() output (see Analysis in R below)\n\n\n\nWe will discuss residuals in more depth when we cover regression analysis. For now, think of residuals as the deviations of each observation away from their group mean. If our analysis is to be trusted, these deviations need to be normally distributed.\nIf both requirements are met, then we can trust the P-value.\n\n\n\n\nMuch of the syntax for ANOVA will look familiar, but we will be using a new function, aov() instead of t.test().\nThe aov() function by itself isn’t as useful as t.test(). However, we can use the summary() function to give us everything we need.\nWe typically name our output using the assignment operator &lt;- to make it easier to extract the information we would like. The inside of aov() will look familiar, using the same ~ notation we’ve used all semester.\nThe generic process for performing an ANOVA is:\n\n# Name the ANOVA output:\noutput &lt;- aov(data$response_variable ~ data$categorical_variable)\n\n# Summarise the ANOVA output to get test statistics, DF, P-value, etc:\nsummary(output)\n\n\n\n\n\nWe can use favstats() to extract the standard deviations of each group, then find the ratio of the max/min to see if it is less than 2.\n\n# extract only the standard deviations from favstats using th `$`:\n\nsds &lt;- favstats(data$response_variable ~ data$categorical_variable)$sd\n\n# Compare the max/min to 2\n\nmax(sds) / min(sds)\n\n# if max/min &lt; 2, then we're ok\n\n\n\n\nWe can assess normality of the residuals with a qqPlot(). We first need to extract the residuals from our output:\noutput &lt;- aov(data$response_variable ~ data$categorical_variable)\n\nqqPlot(output$residuals)\n\nIf most of the points fall within the blue zone, we can be confident that the residuals are normally distributed."
  },
  {
    "objectID": "Class_Notes/ANOVA_Intro.html#the-f-distribution",
    "href": "Class_Notes/ANOVA_Intro.html#the-f-distribution",
    "title": "Analysis of Variance (ANOVA)",
    "section": "",
    "text": "Recall that the shape of the t-distribution depended on how much data was in the sample. The t-distribution was fatter tailed than the standard normal distribution when n was small. The F-statistic also changes shape. Its shape depends on how many data points are in the sample and how many groups we are comparing. This means the F-distribution has 2 sets of degrees of freedom.\nThe numerator, or between groups degrees of freedom is \\(df_{between}=k-1\\), where k is the number of groups you are comparing.\nThe denominator, or within groups degrees of freedom is \\(df_{within}=n-k\\) where n is the total number of data points and k is the number of groups.\nWe can get these degrees of freedom directly from the R output.\nUnlike the t-distribution, the F-distribution is not centered around zero and can never be negative. F is the ratio of 2 positive numbers and is, therefore, always positive.\nTo summarize:\n\nF is always positive because it is the ratio of 2 positive numbers\nF is always right skewed\nF changes shape depending on the number of groups (numerator degrees of freedom) and the number of total data points (denominator degrees of freedom)\n\n\nThe P-value for an F-statistic is always one-tailed. The probability of observing a test statistic, F, if the null hypothesis is true can be visualized:\n\nIn practice, the computer calculates the test statistic, P-value, and degrees of freedom and we interpret the output as with other statistical tests."
  },
  {
    "objectID": "Class_Notes/ANOVA_Intro.html#hypothesis-test",
    "href": "Class_Notes/ANOVA_Intro.html#hypothesis-test",
    "title": "Analysis of Variance (ANOVA)",
    "section": "",
    "text": "The null and alternative hypotheses are always the same for an ANOVA:\n\\[H_o: \\mu_1 = \\mu_2 = ...\\mu_k\\] where k is the number of groups in the data.\n\\[H_a: \\text{at least one } \\mu_k \\text{ is different}\\] Note: The F-test does not tell us which group is different or how many are different from each other. The F-test only tells us that something is different."
  },
  {
    "objectID": "Class_Notes/ANOVA_Intro.html#test-requirements",
    "href": "Class_Notes/ANOVA_Intro.html#test-requirements",
    "title": "Analysis of Variance (ANOVA)",
    "section": "",
    "text": "Just as with other statistical tests we’ve done so far, the F-test has certain requirements we must check to validate our P-values. Because of the way we calculate F, we are less concerned with the normality of the individual groups as we are with the variation within the groups.\nWhat to check:\n\nAre the standard deviations of each group “equal”?\nAre the residuals normally distributed?\n\n\n\nTo check the first requirement we can compare the biggest standard deviation to the smallest. If the ratio of the biggest to the smallest is less than 2, we conclude that the population standard deviations are “equal”.\nNOTE: Intuitively, this means that if the biggest standard deviation is more than twice as big as the smallest, then we might have cause for concern.\nThis can be checked using the standard deviations from the favstats() output (see Analysis in R below)\n\n\n\nWe will discuss residuals in more depth when we cover regression analysis. For now, think of residuals as the deviations of each observation away from their group mean. If our analysis is to be trusted, these deviations need to be normally distributed.\nIf both requirements are met, then we can trust the P-value."
  },
  {
    "objectID": "Class_Notes/ANOVA_Intro.html#analysis-in-r",
    "href": "Class_Notes/ANOVA_Intro.html#analysis-in-r",
    "title": "Analysis of Variance (ANOVA)",
    "section": "",
    "text": "Much of the syntax for ANOVA will look familiar, but we will be using a new function, aov() instead of t.test().\nThe aov() function by itself isn’t as useful as t.test(). However, we can use the summary() function to give us everything we need.\nWe typically name our output using the assignment operator &lt;- to make it easier to extract the information we would like. The inside of aov() will look familiar, using the same ~ notation we’ve used all semester.\nThe generic process for performing an ANOVA is:\n\n# Name the ANOVA output:\noutput &lt;- aov(data$response_variable ~ data$categorical_variable)\n\n# Summarise the ANOVA output to get test statistics, DF, P-value, etc:\nsummary(output)\n\n\n\n\n\nWe can use favstats() to extract the standard deviations of each group, then find the ratio of the max/min to see if it is less than 2.\n\n# extract only the standard deviations from favstats using th `$`:\n\nsds &lt;- favstats(data$response_variable ~ data$categorical_variable)$sd\n\n# Compare the max/min to 2\n\nmax(sds) / min(sds)\n\n# if max/min &lt; 2, then we're ok\n\n\n\n\nWe can assess normality of the residuals with a qqPlot(). We first need to extract the residuals from our output:\noutput &lt;- aov(data$response_variable ~ data$categorical_variable)\n\nqqPlot(output$residuals)\n\nIf most of the points fall within the blue zone, we can be confident that the residuals are normally distributed."
  },
  {
    "objectID": "Class_Notes/ANOVA_Intro.html#step-1-read-in-data",
    "href": "Class_Notes/ANOVA_Intro.html#step-1-read-in-data",
    "title": "Analysis of Variance (ANOVA)",
    "section": "Step 1: Read in data",
    "text": "Step 1: Read in data\nFor this demonstration we will be exploring the iris data. This dataset is built in to base R libraries, so we can access it without reading it in using “import”."
  },
  {
    "objectID": "Class_Notes/ANOVA_Intro.html#step-2-review-the-data",
    "href": "Class_Notes/ANOVA_Intro.html#step-2-review-the-data",
    "title": "Analysis of Variance (ANOVA)",
    "section": "Step 2: Review the data",
    "text": "Step 2: Review the data\nThe iris data contains multiple measures on flowers that might be of interest to compare across species.\nLet’s first compare sepal lengths between species."
  },
  {
    "objectID": "Class_Notes/ANOVA_Intro.html#step-2-explore-the-data",
    "href": "Class_Notes/ANOVA_Intro.html#step-2-explore-the-data",
    "title": "Analysis of Variance (ANOVA)",
    "section": "Step 2: Explore the Data",
    "text": "Step 2: Explore the Data\nHow many species do we have in our dataset?\n\ntable(iris$Species)\n\n\n    setosa versicolor  virginica \n        50         50         50 \n\n\nCreate a side-by-side boxplot of species and Sepal Length.\n\nboxplot(Sepal.Length ~ Species, data=iris, col = c(2,3,4))"
  },
  {
    "objectID": "Class_Notes/ANOVA_Intro.html#step-4-perform-the-appropriate-analysis",
    "href": "Class_Notes/ANOVA_Intro.html#step-4-perform-the-appropriate-analysis",
    "title": "Analysis of Variance (ANOVA)",
    "section": "Step 4: Perform the appropriate analysis",
    "text": "Step 4: Perform the appropriate analysis\n\naov_sepal &lt;- aov(Sepal.Length ~ Species, data=iris)\nsummary(aov_sepal)\n\n             Df Sum Sq Mean Sq F value Pr(&gt;F)    \nSpecies       2  63.21  31.606   119.3 &lt;2e-16 ***\nResiduals   147  38.96   0.265                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nBefore we make a conclusion, we want to check that we can trust our output. Every statistical test has requirements that must be satisfied if we are to trust our conclusions. For ANOVA, we need to check the normality and that the variation within groups is roughly the same.\nWe use a QQ-plot to check for normality and the ratio of the largest to the smallest standard deviation to check “equal” variation.\n\n# QQ plots show how closely the the residuals are to a normal distribution\n\nqqPlot(aov_sepal$residuals)\n\n\n\n\n\n\n\n\n[1] 107 132\n\n# Check that there is less than a 2X difference between the largest and smallest standard deviations\n\n# We can assign favstats()$sd to a variable to make it easier to use. Recall the \"$\" can also be used to extract specific output from functions\n\nsds &lt;- favstats(Sepal.Length ~ Species, data=iris)$sd\n\nmax(sds) / min(sds)\n\n[1] 1.803967\n\n\nWhat is the F-statistics?\nWhat are the between-groups degrees of freedom?\nWhat are the within-groups degrees of freedom?\nWhat is the P-value?\nWhat is your conclusion?"
  },
  {
    "objectID": "Application_Activities/AA_Who_Shot_First_Cat_Vars.html",
    "href": "Application_Activities/AA_Who_Shot_First_Cat_Vars.html",
    "title": "Categorical Data Analysis",
    "section": "",
    "text": "Use the Star Wars dataset to answer the following questions:\n\nDo less than 20% of respondents feel Very Favorably towards Emperor Palpatine? (1-sample Z test for proportion)\nWhat is the difference in proportions of females and males who are Very Favorable towards Jar-Jar Binks? (2-sample Proportion)\nCome up with one other 2-sample proportion test using anything from the Star Wars dataset.\nTest to see if income and response to “Which Character Shot First?” are Independent (Chi-square)\n\nFor the proportion tests:\n\nDefine the null and alternative hypotheses\nInclude an explanation and conclusion for hypothesis tests\nInclude Confidence intervals and a sentence explaining each\nCheck the requirements for the hypothesis test and the confidence intervals\n\nFor the Chi-square test:\n\nDefine the null and alternative hypotheses\nInclude an explanation of the conclusion for\nBe sure to check the hypothesis requirements for a test of independence"
  },
  {
    "objectID": "Application_Activities/AA_Who_Shot_First_Cat_Vars.html#explore-the-data",
    "href": "Application_Activities/AA_Who_Shot_First_Cat_Vars.html#explore-the-data",
    "title": "Categorical Data Analysis",
    "section": "Explore the data",
    "text": "Explore the data\n\nnames(sw)\n\n [1] \"Are You a Fan of SW?\"               \"Favorability_Han Solo\"             \n [3] \"Favorability_Luke Skywalker\"        \"Favorability_Princess Leia Organa\" \n [5] \"Favorability_Anakin Skywalker\"      \"Favorability_Obi Wan Kenobi\"       \n [7] \"Favorability_Emperor Palpatine\"     \"Favorability_Darth Vader\"          \n [9] \"Favorability_Lando Calrissian\"      \"Favorability_Boba Fett\"            \n[11] \"Favorability_C-3P0\"                 \"Favorability_R2 D2\"                \n[13] \"Favorability_Jar Jar Binks\"         \"Favorability_Padme Amidala\"        \n[15] \"Favorability_Yoda\"                  \"who_shot_first\"                    \n[17] \"Familiar_with_expanded_universe\"    \"are_you_a_fan_of_expanded_universe\"\n[19] \"fan_of_star_trek\"                   \"Gender\"                            \n[21] \"Age\"                                \"Household.Income\"                  \n[23] \"Education\"                          \"Location\"                          \n\ntable(sw$`Favorability_Han Solo`)\n\n\n                                            \n                                          5 \nNeither favorably nor unfavorably (neutral) \n                                         44 \n                         Somewhat favorably \n                                        151 \n                       Somewhat unfavorably \n                                          8 \n                           Unfamiliar (N/A) \n                                         15 \n                             Very favorably \n                                        610 \n                           Very unfavorably \n                                          1 \n\naddmargins(table(sw$`Favorability_Han Solo`, sw$Gender))\n\n                                             \n                                                  Female Male Sum\n                                                0      2    3   5\n  Neither favorably nor unfavorably (neutral)   1     22   21  44\n  Somewhat favorably                            4     71   76 151\n  Somewhat unfavorably                          1      3    4   8\n  Unfamiliar (N/A)                              0      9    6  15\n  Very favorably                               10    289  311 610\n  Very unfavorably                              0      0    1   1\n  Sum                                          16    396  422 834\n\naddmargins(table(sw$`Favorability_Emperor Palpatine`))\n\n\n                                            \n                                         20 \nNeither favorably nor unfavorably (neutral) \n                                        213 \n                         Somewhat favorably \n                                        143 \n                       Somewhat unfavorably \n                                         68 \n                           Unfamiliar (N/A) \n                                        156 \n                             Very favorably \n                                        110 \n                           Very unfavorably \n                                        124 \n                                        Sum \n                                        834"
  },
  {
    "objectID": "Application_Activities/AA_Who_Shot_First_Cat_Vars.html#one-sample-proportion-test",
    "href": "Application_Activities/AA_Who_Shot_First_Cat_Vars.html#one-sample-proportion-test",
    "title": "Categorical Data Analysis",
    "section": "One-sample Proportion Test",
    "text": "One-sample Proportion Test\nWhat proportion of respondents are very favorable towards Emperor Palpatine?\nIs this significantly less than 20%?"
  },
  {
    "objectID": "Application_Activities/AA_Who_Shot_First_Cat_Vars.html#two-sample-proportion-test",
    "href": "Application_Activities/AA_Who_Shot_First_Cat_Vars.html#two-sample-proportion-test",
    "title": "Categorical Data Analysis",
    "section": "Two-sample Proportion Test",
    "text": "Two-sample Proportion Test\nWhat percent of female respondents are favorable towards Jar-Jar Binks?\nWhat percent of male respondents are favorable towards Jar-Jar Binks?\nAre they significantly different?"
  },
  {
    "objectID": "Application_Activities/AA_Who_Shot_First_Cat_Vars.html#choose-your-own-adventure",
    "href": "Application_Activities/AA_Who_Shot_First_Cat_Vars.html#choose-your-own-adventure",
    "title": "Categorical Data Analysis",
    "section": "Choose your own adventure",
    "text": "Choose your own adventure\nCompare 2 proportions of your choosing and perform a prop.test()."
  },
  {
    "objectID": "Application_Activities/AA_Who_Shot_First_Cat_Vars.html#chi-square-test-for-independence",
    "href": "Application_Activities/AA_Who_Shot_First_Cat_Vars.html#chi-square-test-for-independence",
    "title": "Categorical Data Analysis",
    "section": "Chi-square Test for Independence",
    "text": "Chi-square Test for Independence\nTest to see if how you responded to the question “Who Shot First” is independent of income category.\nState your conclusion:"
  },
  {
    "objectID": "Application_Activities/AA_Unit1_Review.html",
    "href": "Application_Activities/AA_Unit1_Review.html",
    "title": "Unit 1 Review",
    "section": "",
    "text": "Apply the 3 rules of probability in different scenarios\nDescribe the 5 steps of the statistical process\nDifferentiate between an observational study and an experiment\nDifferentiate between a population and a sample\nDescribe the 4 random sampling schemes\nExplain importance of random sampling \nDistinguish between quantitative and categorical variables\nCreate graphical summaries of data (boxplots, histograms) and be able to make conclusions based on the graphs\nCreate numerical summaries of data (mean, sd, 5 number summary, percentiles) and compare groups (centers, spreads)\nDistinguish between a parameter and a statistic\nDetermine the 5 number summary from a boxplot \nState the properties of a normal distribution\nCalculate the z-score for an individual from a normal population with mean, \\(\\mu\\), and std. dev, \\(\\sigma\\)\nCalculate probabilities (&gt;, &lt;, between) of certain values given \\(\\mu\\) and \\(\\sigma\\) \nDescribe the concept of a sampling distribution of the sample mean\nState the Central Limit Theorem\nDetermine the mean of the sampling distribution of the sample mean given \\(\\mu\\) and \\(\\sigma\\)\nDetermine the std. dev. of the sampling distribution of the sample mean given \\(\\mu\\) and \\(\\sigma\\)\nDetermine the shape of the sampling distribution given a sample size, n, and \\(\\mu\\) and \\(\\sigma\\)\nState the Law of Large Numbers"
  },
  {
    "objectID": "Application_Activities/AA_Unit1_Review.html#housing-prices-in-rexburg",
    "href": "Application_Activities/AA_Unit1_Review.html#housing-prices-in-rexburg",
    "title": "Unit 1 Review",
    "section": "Housing Prices in Rexburg",
    "text": "Housing Prices in Rexburg\nYou are interested housing prices in Rexburg, Idaho. Load the housing data and respond to the following questions.\n\n# Load the data:\nhousing &lt;- import('https://github.com/byuistats/Math221D_Cannon/raw/master/Data/MadisonCountyRealEstate.xlsx') %&gt;% select(ListPrice, Bedrooms)\n\nQ2.1:\nCreate a histogram of listing price.\n\nfavstats(housing$ListPrice ~ housing$Bedrooms)\n\n  housing$Bedrooms    min     Q1 median      Q3     max     mean        sd  n\n1                2  96900 108175 117450  129675  160000 119984.6  15206.23 26\n2                3 109900 142150 184700  213650  499999 199120.8  79099.36 48\n3                4 119900 157000 174900  228000  369000 200642.1  71075.72 19\n4                5 137000 189900 219900  275000  924500 250483.8 131359.88 37\n5                6 179900 281500 319000  364000  699000 340740.0 118103.19 15\n6                7 199000 454675 717450 1040000 1475000 777225.0 545108.00  4\n  missing\n1       0\n2       0\n3       0\n4       0\n5       0\n6       0\n\n\nWhat is the shape of the distribution of list prices for homes in Rexburg.\nQ2.2:\nCalculate the mean, standard deviation, 5 number summary and sample size of housing prices for each number of bedrooms.\n\nboxplot(housing$ListPrice ~ housing$Bedrooms, col=c(2,3,4,5,6,7), xlab = \"Number of Bedrooms\", ylab = \"Housing Price\", main = \"blah\")\n\n\n\n\n\n\n\n\nbold italics\nWhat is the standard deviation of homes with 3 bedrooms?\nHow do you interpret that number?\nQ2.3:\nWhat is the maximum value home price for a 2 bedroom house?\nSuppose there was a typo in the data and the maximum price for a 2 bedroom house was accidentally entered as $1,700,000.\nHow would the median change?\nHow would the mean change?\nHow would the standard deviation change?\nQ2.4:\nCreate a side-by-side boxplot of housing prices for each group. Modify the color scheme so that each boxplot is a different color.\nWhat trends do you notice?\nWhy might the box plot for 7 bedroom homes look so different than the others?"
  },
  {
    "objectID": "Application_Activities/AA_Unit1_Review.html#hair-color",
    "href": "Application_Activities/AA_Unit1_Review.html#hair-color",
    "title": "Unit 1 Review",
    "section": "Hair Color",
    "text": "Hair Color\nBrother Cannon has been collecting hair color data about his students for the last several semesters. The proportion of students with each hair color are as follows:\n(run the following R chunk to see the table more clearly)\n\nknitr::kable(tibble(Red=\"???\", Brown=.37, Black=.35, Blond = .22, Other=.02))\n\n\n\n\nRed\nBrown\nBlack\nBlond\nOther\n\n\n\n\n???\n0.37\n0.35\n0.22\n0.02\n\n\n\n\n\nQ3.1:\nWhat percent of Brother Cannon’s students have red hair?\nQ3.2:\nWhat’s the probability that a randomly selected student has either black or brown hair?"
  },
  {
    "objectID": "Application_Activities/AA_Unit1_Review.html#social-media-use",
    "href": "Application_Activities/AA_Unit1_Review.html#social-media-use",
    "title": "Unit 1 Review",
    "section": "Social Media Use",
    "text": "Social Media Use\nQ4.1:\nThe number of hours students spend on social media a week is known to be left skewed with a mean of 12 hours and a standard deviation 1.5.\nSuppose we take a random sample of 50 students and calculate the average time they spend on social media.\nQ4.2:\nWhat is the mean of the distribution of sample means for this sample size?\nQ4.3:\nWhat is the standard deviation of sample means for this sample size?\n\n1.5 / sqrt(50)\n\n[1] 0.212132\n\n\nQ4.4:\nWhat is the shape of the distribution of sample means for this sample size and why?"
  },
  {
    "objectID": "Application_Activities/AA_Unit1_Review.html#faculty-salaries",
    "href": "Application_Activities/AA_Unit1_Review.html#faculty-salaries",
    "title": "Unit 1 Review",
    "section": "Faculty Salaries",
    "text": "Faculty Salaries\nFaculty salaries at a university are known to be normally distributed with a mean of $108,552 and a standard deviation of $13,277.\nYou randomly sample n=15 faculty members and calculate their average salary.\nQ5.1:\nWhat is the mean of the distribution of sample means for this sample size?\nQ5.2:\nWhat is the standard deviation of sample means for this sample size?\n\n13277/sqrt(15)\n\n[1] 3428.107\n\n\nQ5.3:\nWhat is the shape of the distribution of sample means for this sample size and why?"
  },
  {
    "objectID": "Application_Activities/AA_Unit1_Review.html#normal-probability-calculations",
    "href": "Application_Activities/AA_Unit1_Review.html#normal-probability-calculations",
    "title": "Unit 1 Review",
    "section": "Normal Probability Calculations",
    "text": "Normal Probability Calculations\nHigh blood pressure is strongly associated with a host of diseases and increases risk of cardiac arrest (hear attacks) in older patients.\nSuppose that systolic blood pressure in the US is normally distributed with a mean = 128 and a standard deviation of 11.\nUse the following calculator to answer the questions below:\nQ6.1:\nWhat is the probability that a randomly selected person in the US has a systolic blood pressure of greater than 145?\nQ6.2\nWhat is the probability that a randomly selected group of 10 people in the US have an average blood pressure less than 130?\nQ6.3:\nYour doctor says you’re in great shape, and that you are in the 20th percentile for systolic blood pressure.\nWhat does it mean that you are the 20th percentile?\nWhat is your blood pressure?"
  },
  {
    "objectID": "Application_Activities/AA_Data_Summaries.html",
    "href": "Application_Activities/AA_Data_Summaries.html",
    "title": "Summarizing Data",
    "section": "",
    "text": "This example is taken from an experiment listed in the R help files under ?CO2.\n“An experiment on the cold tolerance of the grass species Echinochloa crus-galli was conducted. The CO2 uptake of six plants from Quebec and six plants from Mississippi was measured at several levels of ambient CO2 concentration. Half the plants of each type were chilled overnight before the experiment was conducted.” Plants were considered tolerant to the cold if they were still able to achieve high CO2 uptake values after being chilled.\nIgnoring the Plant ID for a moment, there are three factors that possibly effect the uptake of a plant. These include Type, Treatment, and conc. The factor Type has two levels, Quebec and Mississippi. The factor Treatment has two levels chilled and nonchilled and the factor conc has seven levels 95, 175, 250, 350, 500, 675, and 1000.\nConc is related to the ambient concentration of CO2."
  },
  {
    "objectID": "Application_Activities/AA_Data_Summaries.html#univariate-analysis",
    "href": "Application_Activities/AA_Data_Summaries.html#univariate-analysis",
    "title": "Summarizing Data",
    "section": "Univariate Analysis",
    "text": "Univariate Analysis\nCreate a table with the overall summary statistics for uptake:\n\n# This is NEW!  if you tell R to knit the favstats output using the \"kable\" function, it will look a lot nicer when the document is rendered.\n\nknitr::kable(favstats(CO2$uptake))\n\n\n\n\n\nmin\nQ1\nmedian\nQ3\nmax\nmean\nsd\nn\nmissing\n\n\n\n\n\n7.7\n17.9\n28.3\n37.125\n45.5\n27.2131\n10.81441\n84\n0\n\n\n\n\n\nCreate a histogram of the uptake data:"
  },
  {
    "objectID": "Application_Activities/AA_Data_Summaries.html#impact-of-plant-type-on-uptake",
    "href": "Application_Activities/AA_Data_Summaries.html#impact-of-plant-type-on-uptake",
    "title": "Summarizing Data",
    "section": "Impact of Plant Type on Uptake",
    "text": "Impact of Plant Type on Uptake\nCreate a table of summary statistics of uptake for each type of plants used in the experiment.\nCreate a side-by-side boxplot of uptake for each type of plants used in the experiment."
  },
  {
    "objectID": "Application_Activities/AA_Data_Summaries.html#impact-of-treatment-on-uptake",
    "href": "Application_Activities/AA_Data_Summaries.html#impact-of-treatment-on-uptake",
    "title": "Summarizing Data",
    "section": "Impact of Treatment on Uptake",
    "text": "Impact of Treatment on Uptake\nCreate a table of summary statistics of uptake for each treatment used in the experiment.\nCreate a side-by-side boxplot of uptake for each treatment used in the experiment."
  },
  {
    "objectID": "Application_Activities/AA_Data_Summaries.html#impact-of-ambient-co2-concentration-on-uptake",
    "href": "Application_Activities/AA_Data_Summaries.html#impact-of-ambient-co2-concentration-on-uptake",
    "title": "Summarizing Data",
    "section": "Impact of Ambient CO2 Concentration on Uptake",
    "text": "Impact of Ambient CO2 Concentration on Uptake\nCreate a table of summary statistics of uptake for each level of CO2 found in the experiment.\nCreate a side-by-side boxplot of uptake for each level of CO2 found used in the experiment."
  },
  {
    "objectID": "Application_Activities/AA_Data_Summaries.html#what-makes-this-an-experiment",
    "href": "Application_Activities/AA_Data_Summaries.html#what-makes-this-an-experiment",
    "title": "Summarizing Data",
    "section": "What makes this an experiment?",
    "text": "What makes this an experiment?\nThe background suggests this study was an experiment. In your own words, describe the difference between an experiment and an observational study.\nWhat would this study have looked like if it had been an observational study?"
  },
  {
    "objectID": "Application_Activities/AA_Data_Summaries.html#what-is-the-population-of-this-study",
    "href": "Application_Activities/AA_Data_Summaries.html#what-is-the-population-of-this-study",
    "title": "Summarizing Data",
    "section": "What is the Population of this study?",
    "text": "What is the Population of this study?\nDescribe in your own words what the population of this study could be. How broad can we make our conclusions?"
  },
  {
    "objectID": "Application_Activities/AA_Data_Summaries.html#what-are-your-recommendations",
    "href": "Application_Activities/AA_Data_Summaries.html#what-are-your-recommendations",
    "title": "Summarizing Data",
    "section": "What are your recommendations?",
    "text": "What are your recommendations?\nIf you were to recommend a planting scenario to maximize CO2 uptake, what would you recommend based on the summaries provided?"
  },
  {
    "objectID": "Application_Activities/AA_Hypothesis_Conf_Int.html",
    "href": "Application_Activities/AA_Hypothesis_Conf_Int.html",
    "title": "Hypothesis Tests and Confidence Intervals",
    "section": "",
    "text": "In this activity, you will execute statistical hypothesis tests and generate confidence intervals for each of the Big 5 personality traits using data collected from a random sample of Brother Cannon’s Math 221 students.\nBegin by describing the population of this analysis. This will determine how broad our conclusions can be. Also include a table of the summary statistics and any visualizations that might tell a story about the data.\nThen, for each personality trait, include:\n\nA statement of the null and alternative hypotheses and why you chose the alternative you did.\nChoose alpha $= $\nCheck that your test requirements are satisfied (n &gt; 30 or qqPlot(respons_variable))\nRun the one-sample t-test and state your conclusion\nCalculate a \\(1-\\alpha\\) level confidence interval and describe in words what it means\n\nRecall that we can use “favstats()” to get summary statistics, boxplot() and histogram() to get visualizations, and the t.test() function to get hypothesis tests and confidence intervals. Be sure to label your plots’ axes and include a title.\n\n# Load Libraries\n\nlibrary(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.3.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(mosaic)\n\nRegistered S3 method overwritten by 'mosaic':\n  method                           from   \n  fortify.SpatialPolygonsDataFrame ggplot2\n\nThe 'mosaic' package masks several functions from core packages in order to add \nadditional features.  The original behavior of these functions should not be affected by this.\n\nAttaching package: 'mosaic'\n\nThe following object is masked from 'package:Matrix':\n\n    mean\n\nThe following objects are masked from 'package:dplyr':\n\n    count, do, tally\n\nThe following object is masked from 'package:purrr':\n\n    cross\n\nThe following object is masked from 'package:ggplot2':\n\n    stat\n\nThe following objects are masked from 'package:stats':\n\n    binom.test, cor, cor.test, cov, fivenum, IQR, median, prop.test,\n    quantile, sd, t.test, var\n\nThe following objects are masked from 'package:base':\n\n    max, mean, min, prod, range, sample, sum\n\nlibrary(rio)\n\n\nAttaching package: 'rio'\n\nThe following object is masked from 'package:mosaic':\n\n    factorize\n\nlibrary(car)\n\nWarning: package 'car' was built under R version 4.3.3\n\n\nLoading required package: carData\n\nAttaching package: 'car'\n\nThe following objects are masked from 'package:mosaic':\n\n    deltaMethod, logit\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\nThe following object is masked from 'package:purrr':\n\n    some\n\n# Load Data\nbig5 &lt;- import('https://raw.githubusercontent.com/byuistats/Math221D_Cannon/master/Data/All_class_combined_personality_data.csv')"
  },
  {
    "objectID": "Application_Activities/AA_Hypothesis_Conf_Int.html#conclusion",
    "href": "Application_Activities/AA_Hypothesis_Conf_Int.html#conclusion",
    "title": "Hypothesis Tests and Confidence Intervals",
    "section": "Conclusion",
    "text": "Conclusion\n[Write a concluding paragraph that states your conclusion about the hypothesis test and includes an interpretation of the confidence interval in context of the research question.]"
  },
  {
    "objectID": "Application_Activities/AA_Hypothesis_Conf_Int.html#conclusion-1",
    "href": "Application_Activities/AA_Hypothesis_Conf_Int.html#conclusion-1",
    "title": "Hypothesis Tests and Confidence Intervals",
    "section": "Conclusion",
    "text": "Conclusion\n[Write a concluding paragraph that states your conclusion about the hypothesis test and includes an interpretation of the confidence interval in context of the research question.]"
  },
  {
    "objectID": "Application_Activities/AA_Hypothesis_Conf_Int.html#conclusion-2",
    "href": "Application_Activities/AA_Hypothesis_Conf_Int.html#conclusion-2",
    "title": "Hypothesis Tests and Confidence Intervals",
    "section": "Conclusion",
    "text": "Conclusion\n[Write a concluding paragraph that states your conclusion about the hypothesis test and includes an interpretation of the confidence interval in context of the research question.]"
  },
  {
    "objectID": "Application_Activities/AA_Hypothesis_Conf_Int.html#conclusion-3",
    "href": "Application_Activities/AA_Hypothesis_Conf_Int.html#conclusion-3",
    "title": "Hypothesis Tests and Confidence Intervals",
    "section": "Conclusion",
    "text": "Conclusion\n[Write a concluding paragraph that states your conclusion about the hypothesis test and includes an interpretation of the confidence interval in context of the research question.]"
  },
  {
    "objectID": "Application_Activities/AA_Hypothesis_Conf_Int.html#conclusion-4",
    "href": "Application_Activities/AA_Hypothesis_Conf_Int.html#conclusion-4",
    "title": "Hypothesis Tests and Confidence Intervals",
    "section": "Conclusion",
    "text": "Conclusion\n[Write a concluding paragraph that states your conclusion about the hypothesis test and includes an interpretation of the confidence interval in context of the research question.]"
  },
  {
    "objectID": "Application_Activities/AA_Unit2_Review.html",
    "href": "Application_Activities/AA_Unit2_Review.html",
    "title": "Unit 2 Review",
    "section": "",
    "text": "In this activity, you will use everything we’ve covered in Unit 2 including:\n\nData manipulation using tidyverse functions\nHypothesis tests\n\n1-Sample t-test\n2-sample dependent t-test\n2-sample independent t-test\nANOVA\n\nConfidence Intervals where applicable\n\nWe will be using data collected about students in 2 Portuguese schools including their final grade. The goal is to answer research questions using statistical methods to see what factors significantly impact final grades.\n\n\nIn class, we have reinforced a process for approaching a new dataset. The following is a summary of activities that help us conduct good research:\n\nRead in the data\n\nExplore the dataset as a whole:\n\nWhat are the column names? What do they mean? Where can I find information about them?\nWhat is the response/dependent variable? Could there be more than one?\nWhat are some factors that may impact the response variable? Which are likely the most important?\n\nExplore specific columns\n\nStart with the response variable. Are there any outliers? Obtain summary statistics (favstats()), visualize the data (histogram(), boxplot()).\nExplore the explanatory variables you think are most impact to the response variable. What type of data are they (categorical, quantitative)? For categorical variables, what are all the levels (unique())\n\nFormalize statistical hypotheses. If your factors are categorical, how many groups will you be comparing? Is it a 1-sample t-test, 2-sample t-test, ANOVA?\nPrepare data for analysis. You may need to clean the data (eg. data %&gt;% filter() %&gt;% select())\nPerform the appropriate analysis (t.test(), aov())\n\nAll these activities are important, but we may spend more or less time on any one of them depending on the state of the data."
  },
  {
    "objectID": "Application_Activities/AA_Unit2_Review.html#getting-to-know-a-new-dataset",
    "href": "Application_Activities/AA_Unit2_Review.html#getting-to-know-a-new-dataset",
    "title": "Unit 2 Review",
    "section": "",
    "text": "In class, we have reinforced a process for approaching a new dataset. The following is a summary of activities that help us conduct good research:\n\nRead in the data\n\nExplore the dataset as a whole:\n\nWhat are the column names? What do they mean? Where can I find information about them?\nWhat is the response/dependent variable? Could there be more than one?\nWhat are some factors that may impact the response variable? Which are likely the most important?\n\nExplore specific columns\n\nStart with the response variable. Are there any outliers? Obtain summary statistics (favstats()), visualize the data (histogram(), boxplot()).\nExplore the explanatory variables you think are most impact to the response variable. What type of data are they (categorical, quantitative)? For categorical variables, what are all the levels (unique())\n\nFormalize statistical hypotheses. If your factors are categorical, how many groups will you be comparing? Is it a 1-sample t-test, 2-sample t-test, ANOVA?\nPrepare data for analysis. You may need to clean the data (eg. data %&gt;% filter() %&gt;% select())\nPerform the appropriate analysis (t.test(), aov())\n\nAll these activities are important, but we may spend more or less time on any one of them depending on the state of the data."
  },
  {
    "objectID": "Application_Activities/AA_Unit2_Review.html#categories-labeled-as-numbers",
    "href": "Application_Activities/AA_Unit2_Review.html#categories-labeled-as-numbers",
    "title": "Unit 2 Review",
    "section": "Categories Labeled as Numbers",
    "text": "Categories Labeled as Numbers\nSometimes even correct data can have issues that must be addressed. For example, categories are often labeled as numbers. Software can’t guess when numbers are supposed to be categories, so we have to tell R when a number should be treated as a category.\nTo force a variable to be a category, we use the factor() function in R. We can change the variable type in the data itself or change it in the analysis. We demonstrate both methods below.\n\nChanging a Column Type in a dataset\nFather’s education, Fedu, shows up as a number in R. The website suggests that the numbers represent categories (0 = none, 1 = primary education (4th grade), 2 = 5th to 9th grade, 3 = secondary education or 4 = higher education).\nTo change the data type in the data itself, we can use a mutate statement in the following manner:\n\n# Create a new dataset called fedu_data that begins with the clean data and adds a column that we called Fedu_factor, which is the factorized column, Fedu:\n\nnew_data &lt;- student %&gt;%\n  mutate(Fedu_factor = factor(Fedu))\n\n# Check the column names of the new dataset.  Notice the new column\nnames(new_data)\n\n# glimpse() shows us data types.  Notice after Fedu_factor, the &lt;fct&gt;, which shows us that this is in fact, a factor variable type.  &lt;dbl&gt; stands for \"double\" and is a numeric variable type\n\nglimpse(new_data)\n\n\n\nChanging the Variable Type “on the fly”\nYou may not want to bother changing all the variable types for each potential analysis. Fortunately, you can create a factor “on the fly” within the analysis function itself.\nBecause there are more than 2 levels of Father’s Education, I will demonstrate how this is done in an ANOVA:\n\n# Force Fedu to be treated like a category in ANOVA:\nfedu_anova &lt;- aov(student$G3 ~ factor(student$Fedu))\n\nsummary(fedu_anova)\n\n                      Df Sum Sq Mean Sq F value Pr(&gt;F)  \nfactor(student$Fedu)   4    238   59.53   2.891 0.0222 *\nResiduals            390   8032   20.59                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThis works with most analysis functions including t.test() and aov().\nNOTE: You only have to do this for variables in a dataset that are categories labeled as numbers. If the categories are text, t.test() and aov() automatically recognizes the variable as categorical. However, it does no harm to put a column with text into a factor() statement."
  },
  {
    "objectID": "Application_Activities/AA_Unit2_Review.html#cleaning-the-data",
    "href": "Application_Activities/AA_Unit2_Review.html#cleaning-the-data",
    "title": "Unit 2 Review",
    "section": "Cleaning the Data",
    "text": "Cleaning the Data\nWhile exploring the data, you may have noticed a few students ended up with a final grade of zero. While it may be interesting to explore what factors lead to an incomplete grade, we want to make conclusions about students who completed the course.\nCreate a clean dataset called, clean, that excludes zeros for G3. This will be used for the following analyses."
  },
  {
    "objectID": "Application_Activities/AA_Unit2_Review.html#comparing-schools",
    "href": "Application_Activities/AA_Unit2_Review.html#comparing-schools",
    "title": "Unit 2 Review",
    "section": "Comparing Schools",
    "text": "Comparing Schools\nSuppose the Gabriel Pereira school (GP) has more stringent admissions requirements. We suspect this would lead to higher grades, on average.\nCreate a side-by-side boxplot of the final grades for each school. Change the y-axis label to read “Final Grade out of 20”, the x-axis label to read “School”, and add a title.\nWhat do you notice?\nCreate a table of summary statistics of final grade for each school:\n\nHypothesis Test\nCreate a qqPlot to look at the normality of both groups:\nDo the grades look normally distributed for both groups? If not, should we be concerned? Can we trust the P-value?\nState your null and alternative hypotheses and significance level.\nNOTE: Recall that R uses alphabetical order to determine which group is the reference group. It is useful to put this group on the left side of the null hypothesis and set your alternative hypothesis accordingly.\nHo:\nHa:\n\\(\\alpha = 0.\\)\nWhat is the P-value?\nWhat is your conclusion in context of the research question?\n\n\nConfidence Interval\nCreate a \\((1-\\alpha)\\)% confidence interval and explain it in context of the research question."
  },
  {
    "objectID": "Application_Activities/AA_Unit2_Review.html#comparing-second-period-grade-with-final-grade",
    "href": "Application_Activities/AA_Unit2_Review.html#comparing-second-period-grade-with-final-grade",
    "title": "Unit 2 Review",
    "section": "Comparing Second Period Grade with Final Grade",
    "text": "Comparing Second Period Grade with Final Grade\nWe suspect there is a difference between the second period and the final grade, though we do not know if they go up or down. Carry out a hypothesis test to evaluate this suspicion.\n\nHypothesis Test\nChoose how you will define the difference between final grade and second period grade, and create a new object called diff:\n\n# diff &lt;- \n\nWhat does a negative number mean?\nCreate a qqPlot of diff and check for normality:\nDo the grade differences look normally distributed? If not, should we be concerned? Can we trust the P-value?\nState your null and alternative hypothesis and choose a significance level:\nHo:\nHa:\n\\(\\alpha = 0.\\)\nPerform the appropriate analysis.\nWhat is the P-value?\nWhat conclusion do you make in context of this research question?\n\n\nConfidence Interval\nCreate a \\((1-\\alpha)\\)% confidence interval for the differences and explain it in context of the research question."
  },
  {
    "objectID": "Application_Activities/AA_Unit2_Review.html#absenteeism-in-portugal",
    "href": "Application_Activities/AA_Unit2_Review.html#absenteeism-in-portugal",
    "title": "Unit 2 Review",
    "section": "Absenteeism in Portugal",
    "text": "Absenteeism in Portugal\nIn 2021, Portugal reported having 0% absenteeism for 15-year-olds. We suspect that the actual absenteeism is higher than the reported value (zero).\n\nHypothesis Test\nCreate a qqPlot for absences.\nDo absences look normally distributed? If not, should we be concerned? Can we trust the P-value?\nState your null and alternative hypotheses and choose a significance level:\nHo:\nHa:\n\\(\\alpha = 0.\\)\nPerform the appropriate analysis.\nState your conclusion in context of the research question.\n\n\nConfidence Interval\nCreate a \\((1-\\alpha)\\)% confidence interval for average absences and interpret it in context of the problem."
  },
  {
    "objectID": "Application_Activities/AA_Unit2_Review.html#the-impact-of-mothers-education-level",
    "href": "Application_Activities/AA_Unit2_Review.html#the-impact-of-mothers-education-level",
    "title": "Unit 2 Review",
    "section": "The Impact of Mother’s Education Level",
    "text": "The Impact of Mother’s Education Level\nThe level of education of the mother in the home is thought to have a significant impact on student success.\nCreate a side-by-side boxplot of final grades for each level of mother’s education.\nCreate a table of summary statistics of final grades for each level of mother’s education.\nHow many respondents have a mother with no formal education (level 0)?\nCreate a new dataset, clean_medu, that does not include mother’s education level 0.\n\n# clean_medu &lt;- clean %&gt;%\n\nCreate another boxplot with the new dataset that excludes level 0.\nCreate a summary table of final grades for each level of a mother’s education with the new dataset.\nWhat is the maximum standard deviation?\nWhat is the minimum standard deviation?\nVerify that the maximum is less than twice as large as the minimum to check the “equality of standard deviations”.\n\nHypothesis Test\nState your null and alternative hypotheses and pick alpha:\nHo:\nHa:\n\\(\\alpha = 0.\\)\nPerform the appropriate statistical test.\nWhat is the test statistic?\nWhat is the P-value?\nCheck the normality of the residuals.\nDo the residuals appear roughly normally distributed?\nCan we trust the P-value.\nState your conclusion."
  },
  {
    "objectID": "Application_Activities/AA_Unit2_Review.html#choose-your-own-adventure",
    "href": "Application_Activities/AA_Unit2_Review.html#choose-your-own-adventure",
    "title": "Unit 2 Review",
    "section": "Choose your own adventure",
    "text": "Choose your own adventure\nPick another variable that was not analyzed above.\nCreate a side-by-side boxplot. Be sure to properly label the graph and add sufficient information so readers can know what they are looking at without having to search through the report or code.\nPerform the appropriate analysis. Be sure to include a concise conclusion in the context of the research question, including a hypothesis test (and confidence interval if applicable.)"
  },
  {
    "objectID": "Application_Activities/Semester_Project_Instructions.html",
    "href": "Application_Activities/Semester_Project_Instructions.html",
    "title": "Semester Project Instructions",
    "section": "",
    "text": "For this project, you will create an html report that is an original analysis based on data that you find.\nStart with a research question that interests you. It can be about any topic (finance, music, video games, hunting, weather, mental health, AI…seriously anything!)\nFor this project, a good research question will be interesting to you AND feasible to find available data. You can certainly think of exciting research questions for which the data are impossible to find or collect. Expect to refine your research question as you begin the data search.\nOnce you have a research question in mind, start looking for data relating to it. Below are some links that might be helpful for finding datasets.\n\n\nGoogle actually has a search engine specifically for datasets\nStatista has a datasets for a wide range of topics but is particularly well suited for government policy-related data such as health, crime, social science.\nKaggle runs competitions for companies who outsource data challenges. It has also compiled a large library of datasets on a range of topics. Because businesses run competitions through here, there are a lot of datasets related to specific challenges that businesses face."
  },
  {
    "objectID": "Application_Activities/Semester_Project_Instructions.html#helpful-links",
    "href": "Application_Activities/Semester_Project_Instructions.html#helpful-links",
    "title": "Semester Project Instructions",
    "section": "",
    "text": "Google actually has a search engine specifically for datasets\nStatista has a datasets for a wide range of topics but is particularly well suited for government policy-related data such as health, crime, social science.\nKaggle runs competitions for companies who outsource data challenges. It has also compiled a large library of datasets on a range of topics. Because businesses run competitions through here, there are a lot of datasets related to specific challenges that businesses face."
  },
  {
    "objectID": "Application_Activities/Semester_Project_Instructions.html#define-the-problem",
    "href": "Application_Activities/Semester_Project_Instructions.html#define-the-problem",
    "title": "Semester Project Instructions",
    "section": "1. Define the Problem",
    "text": "1. Define the Problem\nInclude an introduction that describes why you were interested in the topic and what you envisioned for an analysis. Some questions to address:\n\nWhat is the population of your research?\nWhat do you think is the nature of the relationship you hope to discover?\nWhat type of data are you looking for (quantitative? categorical?)\n\nWhat type of analysis are you expecting to do (t-test, regression? ANOVA? Chi-square test for independence? etc.)"
  },
  {
    "objectID": "Application_Activities/Semester_Project_Instructions.html#collect-the-data",
    "href": "Application_Activities/Semester_Project_Instructions.html#collect-the-data",
    "title": "Semester Project Instructions",
    "section": "2. Collect the Data",
    "text": "2. Collect the Data\nTalk about the process of how you found the data and whether you had to make adjustments to your research question."
  },
  {
    "objectID": "Application_Activities/Semester_Project_Instructions.html#describe-the-data",
    "href": "Application_Activities/Semester_Project_Instructions.html#describe-the-data",
    "title": "Semester Project Instructions",
    "section": "3. Describe the Data",
    "text": "3. Describe the Data\nInclude summary statistics (favstats, proportions, contingency tables, etc.)\nInclude GGPlot visualizations (boxplots, histograms/density plots, bar charts, etc.). Make the charts as understandable as possible without having to read the text description. That means make sure:\n\nAxes are labeled appropriately\nIt has a descriptive title\nLabel lines or points that are highlighted in the graph"
  },
  {
    "objectID": "Application_Activities/Semester_Project_Instructions.html#analyze-the-data",
    "href": "Application_Activities/Semester_Project_Instructions.html#analyze-the-data",
    "title": "Semester Project Instructions",
    "section": "4. Analyze the Data",
    "text": "4. Analyze the Data\nPerform the appropriate analysis for the data collected (t-test, F-test, regression, Chi-square, etc.)"
  },
  {
    "objectID": "Application_Activities/Semester_Project_Instructions.html#take-action",
    "href": "Application_Activities/Semester_Project_Instructions.html#take-action",
    "title": "Semester Project Instructions",
    "section": "5. Take Action",
    "text": "5. Take Action\nBased on your statistical inference, what recommendations or actions would you take?"
  },
  {
    "objectID": "Class_Notes/Assessing_Normality.html",
    "href": "Class_Notes/Assessing_Normality.html",
    "title": "Assessing Normality",
    "section": "",
    "text": "Assessing Normality\nWhen we use either the normal distribution or the t-distribution, we must confirm that the distribution of sample means is normally distributed. This is true when:\n\nThe population is normally distributed\n\\(n &gt; 30\\) because of the Central Limit Theorem\n\nBut how do you know if a population is normally distributed? In the real world, there is no teacher to tell you when to assume a population is normal.\nIf our sample size is large enough, we don’t have to worry. We can trust the Central Limit Theorem.\nIf our sample size is &lt; 30, we can assess the normality of our sample to decide if we can still trust output of our hypothesis tests and confidence intervals.\nPreviously, we’ve used histograms to help understand the distribution of a sample. However, when sample sizes are small, even samples from a standard normal distribution can look skewed.\nAll of the examples below are examples of random samples from actual normal distributions:\n\n\nA New Way to Assess Normality\nStatisticians use something called a QQPlot which works better at assessing normality. QQPlots plot the sorted data of each point in a dataset with the theoretical percentile from a normal distribution. If the data and theoretical percentiles line up, then we can be reasonably sure the population is normally distributed.\nThese are easier to use than to explain. We use the car library and the function qqPlot() to create a chart.\nKey Point: If most of the data points line up in the shaded region, we can consider the population as normally distributed.\nBelow are examples of QQPlots for a normal distribution and a right skewed distribution.\n\n\nThese work much better for small sample sizes. Below are several examples of QQPlots for small sample sizes:\n\nWhile not perfect, these are a vastly better tool to assess normality than a histogram.\n\n\n\nPractice\nLet’s try assessing the normality of some data. Below are 3 datasets. Find the response variable(s) from each and determine if the data are sufficiently normally distributed:\n\nlibrary(rio)\nlibrary(tidyverse)\nlibrary(car)\n\nold_faithful &lt;- import('https://github.com/byuistats/Math221D_Cannon/raw/master/Data/OldFaithful.xlsx')\n\nqqPlot(old_faithful$Duration)\n\n\n\n\n\n\n\n\n[1] 19 58\n\nqqPlot(old_faithful$Wait)\n\n\n\n\n\n\n\n\n[1] 265 127\n\nrent &lt;- import('https://github.com/byuistats/Math221D_Cannon/raw/master/Data/Rent.csv')\n\n\nmcat_gpa &lt;- import('https://github.com/byuistats/Math221D_Cannon/raw/master/Data/mcat_gpa.csv')"
  },
  {
    "objectID": "Class_Notes/Chi_Square_Intro.html",
    "href": "Class_Notes/Chi_Square_Intro.html",
    "title": "Chi-square Test of Independence",
    "section": "",
    "text": "When working through statistical inference for means, we progressed from learning 1-sample t-tests to 2-sample t-tests. When we wanted to compare a quantitative variable between multiple groups, we introduced ANOVA. The hypothesis test changed and we introduced the F-statistic.\nRecall that the F-statistics was based on a ratio of squared quantities and was therefore always positive and skewed right.\nSimilarly, when we want to compare a categorical variable across multiple groups, we must modify the hypothesis test from the 2-sample proportion z-test and introduce a new test statistic: \\(\\chi^2\\). The Greek letter, \\(\\chi\\), is pronounced like “ki” in “kite”, not like “chi” in “tai chi”.\nAs can be seen from its name, \\(\\chi^2\\) is a squared value and is thus always positive and right skewed like the F-statistic.\n\n\nThe \\(\\chi^2\\) distribution also has degrees of freedom that determine its shape.\n\\[df = (r-1)*(c-1)\\] Where r is the number of rows and c is the number of columns in a summary table.\n\n\n\nThe null and alternative hypotheses test for \\(\\chi^2\\) test for independence are always the same.\n\\[H_0: \\text{The row variable is independent of the column variable}\\] \\[H_A: \\text{The row variable is not independent of the column variable}\\] While not a fan of the double negative, it serves a technical purpose. Mathematically, we get the same test statistic and p-value if we swap rows and columns. We cannot say the row variable depends on the column variable without also saying that the column variable depends on the row variable.\nThink of Alice at the Mad Hatter’s tea party:\n\n“Then you should say what you mean,” the March Hare went on. “I do,” Alice hastily replied; “at least-at least I mean what I say-that’s the same thing, you know.”\n“Not the same thing a bit!” said the Hatter. “Why, you might just as well say that ‘I see what I eat’ is the same thing as ‘I eat what I see’!”\n“You might just as well say,” added the March Hare, “that ‘I like what I get’ is the same thing as ‘I get what I like’!”\n“You might just as well say,” added the Dormouse, which seemed to be talking in its sleep, “that ‘I breathe when I sleep’ is the same thing as ‘I sleep when I breathe’!”\n“It is the same thing with you.” said the Hatter,”\n\nSo we are resigned to conclude that we have sufficient/insufficient evidence that they are not independent."
  },
  {
    "objectID": "Class_Notes/Chi_Square_Intro.html#degrees-of-freedom",
    "href": "Class_Notes/Chi_Square_Intro.html#degrees-of-freedom",
    "title": "Chi-square Test of Independence",
    "section": "",
    "text": "The \\(\\chi^2\\) distribution also has degrees of freedom that determine its shape.\n\\[df = (r-1)*(c-1)\\] Where r is the number of rows and c is the number of columns in a summary table."
  },
  {
    "objectID": "Class_Notes/Chi_Square_Intro.html#hypothesis-test",
    "href": "Class_Notes/Chi_Square_Intro.html#hypothesis-test",
    "title": "Chi-square Test of Independence",
    "section": "",
    "text": "The null and alternative hypotheses test for \\(\\chi^2\\) test for independence are always the same.\n\\[H_0: \\text{The row variable is independent of the column variable}\\] \\[H_A: \\text{The row variable is not independent of the column variable}\\] While not a fan of the double negative, it serves a technical purpose. Mathematically, we get the same test statistic and p-value if we swap rows and columns. We cannot say the row variable depends on the column variable without also saying that the column variable depends on the row variable.\nThink of Alice at the Mad Hatter’s tea party:\n\n“Then you should say what you mean,” the March Hare went on. “I do,” Alice hastily replied; “at least-at least I mean what I say-that’s the same thing, you know.”\n“Not the same thing a bit!” said the Hatter. “Why, you might just as well say that ‘I see what I eat’ is the same thing as ‘I eat what I see’!”\n“You might just as well say,” added the March Hare, “that ‘I like what I get’ is the same thing as ‘I get what I like’!”\n“You might just as well say,” added the Dormouse, which seemed to be talking in its sleep, “that ‘I breathe when I sleep’ is the same thing as ‘I sleep when I breathe’!”\n“It is the same thing with you.” said the Hatter,”\n\nSo we are resigned to conclude that we have sufficient/insufficient evidence that they are not independent."
  },
  {
    "objectID": "Class_Notes/Chi_Square_Intro.html#test-requirements",
    "href": "Class_Notes/Chi_Square_Intro.html#test-requirements",
    "title": "Chi-square Test of Independence",
    "section": "Test Requirements",
    "text": "Test Requirements\nRecall that 2-sample tests for proportions needed an at least 10 expected successes and at least 10 expected failures (\\(np \\ge 10\\) and \\(n(1-p)\\ge10\\)) for the test statistic to be valid.\nFor a \\(\\chi^2\\) test we need to check the expected counts for all the different combinations.\nWe don’t need to fret about the math behind the expected count calculation. Intuitively, if there was no relationship between the two variables you would expect all the row totals to be proportionally distributed across the column groups.\nWe only need to check that all expected counts are greater than 5.\n\nchisq.test(chiro_table)$expected &gt;= 5\n\n               \n                At Risk Prevention Self Care Sick Role Wellness\n  Australia        TRUE       TRUE      TRUE      TRUE     TRUE\n  Europe           TRUE       TRUE      TRUE      TRUE     TRUE\n  United States    TRUE       TRUE      TRUE      TRUE     TRUE"
  },
  {
    "objectID": "Class_Notes/Chi_Square_Intro.html#visualization",
    "href": "Class_Notes/Chi_Square_Intro.html#visualization",
    "title": "Chi-square Test of Independence",
    "section": "Visualization",
    "text": "Visualization\nWe can use ggplot() to create nice bar charts to help interpret the results.\n\nggplot(chiropractic, aes(x = location, fill = motivation)) +\n  geom_bar(position = \"dodge\") +\n  theme_minimal()"
  },
  {
    "objectID": "Class_Notes/Chi_Square_Intro.html#homework-grades-and-classroom-type",
    "href": "Class_Notes/Chi_Square_Intro.html#homework-grades-and-classroom-type",
    "title": "Chi-square Test of Independence",
    "section": "Homework: Grades and Classroom Type",
    "text": "Homework: Grades and Classroom Type\nThis is the raw data for the Homework Quiz. Use it to answer the homework questions, but also create a Bar Chart using ggplot().\n\ncourse &lt;- import('https://raw.githubusercontent.com/byuistats/Math221D_Cannon/master/Data/course_type_by_grade.csv')\n\nCreate a Bar Chart using ggplot()\nPerform the \\(\\chi^2\\) test of independence.\nAre the requirements for a \\(\\chi^2\\) test satisfied?\nWhat are the null and alternative hypotheses?\nWhat is the test statistic, \\(\\chi^2\\)?\nHow many degrees of freedom does this test have?\nWhat is the P-value?"
  },
  {
    "objectID": "Class_Notes/Chi_Square_Intro.html#heart-disease-among-australian-women-and-men",
    "href": "Class_Notes/Chi_Square_Intro.html#heart-disease-among-australian-women-and-men",
    "title": "Chi-square Test of Independence",
    "section": "Heart Disease among Australian Women and Men",
    "text": "Heart Disease among Australian Women and Men\nIn 1982 in Western Australia, 1317 males and 854 females died of ischemic heart disease, 1119 males and 828 females died of cancer, 371 males and 460 females died of cerebral vascular disease, and 346 males and 147 females died of accidents. A medical researcher wanted to see if gender and cause of death are independent using a level of significance of 0.05.\nThe data read in below are a summary table of counts. One way to create a bar chart to compare heart disease deaths between men and women is to take this data and make it “longer”. This stacks the columns with count data in them and makes 2 new columns, one for the counts and one for the category of cardiovascular death.\nI will comment the code below to walk through each step.\nThe %&gt;% “pipe” below comes from the tidyverse library. You can think of this like making a series of steps where everything before the %&gt;% is pushed to the next step. For example, we assign aussie_death the original data table then move the table into the pivot_long() function which is the function that stacks the data. The output of that function becomes a new data shape, aussie_death.\n\n# Import the table data\naussie_death_table &lt;- import(\"https://byuistats.github.io/M221R/Data/quiz/R/aussie_death.csv\") \naussie_death_table\n\n      V1 heart_disease cancer vascular_disease accident\n1 female           854    828              460      147\n2   male          1371   1119              371      346\n\n# Pipe the original table into the pivot_longer() function\n# Pivot_longer needs to know which columns to \"stack\" which is input by the 'cols = ' argument.\n# We also need to give a name to the new column containing the count information.  The 'values_to=' argument names the column that will have the values, in this case we use \"count\".  \n# The 'names_to = ' argument names the column that will contain the labels of each category\n\n# Run the code and see if you can follow what happened\n\naussie_death &lt;- aussie_death_table %&gt;% \n  pivot_longer(cols = c('heart_disease', 'cancer', 'vascular_disease', 'accident'), values_to = 'count', names_to = \"reason\")\n\naussie_death\n\n# A tibble: 8 × 3\n  V1     reason           count\n  &lt;chr&gt;  &lt;chr&gt;            &lt;int&gt;\n1 female heart_disease      854\n2 female cancer             828\n3 female vascular_disease   460\n4 female accident           147\n5 male   heart_disease     1371\n6 male   cancer            1119\n7 male   vascular_disease   371\n8 male   accident           346\n\n# V1 was the default label and really represents \"Gender\" in this study.  We could change the name or leave it as is and fix it in the graphs\n\n\n# When using raw data, 'geom_bar()' creates the counts automatically from the categorical columns in a raw dataset.  The data in aussie_death is still a summary.  If we have the counts already, then we use the `geom_col()` function and have to specify a y variable to define how high to make the bars (in our case \"count\")\n\nggplot(aussie_death, aes(x = reason, y = count, fill = V1)) +\n  geom_col(position=\"dodge\")"
  },
  {
    "objectID": "Class_Notes/Distribution_of_Phat.html",
    "href": "Class_Notes/Distribution_of_Phat.html",
    "title": "Sampling Distribution of P_hat",
    "section": "",
    "text": "Categorical data is often summarized as a percent. If we randomly select 500 students and find black hair 276 have brown hair, we can estimate the population proportion using \\(\\hat{p} = \\frac{X}{N}\\) where X is the number with brown hair and N is the number in our sample. We often interchange percents and proportions, but strictly speaking, a proportion is a number between zero and 1. This can be interpreted as a probability as well.\nIf another researcher were to collect a different sample of 500 from the same population, they would almost certainly get a different number of people with brown hair than the first study. We can imagine taking many samples of 500 students and imagine the theoretical distribution of all possible \\(\\hat{p}\\). If we are taking good samples, most of these \\(\\hat{p}\\)’s should be near the population proportion, \\(p\\).\nIn fact, under certain conditions we can know the distribution of \\(\\hat{p}\\). As you probably guessed, the distribution of \\(\\hat{p}\\) is approximately normal with:\n\\[\\mu_{\\hat{p}} = p\\] \\[\\sigma_{\\hat{p}} = \\sqrt{\\frac{p(1-p)}{N}}\\]\nThat is to say, the mean of all sample proportions is the population proportion.\nJust as it was with a sample mean, \\(\\bar{x}\\), we have to check certain conditions to assume the distribution is approximately normal. For \\(\\bar{x}\\), we needed the population to be normally distributed or to have a sample size greater than 30. The principle of having a large enough sample applies, but it’s a different for a proportion.\nWe can assume the distribution is approximately normal if:\n\\[np \\geq 10\\] \\[n(1-p) \\geq 10\\]\nIn plain English, this means our sample size has to be big enough to have at least 10 “successes” and 10 “failures”. For example, if we’re estimating the proportion of left handed people, we would need a sample size large enough to have at least 10 left handed people and 10 right handed people.\nIf the distribution of sample means is approximately normal according to the conditions above, we can calculate a z-score as we did in Unit 1 and 2:\n\\[z = \\frac{\\hat{p}-p}{\\sqrt{\\frac{p(1-p)}{N}}}\\]\nthen use pnorm() as before."
  },
  {
    "objectID": "Class_Notes/Distribution_of_Phat.html#example",
    "href": "Class_Notes/Distribution_of_Phat.html#example",
    "title": "Sampling Distribution of P_hat",
    "section": "Example",
    "text": "Example\nSuppose we have a population where the true proportion of success is \\(p = 0.6\\). We take a random sample of size \\(n = 100\\) from this population. We want to find the probability that the sample proportion \\(\\hat{p}\\) is less than \\(0.55\\).\n\n# Given data\nx &lt;- 55\nn &lt;- 100\np_hat &lt;- x/n\n\n# population proportion\np &lt;- 0.6\n\n# Calculate standard deviation\nsigma &lt;- sqrt(p * (1 - p) / n)\n\n# Calculate z-score\nz &lt;- (p_hat - p) / sigma\n\n# Left Tail (lower than p)\npnorm(z)\n\n[1] 0.1537171\n\n# Right Tail (greater than p)\n1-pnorm(z)\n\n[1] 0.8462829"
  },
  {
    "objectID": "Class_Notes/Distribution_of_Phat.html#your-turn",
    "href": "Class_Notes/Distribution_of_Phat.html#your-turn",
    "title": "Sampling Distribution of P_hat",
    "section": "Your Turn",
    "text": "Your Turn\nThe nationwide, fully-vaccinated rate if November 2021 was 58%. Based on survey responses of 150, 81% of all BYU-I students on campus during Fall 2021 semester had received at least one vaccination dose against COVID-19 with 74% being fully vaccinated.\nWhat is the probability that we get a random sample of 150 individuals with a \\(\\hat{p}\\) higher than the fully vaccinated rate that we observed?\n\n# given data\n\n# Population Proportion\n\n# Calculate Z\n\n# P-value"
  },
  {
    "objectID": "Class_Notes/Inference_for_mean_sigma_unknown.html",
    "href": "Class_Notes/Inference_for_mean_sigma_unknown.html",
    "title": "Inference for a Mean - Sigma Unknown",
    "section": "",
    "text": "When we know what the population standard deviation, \\(\\sigma\\), for individuals, we can calculate Z-scores and use the Standard Normal Distribution to calculate probabilities. We can think of Z as having a standard normal distribution, meaning it has a mean of 0 and a standard deviation of 1.\n\\[ z= \\frac{\\bar{x} - \\mu}{\\frac{\\sigma}{\\sqrt{n}}}\\]\nThough rare, there are situations where we might know the population standard deviation from published research or census data. Standardized tests, for example, publish population-level summaries which would allow us to test how our sample compares to the general population using the Z formula.\n\n\nWhen testing a null hypothesis, the \\(\\mu\\) in the z-score formula becomes the hypothesized population mean. Our z-score is then interpreted as the number of standard deviations away from the hypothesized mean. If we know the population standard deviation, \\(\\sigma\\), then we can calculate the probability of getting a sample mean more extreme than the one we observed if the null hypothesis is true.\nKEY DEFINITION: A P-value is the probability of observing a test statistic as extreme, or more extreme than the one we observed in our sample, if the null hypothesis is true.\nWe use “as or more extreme” because the direction (greater than or less than) depends on our alternative hypothesis. In the above example, if I believe that my students scored higher than the general population average, I can say, there is only a 0.0011 chance of getting a test statistic higher than the one I observed if the null hypothesis is true. Because that probability is very low, I am willing to reject the null hypothesis in favor of the alternative that my students scored higher, on average, than the general population.\nEXAMPLE: A factory claims its light bulbs last, on average, \\(\\mu = 800\\) hours with a standard deviation, \\(\\sigma = 40\\). We randomly select 40 light bulbs to test to see if the life expectancy is actually less than that.\nState the Null and alternative Hypotheses:\n\\[ H_0: \\mu \\]\n\\[ H_a: \\mu \\]\nCalculate the Z-score and find the P-value for the test:\nState your conclusion:\n\n\n\nWe can also calculate a confidence interval for the above example. Recall the formula for a confidence interval is, for a given z* associated with a desired level of confidence:\n\\[ CI = \\bar{x} \\pm z^*\\frac{\\sigma}{\\sqrt{n}}\\]\nRecall the z* for common confidence levels:\n\nlibrary(tidyverse)\nlibrary(pander)\ntibble(`Conf. Level` = c(0.99, 0.95, 0.90), `Z*` = c(2.576, 1.96, 1.645)) %&gt;% pander()\n\n\n\n\n\n\n\n\nConf. Level\nZ*\n\n\n\n\n0.99\n2.576\n\n\n0.95\n1.96\n\n\n0.9\n1.645"
  },
  {
    "objectID": "Class_Notes/Inference_for_mean_sigma_unknown.html#hypothesis-testing",
    "href": "Class_Notes/Inference_for_mean_sigma_unknown.html#hypothesis-testing",
    "title": "Inference for a Mean - Sigma Unknown",
    "section": "",
    "text": "When testing a null hypothesis, the \\(\\mu\\) in the z-score formula becomes the hypothesized population mean. Our z-score is then interpreted as the number of standard deviations away from the hypothesized mean. If we know the population standard deviation, \\(\\sigma\\), then we can calculate the probability of getting a sample mean more extreme than the one we observed if the null hypothesis is true.\nKEY DEFINITION: A P-value is the probability of observing a test statistic as extreme, or more extreme than the one we observed in our sample, if the null hypothesis is true.\nWe use “as or more extreme” because the direction (greater than or less than) depends on our alternative hypothesis. In the above example, if I believe that my students scored higher than the general population average, I can say, there is only a 0.0011 chance of getting a test statistic higher than the one I observed if the null hypothesis is true. Because that probability is very low, I am willing to reject the null hypothesis in favor of the alternative that my students scored higher, on average, than the general population.\nEXAMPLE: A factory claims its light bulbs last, on average, \\(\\mu = 800\\) hours with a standard deviation, \\(\\sigma = 40\\). We randomly select 40 light bulbs to test to see if the life expectancy is actually less than that.\nState the Null and alternative Hypotheses:\n\\[ H_0: \\mu \\]\n\\[ H_a: \\mu \\]\nCalculate the Z-score and find the P-value for the test:\nState your conclusion:"
  },
  {
    "objectID": "Class_Notes/Inference_for_mean_sigma_unknown.html#confidence-interval",
    "href": "Class_Notes/Inference_for_mean_sigma_unknown.html#confidence-interval",
    "title": "Inference for a Mean - Sigma Unknown",
    "section": "",
    "text": "We can also calculate a confidence interval for the above example. Recall the formula for a confidence interval is, for a given z* associated with a desired level of confidence:\n\\[ CI = \\bar{x} \\pm z^*\\frac{\\sigma}{\\sqrt{n}}\\]\nRecall the z* for common confidence levels:\n\nlibrary(tidyverse)\nlibrary(pander)\ntibble(`Conf. Level` = c(0.99, 0.95, 0.90), `Z*` = c(2.576, 1.96, 1.645)) %&gt;% pander()\n\n\n\n\n\n\n\n\nConf. Level\nZ*\n\n\n\n\n0.99\n2.576\n\n\n0.95\n1.96\n\n\n0.9\n1.645"
  },
  {
    "objectID": "Class_Notes/Inference_for_mean_sigma_unknown.html#calculating-p-values-by-hand-with-the-t-distribution-not-recommended",
    "href": "Class_Notes/Inference_for_mean_sigma_unknown.html#calculating-p-values-by-hand-with-the-t-distribution-not-recommended",
    "title": "Inference for a Mean - Sigma Unknown",
    "section": "Calculating P-values by hand with the T Distribution (NOT RECOMMENDED)",
    "text": "Calculating P-values by hand with the T Distribution (NOT RECOMMENDED)\nSuppose we have 25 test scores defined as “data” in the code chunk below. We can calculate the t-statistic just like we did with the z-score.\nSuppose we believe that the mean score of these 25 students is significantly higher than 50. Our null and alternative hypothesis are as follows:\n\\[ H_0:  \\mu = 50 \\] \\[ H_a: \\mu &gt; 50 \\]\n\n# we can use the t-distribution, pt(), just like pnorm() but must also add the degrees of freedom\n\ndata &lt;- c(88,81,27,92,46,79,67,44,46,88,21,60,71,81,79,52,100,44,42,58,52,48,83,65,98)\n\n# Hypothesized Mean:\nmu_0 &lt;- 50\n\n# Sample size, sample Mean and sample SD\n#  The length function tells us how many data points are in the list.  \nn &lt;- length(data)\nxbar &lt;- mean(data)\ns &lt;- sd(data)\n\ns_xbar = s / sqrt(n)\n\nt &lt;- (xbar - mu_0) / s_xbar\n\n# Probability of getting a test statistic at least as extreme as the one we observed if the null hypothesis is true\n1-pt(t, n-1)\n\n[1] 0.00151866\n\n\nThis means that if the true population mean was 50, then there is only a 0.00152 probability of observing a test statistic, t, as high as the one we got (P-value).\nThe good news is that the more complicated the math becomes, the less of it we have to do! Instead of using R like a calculator to calculate z or t scores and calculating probabilities “by hand” (using pnorm() or pt()), we can use R functions on the data directly and get much more useful output."
  },
  {
    "objectID": "Class_Notes/Inference_for_mean_sigma_unknown.html#step-1-read-in-the-data",
    "href": "Class_Notes/Inference_for_mean_sigma_unknown.html#step-1-read-in-the-data",
    "title": "Inference for a Mean - Sigma Unknown",
    "section": "Step 1: Read in the data",
    "text": "Step 1: Read in the data\n\n# Load Libraries\n\nlibrary(tidyverse)\nlibrary(mosaic)\nlibrary(rio)\nlibrary(car)\n\n\n# Read in data\nbig5 &lt;- import('https://raw.githubusercontent.com/byuistats/Math221D_Cannon/master/Data/All_class_combined_personality_data.csv')"
  },
  {
    "objectID": "Class_Notes/Inference_for_mean_sigma_unknown.html#step-2-review-the-data",
    "href": "Class_Notes/Inference_for_mean_sigma_unknown.html#step-2-review-the-data",
    "title": "Inference for a Mean - Sigma Unknown",
    "section": "Step 2: Review the Data",
    "text": "Step 2: Review the Data\nIn this step, we are looking to see if the data are as expected. Are the columns we’re interested in numeric? Categorical? and do these match expectations. We can also start to look for strange data and outliers. Visualizations can assist with that as well. Other common issues to look for include: negative numbers that should only be positive, date values that shouldn’t exist, missing values, character variables inside what should be a number.\nIn the real world, data are messy. Reviewing the data is a critical part of an analysis.\nTake a look through the personality dataset and see if there are any anomalies that might need to be addressed."
  },
  {
    "objectID": "Class_Notes/Inference_for_mean_sigma_unknown.html#step-3-visulize-the-data",
    "href": "Class_Notes/Inference_for_mean_sigma_unknown.html#step-3-visulize-the-data",
    "title": "Inference for a Mean - Sigma Unknown",
    "section": "Step 3: Visulize the data",
    "text": "Step 3: Visulize the data\nSo far we have been discussing a single, quantitative variable of interest, like test scores, reaction times, heights, etc. When we start looking at more complicated data, we will expand our repertoire of visualizations, but Histograms are very good when looking at one variable at a time, and boxplots are very good for comparison between groups.\nCreate a histogram of Extroversion. Describe some features of the data. Is it symmetric? Skewed? Are there outliers?\n\n# Basic Graph\nhistogram(big5$Extroversion)\n\n\n\n\n\n\n\n# Improved graph\nhistogram(big5$Extroversion, main = \"Extroversion Scores\", xlab = \"Extroversion\")"
  },
  {
    "objectID": "Class_Notes/Inference_for_mean_sigma_unknown.html#step-4-perform-the-appropriate-analysis",
    "href": "Class_Notes/Inference_for_mean_sigma_unknown.html#step-4-perform-the-appropriate-analysis",
    "title": "Inference for a Mean - Sigma Unknown",
    "section": "Step 4: Perform the appropriate analysis",
    "text": "Step 4: Perform the appropriate analysis\nIn this example, we will be testing the hypothesis that Brother Cannon’s students are similar to the general population. We suspect that these youthful BYU-I students are, on average, more extroverted.\n\nHypothesis Test\nWrite out the Null and alternative hypotheses.\n\\[ H_0: \\mu_{extroversion} = 50\\]\n\\[ H_A: \\mu_{extroversion}  &gt; 50\\]\n\\[\\alpha = 0.05\\]\nPerform the one-sample t-test using the “t.test()” function in R. If you ever get stuck remembering how to use a function in R, you can run ?t.test to see documentation. The question mark will open up the help files for any given function in R.\n\n#?t.test\n\n# One-sided Hypothesis Test\nt.test(big5$Extroversion, mu = 50, alternative = \"greater\")\n\n\n    One Sample t-test\n\ndata:  big5$Extroversion\nt = 6.6529, df = 403, p-value = 4.697e-11\nalternative hypothesis: true mean is greater than 50\n95 percent confidence interval:\n 55.25232      Inf\nsample estimates:\nmean of x \n 56.98267 \n\n\nQ: What is the P-value?\nQ: How do we explain our conclusion in context of our research question?\nTechnical Conclusion:\nContextual Conclusion:\n\n\nConfidence Intervals\nWe can also use the t.test() function to create confidence intervals. Recall that confidence intervals are always 2-tailed. Confidence intervals are typically written in the form: (lower limit, upper limit).\n\n# Confidence Intervals are by definition 2-tailed\n# We can also change the confidence level\n\nt.test(big5$Extroversion, mu = 50, alternative = \"two.sided\", conf.level = .99)\n\n\n    One Sample t-test\n\ndata:  big5$Extroversion\nt = 6.6529, df = 403, p-value = 9.394e-11\nalternative hypothesis: true mean is not equal to 50\n99 percent confidence interval:\n 54.26631 59.69903\nsample estimates:\nmean of x \n 56.98267 \n\n# If we only want the output to show the confidence interval and not the hypothesis test, we can use a `$` to select specific output:\n# Because the default option for the `alternative =`  in the t.test fucntion is \"two.sided\", we don't actually need to include it\n\nt.test(big5$Extroversion, mu = 50, conf.level = .99)$conf.int\n\n[1] 54.26631 59.69903\nattr(,\"conf.level\")\n[1] 0.99\n\n\nDescribe in words the interpretation of the confidence interval in context of Extroversion."
  },
  {
    "objectID": "Class_Notes/Inference_for_mean_sigma_unknown.html#body-temperature-data",
    "href": "Class_Notes/Inference_for_mean_sigma_unknown.html#body-temperature-data",
    "title": "Inference for a Mean - Sigma Unknown",
    "section": "Body Temperature Data",
    "text": "Body Temperature Data\nThe data below contains information about body temperatures of healthy adults.\n\nLoad the data:\n\n# These lines load the data into the data frame body_temp:\n\nbody_temp &lt;- import(\"https://byuistats.github.io/M221R/Data/body_temp.xlsx\")\n\n\n\nReview the Data\nWhat is the mean and standard deviation of the body temperatures in our data set?\n\n\nVisualize the Data\nCreate a histogram to visualize the body temperature data. Which of the following best describes the distribution of the data?\n\n\nAnalyze the Data\nIt’s widely accepted that normal body temperature for healthy adults is 98.6 degrees Fahrenheit.\nSuppose we suspect that the average temperature is different than 98.6\nUse a significance level of \\(\\alpha = 0.01\\) to test whether the mean body temperature of healthy adults is equal to 98.6 degrees Fahrenheit.\nWhat is the P-value?\nExplain our conclusion.\nCreate a 99% confidence interval for the true population average temperature of healthy adults."
  },
  {
    "objectID": "Class_Notes/Linear_Regression_Intro.html",
    "href": "Class_Notes/Linear_Regression_Intro.html",
    "title": "Simple Linear Regression",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(rio)\nlibrary(car)\n\nmath &lt;- import('https://byuistats.github.io/BYUI_M221_Book/Data/MathSelfEfficacy.xlsx')"
  },
  {
    "objectID": "Class_Notes/Linear_Regression_Intro.html#plotting-the-regression-line",
    "href": "Class_Notes/Linear_Regression_Intro.html#plotting-the-regression-line",
    "title": "Simple Linear Regression",
    "section": "Plotting the Regression Line",
    "text": "Plotting the Regression Line\nScatter plots by themselves are nice, but we would also like to see the regression line. Simple graphics in R can be augmented by using some functions. The abline() function, when executed right after a graphing function can add lines. We’ve used this to add vertical lines and horizontal line already in class. We can also use this function to add a regression line. We simply insert our linear model output into the abline() function as follows:\n\nplot(Score ~ ConfidenceRatingMean, data = math)\nabline(math_lm)\n\n\n\n\n\n\n\n\nJust as with the other plotting functions we’ve used, we can change the color, type and width of the line:\n\nplot(Score ~ ConfidenceRatingMean, data = math, pch = 16, main = \"Title\")\nabline(math_lm, col = \"purple\", lwd = 3, lty = 3)"
  },
  {
    "objectID": "Class_Notes/Linear_Regression_Intro.html#hypothesis-testing-for-regression",
    "href": "Class_Notes/Linear_Regression_Intro.html#hypothesis-testing-for-regression",
    "title": "Simple Linear Regression",
    "section": "Hypothesis Testing for Regression",
    "text": "Hypothesis Testing for Regression\nA linear equation has 2 parameters: Slope and Intercept. In most situations, the intercept isn’t very interesting by itself and is often absurd. We are most interested in the slope\n\\[H_o: \\beta_1 = 0\\] \\[H_a: \\beta_1 \\neq 0\\]\nThese are the same for all regression questions.\nTo get the p-value and test statistics, we use the summary() function as we did with aov:\n\nsummary(math_lm)\n\n\nCall:\nlm(formula = Score ~ ConfidenceRatingMean, data = math)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-38.200  -6.163   1.292   7.567  23.422 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)            18.690      4.610   4.054  8.4e-05 ***\nConfidenceRatingMean   12.695      1.022  12.424  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.27 on 137 degrees of freedom\nMultiple R-squared:  0.5298,    Adjusted R-squared:  0.5264 \nF-statistic: 154.4 on 1 and 137 DF,  p-value: &lt; 2.2e-16\n\n\nWe can also calculate confidence intervals for the slope by using the confint() function. This function requires you to tell it which model to extract a confidence intervals from. You can specify which parameter you’re interested in, and the level of confidence:\n\n# input the model into the following function:\nconfint(math_lm, level = .95)\n\n                         2.5 %   97.5 %\n(Intercept)           9.573588 27.80654\nConfidenceRatingMean 10.674297 14.71535\n\n\nHow do we interpret this confidence interval for a slope?\n95% Confident that the true population slope is within (10.674297, 14.71535)\nFor every 1 unit increase in Confidence Rating, test scores go up by between (10.674297, 14.71535) on average."
  },
  {
    "objectID": "Class_Notes/Linear_Regression_Intro.html#regression-requirements",
    "href": "Class_Notes/Linear_Regression_Intro.html#regression-requirements",
    "title": "Simple Linear Regression",
    "section": "Regression Requirements",
    "text": "Regression Requirements\nThere are certain requirements for all statistical tests to be valid. For means, we needed to make sure that the Central Limit Theorem applied. This meant that we had a large enough sample size (N&gt;30) or that the population itself was normally distributed.\nFor ANOVA, we had to check that the residuals were normally distributed and that the population standard deviations were the same.\nRegression analysis has 5 requirements to be valid. While this sounds daunting, in practice we can check most of them very quickly.\n\nRelationship between X and Y is Linear\nThe residuals, \\(\\epsilon\\), are normally distributed\nThe Variance of the error terms is constant for all values of X\nThe X’s are fixed and measured without error (i.e. X’s can be considered as known constants)\nThe observations are independent\n\nThe linear relationship is assessed visually with the scatter plot. If there is obvious curvature or non-linearity then fitting a line isn’t the best model.\nWe check the normality of the residuals with a qqplot() exactly as with the aov() output.\nThe constant Variance is checked with a new plot, that looks at how the predicted values relate to the residuals. This is important because we want our predictions to be “wrong” about the same regardless of the value of the prediction. We’re looking for random scatter.\nRequirements 4 cannot be analyzed directly. It is important because X is the independent variable. If there is uncertainty about the input, then the simple linear regression might not be the most appropriate model.\nRequirement 5 also cannot be analyzed, but random sampling is usually satisfies this requirement.\n\n# Requirement 1:  Check for linear relationship\nplot(Score ~ ConfidenceRatingMean, data = math, pch = 19, main = \"Title\")\n\n\n\n\n\n\n\n# Req 2: Normality of residuals:\nqqPlot(math_lm$residuals)\n\n\n\n\n\n\n\n\n[1] 37 89\n\n# Req 3: Constant variance (look odd patterns). When you put lm() output into the plot function it gives you several different plots. The residual plots we're most interested in are 1 and 2\n\nplot(math_lm, which = 1)"
  },
  {
    "objectID": "Class_Notes/One_Sample_Proportion_Ztest.html",
    "href": "Class_Notes/One_Sample_Proportion_Ztest.html",
    "title": "One-sample Proportion Tests",
    "section": "",
    "text": "In statistics, one-sample proportion tests are used to compare proportions or percentages to a hypothesized value. These tests are useful when dealing with categorical data. We here discuss these tests and provide examples of their application in R.\nThe hypothesis test should look very familiar:\n\\[H_0: p = p_0\\] \\[H_a: p\\; (&lt;,&gt;,\\neq) \\: p_0\\] where \\(p_0\\) is some hypothesized value for the population proportion.\nSo far, we have been using Greek letters to represent population parameters. We deviate from that now due to the fact that the Greek letter for p, \\(\\pi\\), already has a long-established meaning in mathematics. In the hypothesis definition above, \\(p\\) represents the population proportion."
  },
  {
    "objectID": "Class_Notes/One_Sample_Proportion_Ztest.html#example-1-one-sample-proportion-test",
    "href": "Class_Notes/One_Sample_Proportion_Ztest.html#example-1-one-sample-proportion-test",
    "title": "One-sample Proportion Tests",
    "section": "Example 1: One Sample Proportion Test",
    "text": "Example 1: One Sample Proportion Test\nSuppose we want to test whether the proportion of students who passed an exam is significantly less than 0.75. We have a sample of 100 students, of which 72 passed.\n\\[\\hat{p} = \\frac{X}{N} = \\frac{72}{100} = .72\\]\nIf we want to test if this is significantly less than 75%, we can use the prop.test() which is very similar to t.test(). Instead of putting in a sample mean, \\(\\bar{x}\\), with a hypothesized \\(\\mu\\), we put in \\(X\\) and \\(N\\) and a hypothesized \\(p\\). Setting the alternative and confidence level operates the same as t.test().\nConfidence intervals for proportions for proportions can also be obtained just as with t.test().\n\n# One Sample Proportion Test Example\n# Hypothesized proportion: 0.75\n# Sample size: 100\n# Number of successes: 72\n\nprop.test(x = 72, n = 100, p = 0.75, alternative = \"less\", conf.level = .9)\n\n\n    1-sample proportions test with continuity correction\n\ndata:  72 out of 100, null probability 0.75\nX-squared = 0.33333, df = 1, p-value = 0.2819\nalternative hypothesis: true p is less than 0.75\n90 percent confidence interval:\n 0.0000000 0.7782396\nsample estimates:\n   p \n0.72 \n\nprop.test(x = 72, n = 100, conf.level = .9)$conf.int\n\n[1] 0.6358512 0.7917861\nattr(,\"conf.level\")\n[1] 0.9\n\n\n\nThe distribution of \\(\\hat{p}\\)\nRecall that the sampling distribution for \\(\\hat{p}\\) is approximately normally distributed when our sample has more than 10 expeted “successes” and more than 10 expected “failures”. We test this by looking at\nHypothesis Test Requirements:\n\\[np \\geq 10\\] \\[n(1-p) \\geq 10\\]\nWe use p, not \\(\\hat{p}\\) for hypothesis testing because hypothesis testing always assumes the null hypothesis is true. Confidence intervals, on the other hand, make no such assumption.\nTo see if the calculated confidence interval is appropriate, we use \\(\\hat{p}\\).\nConfidence Interval Requirements: \\[n\\hat{p} \\geq 10\\] \\[n(1-\\hat{p}) \\geq 10\\] Can we trust the p-value and confidence interval?\nYou can use a calculator for this, or simply use R as a calculator:\n\nx &lt;- 72\nn &lt;- 100\np_hat &lt;- x/n\np &lt;- .75\n\n# For Hypothesis Testing:\nn*p &gt;= 10\n\n[1] TRUE\n\nn*(1-p) &gt;= 10\n\n[1] TRUE\n\n# For Confidence Intervals:\nn*p_hat &gt;= 10\n\n[1] TRUE\n\nn*(1-p_hat) &gt;=10\n\n[1] TRUE"
  },
  {
    "objectID": "Class_Notes/One_Sample_Proportion_Ztest.html#example-2-handedness",
    "href": "Class_Notes/One_Sample_Proportion_Ztest.html#example-2-handedness",
    "title": "One-sample Proportion Tests",
    "section": "Example 2: Handedness",
    "text": "Example 2: Handedness\nSuppose the United States national average percent of left-handed people is 11%. A researcher wants to know if visual arts majors are significantly more likely to be left handed. She samples 250 visual arts majors and finds that 36 are left handed.\nPerform a one-sample proportion to see if visual arts majors are significantly more left-handed than the general population.\nState the null and alternative hypotheses and your significance level.\n\\[H_0: p = \\] \\[H_a: p \\] \\[\\alpha = \\]\nMake a \\((1-\\alpha)\\) level confidence interval for the true population proportion.\nAre the test requirments for the normlity of \\(\\hat{p}\\) satisfied?\nAre the requirements for a confidence interval for \\(p\\) satisfied?"
  },
  {
    "objectID": "Class_Notes/Summarizing_Data_Multiple_Groups.html",
    "href": "Class_Notes/Summarizing_Data_Multiple_Groups.html",
    "title": "Summarizing Data",
    "section": "",
    "text": "In this document, we will demonstrate how to summarize quantitative data for multiple groups in a dataset.\n\n\n\nlibrary(rio)\nlibrary(mosaic)\nlibrary(tidyverse)\nlibrary(car)\n\nbig5 &lt;- import('https://raw.githubusercontent.com/byuistats/Math221D_Cannon/master/Data/All_class_combined_personality_data.csv')"
  },
  {
    "objectID": "Class_Notes/Summarizing_Data_Multiple_Groups.html#load-the-data-and-libraries",
    "href": "Class_Notes/Summarizing_Data_Multiple_Groups.html#load-the-data-and-libraries",
    "title": "Summarizing Data",
    "section": "",
    "text": "library(rio)\nlibrary(mosaic)\nlibrary(tidyverse)\nlibrary(car)\n\nbig5 &lt;- import('https://raw.githubusercontent.com/byuistats/Math221D_Cannon/master/Data/All_class_combined_personality_data.csv')"
  },
  {
    "objectID": "Class_Notes/Summarizing_Data_Multiple_Groups.html#summary-statistics",
    "href": "Class_Notes/Summarizing_Data_Multiple_Groups.html#summary-statistics",
    "title": "Summarizing Data",
    "section": "Summary Statistics",
    "text": "Summary Statistics\nWe can easily extend favstats() to output our favorite statistics for multiple groups. We first must identify the quantitative factor we want to compare. For example, we could compare agreeableness between the sexes.\n\n# This gives us the summary statistics for Agreeableness across all groups\nfavstats(big5$Agreeableness)\n\n min Q1 median Q3 max     mean       sd   n missing\n  21 67     75 81 100 73.43457 13.24909 405       0\n\n# Adding the '~' tells R to break the data into groups (determined by the right side of the '~') and calculate the means of the variable on the left\nfavstats(big5$Agreeableness ~ big5$`Sex(M/F)`)\n\n  big5$`Sex(M/F)` min Q1 median Q3 max     mean       sd   n missing\n1               F  21 69     77 85 100 75.92035 12.94640 226       0\n2               M  25 63     73 79  94 70.29609 12.99218 179       0"
  },
  {
    "objectID": "Class_Notes/Summarizing_Data_Multiple_Groups.html#visual-summaries-by-group",
    "href": "Class_Notes/Summarizing_Data_Multiple_Groups.html#visual-summaries-by-group",
    "title": "Summarizing Data",
    "section": "Visual Summaries by Group",
    "text": "Visual Summaries by Group\nWe can use the exact same format as we used for favstats() for boxplot():\n\nboxplot(big5$Agreeableness ~ big5$`Sex(M/F)`)"
  },
  {
    "objectID": "Class_Notes/Summarizing_Data_Multiple_Groups.html#improving-graphs",
    "href": "Class_Notes/Summarizing_Data_Multiple_Groups.html#improving-graphs",
    "title": "Summarizing Data",
    "section": "Improving Graphs",
    "text": "Improving Graphs\nThroughout this course, we will ease into making better visualizations. For now, here are some basic techniques that will usually apply to all graphing functions in R:\n\n# Changing color by sepecifying the `col = c()`\nboxplot(big5$Agreeableness ~ big5$`Sex(M/F)`, col = c(\"red\", \"blue\"))\n\n\n\n\n\n\n\n# R also assigns a numerical value to `col = `.  Try different numbers\nboxplot(big5$Agreeableness ~ big5$`Sex(M/F)`, col = c(2,3))\n\n\n\n\n\n\n\nboxplot(big5$Agreeableness ~ big5$`Sex(M/F)`, col = c(4,6))\n\n\n\n\n\n\n\n# Adding better axis labels using `xlab = ` and `ylab = `:\nboxplot(big5$Agreeableness ~ big5$`Sex(M/F)`, xlab = \"Biosex\", ylab = \"Trait Agreeableness\")\n\n\n\n\n\n\n\n# Adding a title:\nboxplot(big5$Agreeableness ~ big5$`Sex(M/F)`, main = \"Comparing Agreeableness by Biosex\")\n\n\n\n\n\n\n\n# Putting it all together:\nboxplot(big5$Agreeableness ~ big5$`Sex(M/F)`,main = \"Comparing Agreeableness by Biosex\", xlab = \"Biosex\", ylab = \"Trait Agreeableness\", col = c(3, 4))"
  },
  {
    "objectID": "Class_Notes/Two_Sample_Proportion_Ztest.html",
    "href": "Class_Notes/Two_Sample_Proportion_Ztest.html",
    "title": "Two-sample Proportion Tests",
    "section": "",
    "text": "In statistics, two sample proportion tests are used to compare proportions or percentages between groups. These tests are useful when dealing with categorical data between two groups. We here discuss these tests and provide examples of their application in R.\nThe hypothesis test should not be surprising:\n\\[H_0: p_1 = p_2\\] \\[H_a: p_1\\; (&lt;,&gt;,\\neq) \\: p_2\\] where \\(p_1\\) represents the unknown population proportion for group 1 and \\(p_2\\) represents the unknown population proportion for group 2."
  },
  {
    "objectID": "Class_Notes/Two_Sample_Proportion_Ztest.html#example-1-voting-behaviour-by-gender",
    "href": "Class_Notes/Two_Sample_Proportion_Ztest.html#example-1-voting-behaviour-by-gender",
    "title": "Two-sample Proportion Tests",
    "section": "Example 1: Voting Behaviour by Gender",
    "text": "Example 1: Voting Behaviour by Gender\nSuppose we want to test if women are more likely to identify as Democrat than men. We sample 250 men and 250 women and measure their political affiliation. We find that 80 men identify as Democrat and 102 females identify as Democrat.\nJust as with the two-sample t-test for means, we must define a reference group. In this example, we will use females as the reference group so that our alternative will be relative to that group.\n\\[H_0: p_{femaleDem} = p_{maleDem}\\] \\[H_a: p_{femaleDem} &gt; p_{maleDem}\\] We will use \\(\\alpha = 0.05\\)\n\nprop.test(x = c(102, 80), n = c(250, 250), alternative = \"greater\")\n\n\n    2-sample test for equality of proportions with continuity correction\n\ndata:  c(102, 80) out of c(250, 250)\nX-squared = 3.8099, df = 1, p-value = 0.02548\nalternative hypothesis: greater\n95 percent confidence interval:\n 0.01350993 1.00000000\nsample estimates:\nprop 1 prop 2 \n 0.408  0.320 \n\n\nWe can also create a confidence interval for the difference:\n\nprop.test(x = c(102, 80), n = c(250, 250))$conf.int\n\n[1] 5.904086e-06 1.759941e-01\nattr(,\"conf.level\")\n[1] 0.95\n\n\nConfidence intervals for differences can be positive and negative. In this example, a negative number would indicate that Females are less likely to be Democrat and a positive number means they are more likely to be Democrat.\nOur confidence interval is just above zero on the lower end. We are 95% confident that females are between 0.000% and 17.6% more likely to be Democrat than men.\n\nTest Requirments\nJust as with 1-sample proportion tests, we must validate that we have a large enough sample size to ensure that \\(\\hat{p}\\) is approximately normally distributed. When we have 2 samples, however, we must check both \\(\\hat{p}\\)’s. For both hypothesis testing and confidence intervals we check:\nRequirements for Hypothesis Testing and Confidence Intervals\n\\[ n_1\\hat{p}_1 \\ge 10\\] \\[n_1(1-\\hat{p}_1) \\ge 10\\] \\[ n_2\\hat{p}_2 \\ge 10\\] \\[n_2(1-\\hat{p}_2) \\ge 10\\]\nAn easy R calculator to check this is:\n\n# All must be true:\n\nx1 &lt;- 102\nn1 &lt;- 250\nphat1 &lt;- x1/n1\n\nn1*phat1 &gt;= 10\n\n[1] TRUE\n\nn1*(1-phat1) &gt;=10\n\n[1] TRUE\n\nx2 &lt;- 80\nn2 &lt;- 250\nphat2 &lt;- x2 / n2\n\nn2*phat2 &gt;= 10\n\n[1] TRUE\n\nn2*(1-phat2) &gt;=10\n\n[1] TRUE\n\n\nIf all conditions are greater than or equal to 10, we can trust our p-values and confidence intervals."
  },
  {
    "objectID": "Class_Notes/Two_Sample_Proportion_Ztest.html#example-2-favorite-sports",
    "href": "Class_Notes/Two_Sample_Proportion_Ztest.html#example-2-favorite-sports",
    "title": "Two-sample Proportion Tests",
    "section": "Example 2: Favorite Sports",
    "text": "Example 2: Favorite Sports\nSoccer is becoming much more popular in the United States. We would like to test if this is being driven by demographic shifts in the population where the younger generation is more likely to favor soccer.\nA researcher samples 524 individuals under 40 and 655 individuals older than 40 and asks what their preferred sport is. Of the 524 respondents under 40, 44 identified soccer as their favorite sport. Of the 655 respondents over 40, 27 identified soccer as their favorite sport.\nPerform a 2-sample proportion test to determine if significantly more younger people identify soccer as their favorite sport.\nCreate and interpret the confidence interval for the difference in the proportions.\nAre the requirements for the hypothesis test and confidence interval satisfied?"
  },
  {
    "objectID": "Homework/Introducing_R_Homework.html#calculate-the-summary-statistics-for-duration",
    "href": "Homework/Introducing_R_Homework.html#calculate-the-summary-statistics-for-duration",
    "title": "Introducing R",
    "section": "Calculate the Summary Statistics for Duration",
    "text": "Calculate the Summary Statistics for Duration\nWhat is the mean duration time of Old Faithful eruptions?\nWhat is the standard deviation of duration?"
  },
  {
    "objectID": "Homework/Introducing_R_Homework.html#create-a-historgram-for-duration",
    "href": "Homework/Introducing_R_Homework.html#create-a-historgram-for-duration",
    "title": "Introducing R",
    "section": "Create a Historgram for Duration",
    "text": "Create a Historgram for Duration\nCreate a histogram and describe the shape of the distribution of duration:"
  },
  {
    "objectID": "Homework/Introducing_R_Homework.html#calculate-summary-statistics-for-wait-time",
    "href": "Homework/Introducing_R_Homework.html#calculate-summary-statistics-for-wait-time",
    "title": "Introducing R",
    "section": "Calculate Summary Statistics for Wait time",
    "text": "Calculate Summary Statistics for Wait time\nWhat is the mean wait time between eruptions?\nWhat is the maximum wait time between eruptions?\nThe middle 50% of wait times will be between what 2 numbers?"
  },
  {
    "objectID": "Textbook/Course_Introduction.html",
    "href": "Textbook/Course_Introduction.html",
    "title": "Course Introduction",
    "section": "",
    "text": "By the end of this lesson, you should be able to:\n\nExplain the course policies\nAccess course resources (course outline, lesson schedule, preparation activities, reading quizzes, homework assignments, assessments, etc.)\nCommunicate with the instructor and group members\nAccess statistical analysis software tools for class quizzes, assignments, and exams\nApply principles of the gospel of Jesus Christ in this class\nApply the three rules of probability for different probability scenarios"
  },
  {
    "objectID": "Textbook/Course_Introduction.html#lesson-outcomes",
    "href": "Textbook/Course_Introduction.html#lesson-outcomes",
    "title": "Course Introduction",
    "section": "",
    "text": "By the end of this lesson, you should be able to:\n\nExplain the course policies\nAccess course resources (course outline, lesson schedule, preparation activities, reading quizzes, homework assignments, assessments, etc.)\nCommunicate with the instructor and group members\nAccess statistical analysis software tools for class quizzes, assignments, and exams\nApply principles of the gospel of Jesus Christ in this class\nApply the three rules of probability for different probability scenarios"
  },
  {
    "objectID": "Textbook/Course_Introduction.html#welcome-to-the-course",
    "href": "Textbook/Course_Introduction.html#welcome-to-the-course",
    "title": "Course Introduction",
    "section": "Welcome to the Course!",
    "text": "Welcome to the Course!\nIn this course, you will explore important connections between the academic discipline of Statistics and the world around us. By pondering these ideas, your understanding of statistics will increase, as will your knowledge and testimony of the restored Gospel of Jesus Christ. In addition, that which you learn in this course will increase your ability to serve others as a disciple of Jesus Christ and help build Zion.\nThis course has been designed to help you slowly build up a knowledge base of ideas and skills. Not all of these ideas and skills will come easily. It takes a lot of work and practice before some things will even start to make sense, so you should not be surprised to find that it may take you a little time to comprehend these ideas. Just be patient. Once you’re far enough into the course, the ideas will start to come together, and you will see how much progress you have really made. You will understand what this course is all about, and you will be glad you persisted in your efforts to learn."
  },
  {
    "objectID": "Textbook/Course_Introduction.html#course-description",
    "href": "Textbook/Course_Introduction.html#course-description",
    "title": "Course Introduction",
    "section": "Course Description",
    "text": "Course Description\nThis course covers the following topics as they are applied to Statistics: graphical representations of data, measures of center and spread; elementary probability; sampling distributions; correlation and regression; statistical inference involving means, proportions, and contingency tables.\n\n\nCourse Learning Outcomes\nIn this course, we will:\n\nSummarize data numerically and graphically using spreadsheets\nMake decisions regarding situations with inherent randomness\nApply probability distributions to investigate questions\nEmploy confidence intervals in various situations\nImplement tests of diverse hypotheses\nCommunicate the results of statistical analyses to relevant audiences\n\n\n\n\nHow the Outcomes will Be Assessed\nWhile you may not be tested on everything you learn in this course, the instructor will be assessing your mastery of the Course Learning Outcomes. The general types of assessments used to measure these outcomes may include selected response tests such as multiple-choice, true-false, matching, and fill-in-the-blank questions. You may also be asked to complete essays or other writing assignments. At times, the instructor may assess your performance of a skill, or the instructor may assess products you create using particular skills. In addition, the instructor may engage in personal communication with you to determine how well you understand the course content.\n\n\n\nKeys to Success\n\nFive Principles of the Learning Model\nYou will experience much deeper learning if you follow the Five Principles of the BYU-Idaho Learning Model\n\nExercise Faith: Exercise faith in the Lord Jesus Christ as a principle of action and power.\nLearn by the Holy Ghost: Understand that true teaching is done by and with the Holy Ghost.\nLay Hold on the Word of God: Lay hold of the word of God.\nAct for Themselves: Act for yourself and accept responsibility for learning and teaching.\nLove, Serve, and Teach One Another: Love, serve, and teach other students in your classes.\n\n\n\nThree Process Steps of the Learning Model\nYou will learn more in less time if you follow the Three Process Steps of the BYU-Idaho Learning Model\n\nPrepare: This involves (a) spiritual preparation, (b) individual preparation, and (c) group preparation.\nTeach One Another: You should (a) be on time, (b) pray together, and (c) actively engage with other students.\nPonder/Prove: You should (a) ponder what you have learned, (b) record your learning, and (c) pursue unanswered questions and discuss what you learn with others.\n\nIf you feel confused or have questions about anything in the lesson, take immediate action (Exercise Faith; Act for Themselves) and talk with your classmates, the teaching assistant, or the instructor (Love, Serve, and Teach One Another).\nTeach One Another\nAt BYU-Idaho, an “A” student will demonstrate “diligent application of Learning Model principles, including initiative in serving other students” (BYU-Idaho Catalog). In this class, you will have the opportunity to work with other students.\nDoctrine and Covenants 84:106 states, “And if any man among you be strong in the Spirit, let him take with him him that is weak, that he may be edified in all meekness, that he may become strong also.” In the spirit of this revelation, you will have the opportunity to help others in the class when you have developed an understanding of a principle. Likewise, you will be able to receive help from others (peers, tutors, TA, and your instructor) when you are still working to understand concepts.\nIn a spirit of love and service, please reach out to others. You are not graded on a curve. If someone else does well, it does not affect you adversely. Research has shown that students who help other students to understand the material gain a much deeper grasp on the concepts of the course. Please take opportunities to help your peers succeed."
  },
  {
    "objectID": "Textbook/Course_Introduction.html#course-structure",
    "href": "Textbook/Course_Introduction.html#course-structure",
    "title": "Course Introduction",
    "section": "Course Structure",
    "text": "Course Structure\nThis course consists of 24 lessons. They are presented in a topical order in which concepts and skills learned in the earlier lessons provide the requisite knowledge to succeed in later lessons. If the general order of the lessons doesn’t make sense at first, don’t worry. It will all come together in the end, and you’ll see the reasoning behind why the lessons have been presented in this particular order.\nYour main goal as a student will be to complete all of the learning activities within each lesson by their due dates every week. These activities follow a consistent weekly schedule, and it will be up to you to make sure that you keep on pace with all your assignments. These weekly activities may include the following:\n\nReading assigned texts or viewing presentations.\nTaking quizzes.\nParticipating in group discussions and assignments with other class members.\nWriting papers and/or developing presentations.\nParticipating in meetings with the instructor, teaching assistants, and other students.\n\nFor many of these activities, the due dates will fall on the same time each week. This will make it easier for you to plan out your weekly study schedule. However, there may be a need to make adjustments to the schedule from time to time. If in doubt, refer to the due dates your instructor has posted in I-learn.\nYou should create a study schedule that will keep you on pace throughout the semester. This is a rigorous course with a lot of subject matter to cover, and it can be extremely difficult to recover if you fall too far behind in your work. So, please make every effort to study on a regular basis and get your work turned in on time.\nThe lessons in this course have a similar structure and contain similar basic elements. A typical week consists of two lessons. Each lesson will consist of a reading assignment, a reading/preparation quiz, and a homework quiz.\nThe structure of this course fully integrates the BYU-Idaho Learning model with a mixture of preparation activities, teach-one-another activities, and ponder-and-prove activities."
  },
  {
    "objectID": "Textbook/Course_Introduction.html#course-materials",
    "href": "Textbook/Course_Introduction.html#course-materials",
    "title": "Course Introduction",
    "section": "Course Materials",
    "text": "Course Materials\nThis course has been designed with the student in mind. Every effort has been made to provide a high quality experience at the lowest possible cost.\nTextbook\nTo keep costs as low as possible for students and their families, no physical textbook is required for this class. The readings for this course are provided on this website and will continue to be available to you after the course is completed. Please report any problems with the textbook (links not working, loading slowly, inability to view images, etc.) to your instructor. A link to the textbook is found in the Quick Links module. It is highly recommended you bookmark the textbook so that you can easily reference each lesson’s reading.\nComputer Equipment\nYou will need: - A laptop - Access to Microsoft Excel 2016 or later"
  },
  {
    "objectID": "Textbook/Course_Introduction.html#course-resources",
    "href": "Textbook/Course_Introduction.html#course-resources",
    "title": "Course Introduction",
    "section": "Course Resources",
    "text": "Course Resources\nPeer Support\nYour experience in this course will be enhanced as you work with other students to learn and grow together.\nHelp Desk\nThe BYU-Idaho Help Desk has been established to help students with technological problems related to approved course software. You can access the Help Desk at any time in three ways: - Walk-in: The Help Desk is located in room 322 of the McKay Library - Call in: 208-496-1411 (toll free) - Email: helpdesk@byui.edu Additional information is available at the Help Desk web page: http://www.byui.edu/helpdesk/\nWhen you have technical problems with I-Learn, you should first try contacting the Help Desk before you contact your instructor. They are connected with the IT support staff who can resolve problems with I-Learn. Please take a moment now to look at the Help Desk web page. That way, if a problem does arise later on in the course, you will know where to go for help.\nTutoring Center\nThe BYU-Idaho Study Skills/Tutoring Center is a powerful resource for students who would like a little extra help with a course. The Tutoring Center is located in the McKay Library in room 272. This is in the east wing of the second floor.\nThe Tutoring Center provides many services to help students succeed: - Individual tutors - Walk-in tutoring in the Math Study Center (McKay 266 & 270) - Virtual tutoring\nPlease take 5 minutes to explore the Study Skills/Tutoring Center web site.\nFaculty Support\nYour instructor is committed to your success. If you have any needs or concerns, please contact your instructor for help. If you feel yourself getting behind or struggling, talk to your teacher right away. If caught in time, a small problem can be addressed quickly before it grows.\nWith all of that said, let’s begin looking at a foundational idea of statistics: probability."
  },
  {
    "objectID": "Textbook/Shape_and_Center.html",
    "href": "Textbook/Shape_and_Center.html",
    "title": "Describing Quantitative Data (Shape & Center)",
    "section": "",
    "text": "By the end of this lesson, you should be able to:\n\nCreate a histogram from data\nInterpret data presented in a histogram\nIdentify left-skewed, right-skewed, and symmetric distributions from histograms\nCalculate the mean, median, and mode for quantitative data using software\nCompare the centers of distributions using graphical and numerical summaries\nDescribe the effects that skewness or outliers have on the relationship between the mean and median\nDistinguish between a parameter and a statistic"
  },
  {
    "objectID": "Textbook/Shape_and_Center.html#lesson-outcomes",
    "href": "Textbook/Shape_and_Center.html#lesson-outcomes",
    "title": "Describing Quantitative Data (Shape & Center)",
    "section": "",
    "text": "By the end of this lesson, you should be able to:\n\nCreate a histogram from data\nInterpret data presented in a histogram\nIdentify left-skewed, right-skewed, and symmetric distributions from histograms\nCalculate the mean, median, and mode for quantitative data using software\nCompare the centers of distributions using graphical and numerical summaries\nDescribe the effects that skewness or outliers have on the relationship between the mean and median\nDistinguish between a parameter and a statistic"
  },
  {
    "objectID": "Textbook/Shape_and_Center.html#review-of-the-five-steps-of-the-statistical-process",
    "href": "Textbook/Shape_and_Center.html#review-of-the-five-steps-of-the-statistical-process",
    "title": "Describing Quantitative Data (Shape & Center)",
    "section": "Review of the Five Steps of the Statistical Process",
    "text": "Review of the Five Steps of the Statistical Process\nWe will use the five steps in the Statistical Process throughout the course. Recall the five steps (and the mnemonic “Daniel Can Discern More Truth) before you begin this lesson.\n\n\n\n\n\nStep 1:\n\n\n\n\nDaniel\n\n\n\n\nDesign the study\n\n\n\n\n\n\n\n\nStep 2:\n\n\n\n\nCan\n\n\n\n\nCollect data\n\n\n\n\n\n\nStep 3:\n\n\n\n\nDiscern\n\n\n\n\nDescribe the data\n\n\n\n\n\n\nStep 4:\n\n\n\n\nMore\n\n\n\n\nMake inferences\n\n\n\n\n\n\nStep 5:\n\n\n\n\nTruth\n\n\n\n\nTake action"
  },
  {
    "objectID": "Textbook/Shape_and_Center.html#shape-of-a-distribution",
    "href": "Textbook/Shape_and_Center.html#shape-of-a-distribution",
    "title": "Describing Quantitative Data (Shape & Center)",
    "section": "Shape of a Distribution",
    "text": "Shape of a Distribution\nCost to Treat Tuberculosis in India\n\n  Step 1: Design the study.\nTuberculosis (TB) is the deadliest bacterial disease in the world. In 2009, nine million new cases of tuberculosis were diagnosed, leading to almost 2 million deaths worldwide. Currently, the principal vaccine used to prevent tuberculosis is Bacille Calmette Guerin (BCG). Unfortunately, BCG is only moderately effective at preventing tuberculosis. Historically, India has had a high number of tuberculosis cases. The Indian Government wants to reduce the prevalence of this disease.\nIn this activity, you will compare the average costs of treating a person who contracts tuberculosis to the costs of preventing a case of tuberculosis in India.\n  Step 2: Collect data.\nHealth Care records of tuberculosis patients in India were surveyed to estimate the cost to treat patients with tuberculosis. The following data are representative of the total costs (in US dollars) incurred by society in the treatment of 10 randomly selected tuberculosis patients in India.\n\n15,100     19,000     4,800     6,500     14,900     600     23,500     11,500     12,900     32,200\n\nThese costs include health care treatment, time missed from work, and in some cases utility lost due to death.\n  Step 3: Describe the data.\n\nVisualizing Quantitative Data: Histograms\nThe following data are representative of the total costs (in US dollars) incurred by society in the treatment of 10 randomly selected tuberculosis patients in India.\n\n15,100     19,000     4,800     6,500     14,900     600     23,500     11,500     12,900     32,200\n\nTo help us visualize these data, we will create a graph called a histogram. To make a histogram, we will divide the number line from 0 to 35,000 in seven equal parts. We will then count the number of data points in each of these intervals:\n\n\n\n\n\n\nInterval\n\n\n\n\nNumber of Observations\n\n\n\n\n\n\n\n\nAt least 0 and less than 5,000\n\n\n\n\n2\n\n\n\n\n\n\nAt least 5,000 and less than 10,000\n\n\n\n\n1\n\n\n\n\n\n\nAt least 10,000 and less than 15,000\n\n\n\n\n3\n\n\n\n\n\n\nAt least 15,000 and less than 20,000\n\n\n\n\n2\n\n\n\n\n\n\nAt least 20,000 and less than 25,000\n\n\n\n\n1\n\n\n\n\n\n\nAt least 25,000 and less than 30,000\n\n\n\n\n0\n\n\n\n\n\n\nAt least 30,000 and less than 35,000\n\n\n\n\n1\n\n\n\n\n\n\nFor each of these intervals, called bins, we draw a bar on the histogram. The width of the bars is determined by the width of the bin (5000 in this example). The height of the bars is equal to the number of observations that fall in each bin. As we look at the histogram shown below, we see bars ranging from $0 to $35,000. We also see higher bars in the middle between $10,000 to $20,000 show that these values are more commonly occurring than the other values. If we computed the average of the values contained in our histogram, we would compute the number \\[\n  \\frac{15,100 + 19,000 + 4,800 + 6,500 + 14,900 + 600 + 23,500 + 11,500 + 12,900 + 32,200}{10} = 14,100\n\\] showing that the center of the histogram (or average) is at $14,100.\n\nThis is a histogram created in R: \n\n\n\nTo create this histogram in R, you can copy and paste the following code into R:\n\n# Create a dataset with the costs from 10 randomly selected patients:\ndata &lt;- c(15100, 19000, 4800, 6500, 14900, 600, 23500, 11500,12900, 32200)\n\n# Create a histogram. We add x-axis labels using `xlab = \"\"` and a title `main = \"\"`\nhist(c(15100, 19000, 4800, 6500, 14900, 600, 23500, 11500,12900, 32200), xlab=\"Treatment Costs\", main = \"Tuberculosis Costs in India\")\n\n\n\n\n\n\n\n\n\n\nMaking Inference About the Population\nAfter summarizing the data from our sample of the populations both numerically and graphically, we can use this information to make inference about the full population. \n  Step 4: Make inferences.\nIn the past, the total average cost to society to treat a case of tuberculosis in India was known to be $13,800. As shown in our Step 3 calculations, the 10 randomly selected patients showed an average cost that was higher than the historic value at $14,100. This might make us believe that the actual total average cost to society is also $14,100. However, in depth statistical calculations (that you will be taught how to do later this semestr) show that there is a 46% chance that our sample had an average of $14,100 just by random chance. This isn’t too hard to believe since we only had a sample size of 10 people, and $14,100 is only $300 above $13,800, so it turns out to be fairly likely (46% chance) that because of random chance our sample had an average that was a little higher than the actual value from the population. So we will conclude that the total average cost to society is still essentially the same as it has been in the past.\n\n  Step 5: Take action.\nAfter making inferences, you take action. The motivation for conducting a study like this is usually to see if there is inflation in the costs.\n\nAnswer the following question:\n\n\n\nGiven our conclusion in Step 4 (that the results of our random sample being at an average $14,100 had a 46% probability of just being caused by random chance) do you think the Government of India needs to take any special action to stop the increase in the cost to treat tuberculosis?\n\n\n\nShow/Hide Solution\n\n\nAnswers may vary. – However, we could not say that the true mean cost has really changed from $13,800. So, there is not enough evidence of inflation. There is no need for the Government of India to take action.\n\n\n\n\nOne benefit of using a histogram is that it allows you to visualize the distribution of the data. A histogram illustrates the overall shape of the distribution of the data. The height of the bars show how many observations fall in that range.\n\nAnswer the following question:\n\n\n\nWhich bin of the histogram of tuberculosis costs contained the most data points?\n\n\n\nShow/Hide Solution\n\n\nThe bin going from $10,000 to $15,000 contained 3 observations ($11,500, $12,900, and $14,900), which was the most of any of the bins in the histogram. This can be seen visually in the histogram by looking at the height of each bar and the starting and stopping points of the bar along the x-axis of the graph.\n\n\n\n\nWe will describe the shape of the distribution of a data set using the following basic categories: symmetric, bell-shaped, skewed right, and skewed left. Additionally, we can label the shape of a distribution as uniform, unimodal, bimodal, or multimodal.\nA distribution is symmetric if both the left and right side of the distribution appear to be roughly a mirror image of each other. A special symmetric distribution is a bell-shaped distribution. When data follow a bell-shaped distribution, the histogram looks like a bell. Bell-shaped distributions play an important role in Statistics and will play a role in most of the future lessons.\nA distribution is right-skewed if a histogram of the distribution shows a long right tail. This can occur if there are some very large outliers on the right-hand side of the distribution. A distribution is left-skewed if a histogram shows that it has a long tail to the left.\n\n\n\nRight-skewed\n\n\nSymmetric & Bell-shaped\n\n\nLeft-skewed\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMean: $10.45\nMedian: $9.04\nMean is to the right of the median.\n\n\nMean: 71.1 inches\nMedian: 71 inches\nMean and median are roughly equal.\n\n\nMean: 3.42\nMedian: 3.45\nMean is to the left of the median.\n\n\n\n\nIf a distribution has only one peak, it is said to be unimodal. The three distributions illustrated above are all unimodal distributions. Some people might argue that there are several peaks in the GPA data, so it should not be considered unimodal. Even though there are jagged bumps in the histogram, it is important to visualize the overall shape in the data. When interpreting a histogram, it can be helpful to blur your eyes and imagine the overall shape after smoothing out the bumps. If the overall trend indicates that there is more than one bump, then we do not consider the distribution to be unimodal. We will usually only work with unimodal data sets in this course.\nSome distributions have no distinct peak, others have more than one peak. When there is no distinct peak, and the histogram shows a relatively flat shape, we might say the data follow a uniform distribution. If there are two distinct peaks, a distribution is called bimodal. If there are more than two peaks, we refer to the distribution as multimodal."
  },
  {
    "objectID": "Textbook/Shape_and_Center.html#center-of-a-distribution",
    "href": "Textbook/Shape_and_Center.html#center-of-a-distribution",
    "title": "Describing Quantitative Data (Shape & Center)",
    "section": "Center of a Distribution",
    "text": "Center of a Distribution\n\nStep 3: Describe the data.\nSometimes people talk about the “typical” BYU-Idaho student or the average waiting time for a bus. But what does it mean for something or someone to be “average?” How can we quantify what it means to be typical or average? In the example below, we will explore one way to define what “average” means.\nWhen we talk about the “typical” or “average” value, we are essentially describing the center of a population. If we want to estimate the “average” costs to treat a tuberculosis patient, there are several ways we can do it.\n\n\nMeasuring the Center of a Distribution\n\nMean\nThe sample mean or sample arithmetic mean is the most common tool to estimate the center of a distribution. It is referred to simply as the mean. It is computed by adding up the observed data and dividing by the number of observations in the data set.\nIn Statistics, important ideas are given a name. Very important ideas are given a symbol. The sample mean has both a name (mean) and a symbol (\\(\\bar x\\), called “x-bar”).\n\\[\n  \\bar{x} \\text{ is used to denote the sample mean}\n\\]\nYou may have heard people refer to the sample mean as the average. Technically, the word average refers to any number that is used to estimate the center of a distribution. The mean, median and mode are all examples of “averages.” To avoid confusion, it is best to use the words mean, median, and mode instead of the word average, so that it is clear which “average” your are referencing.\n\nAnswer the following question:\n\n\n\nPractice finding the mean, \\(\\bar x\\), for the tuberculosis treatment costs of the 10 patients in India by simplifying the following: \\[\\bar x=\\frac{15100 + 19000 + 4800 + 6500 + 14900 + 600 + 23500 + 11500 + 12900 + 32200}{10}=\\]\n\n\n\nShow/Hide Solution\n\n\nThe mean cost to treat the 10 TB patients in India is: \\(\\bar x = \\$14,100\\). To see how to calculate the mean in Excel, see the “Excel Instructions” below.\n\n\n\n\n\n\nMedian\nThe median is the middle value in a sorted data set. Half of the observations in the data set are below the median and half are above the median. To find the median, you:\n\nSort the values from smallest to largest\n\nDo one of the following:\n\nIf there are an odd number of values, the median is the middle value in the sorted list.\nIf there are an even number of values, the median is the mean of the two middle values in the sorted list.\n\n\n\n\nAnswer the following questions:\n\n\n\nPractice finding the median of the tuberculosis treatment costs for the 10 patients in India. First, sort the data from smallest to largest.\n\n\n\nShow/Hide Solution\n\n\n600\n4800\n6500\n11500\n12900\n14900\n15100\n19000\n23500\n32200\n\n\n\n\nSince there are an even number of observations (n=10), the median is computed as the mean of the middle two values. Use your answer to the previous question to find the median of the data. What is the median?\n\n\n\nShow/Hide Solution\n\n\n600\n4800\n6500\n11500\n12900\n14900\n15100\n19000\n23500\n32200\n\nThe middle two numbers are 12900 and 14900. The mean of these two numbers is:\n\n\\(\\text{Median } = \\frac{12900 + 14900}{2} = 13900\\)\n\nThe median cost to treat the ten TB patients in India is $13,900.\n\n\n\n\n\nMode\nThe most frequently occurring value is called the mode. Sometimes there is more than one mode. For example, in the data set\n\\[{1,~~2, ~~2, ~~2, ~~3, ~~4, ~~4, ~~5, ~~5, ~~5, ~~6}\\]\nthe modes are 2 and 5. Both of these values occur three times, which is more times than any other value.\nIf no number occurs more than once in the data set, we say that there is no mode. For the data set representing the costs to treat tuberculosis in India, none of the values is repeated. So, there is no mode for these data.\n\nAnswer the following question:\n\n\n\nFor a particular data set, which of the following can occur?\n\n\nThere may be no mode.\nThere may be exactly one mode.\nThere may be several modes.\nOnly A and B can occur.\nA, B, and C can all occur.\n\n\n\nShow/Hide Solution\n\n\nA, B, and C can all occur.\n\n\n\n\n\n\nExcel Instructions for Mean, Median, and Mode\n\nR Instructions\n\n\n\nTo calculate most numerical summaries (such as the mean, median, and mode) in Excel, follow these general steps:\n\nOpen R.\nEnter the data using the “assignment operator”, &lt;-, and c() which establishes a “collection” of things, in this case numbers.\n\nCalculate summary statisti\nThen, highlight the data (by clicking on it) to which you want to apply the function. The cell reference range will automatically be added to your formula. Then type a closed parenthesis, “)” and hit enter.\n\nCaculate a Mean\nFor example, to calculate the mean of the sample of tuberculosis patient costs in India:\n\n# Create a dataset called `data` with the costs from 10 randomly selected patients:\ndata &lt;- c(15100, 19000, 4800, 6500, 14900, 600, 23500, 11500,12900, 32200)\n\nmean(data)\n\n[1] 14100\n\n\n\nCalculate a Median\nSimply replace the word “mean” in the formula with the word “median”. Try it with the tuberculosis patient data, you should get the same value that was calculated by hand above.\n\nmedian(data)\n\n[1] 13900\n\n\n\nCalculate a Mode\nR makes it difficult to use the mode because there are fewer situations when it is useful for quantitative data where few values repeat. In the rare occasion we are not interested in a mean or median, we can tabulate the frequency of specific values using the table() function:\n\n# Create a new dataset called data2:\ndata2 &lt;- c(3,4,9,5,2,3,5,4,2,3,1,5,3,1,2,6,2,4,6,2,2,2,9,1,2,7,8)\n\n# The `table()` function counts up all the times specific values show up.  This works for numbers or categories:\ntable(data2)\n\ndata2\n1 2 3 4 5 6 7 8 9 \n3 8 4 3 3 2 1 1 2 \n\n\nThe first row of the table() output is the value being counted. The second row is the frequency of occurrence.\nWhich value is most frequently occurring?\nIf there are lots of occuring values, we can use R to sort() the table output to make it easier to see which is the mode:\n\nsort(table(data2))\n\ndata2\n7 8 6 9 1 4 5 3 2 \n1 1 2 2 3 3 3 4 8 \n\n\nThe rows are the same as before but are sorted in ascending order. We can now easily see that 2 occurred 8 times making 2 the mode.\n\n\n\nParameters and Statistics\nWe only have data on the cost to treat ten randomly selected tuberculosis patients. This represents a random sample from the population. The sample obtained by the researchers depends on random chance. If the study was repeated and a new sample of ten patients was randomly drawn from all cases of tuberculosis in India, would we observe the same data values? Certainly not!\nHowever, if we took a second random sample from the population, we would expect the mean of the new sample to be somewhat similar to the mean for our original sample. And if we took a third sample of data, we should expect the mean of this sample to be different than the means of the other two samples. In fact, every sample will give us a different sample mean, but all of these sample means will be fairly similar in value.\nOne of the primary purposes of collecting and analyzing data is to estimate the true mean of a population. Since collecting data on the entire population is usually not feasible, we usually never know what the true mean is. So we estimate the true population mean with the sample mean from a single sample of data from the population.\nThe sample mean is an example of a statistic. A statistic is a number that describes a sample. The true (usually unknown) population mean is an example of a parameter. A parameter is any number that describes a population.\nAn easy way to distinguish between a parameter and a statistic is to note the repetition in the first letters:\n\nPopulation Parameter True (usually unknown) value describing a population\nSample Statistic Estimate of the population parameter obtained from a sample\n\nIn the example above, the sample mean \\(\\bar x\\) = $14,100 is a statistic. Over the last few years, the total mean cost to treat tuberculosis in India has been $13,800. This $13,800 is considered a parameter because it is the “known” value for the full population.\nDifferent symbols are used to distinguish between the sample mean (a statistic) and the population mean (a parameter). The symbol for the sample mean is \\(\\bar x\\). The symbol for the population mean is \\(\\mu\\).\nPerspective\nThe mean cost to treat the ten tuberculosis patients in the sample was \\(\\bar x\\) = $14,100. This number gives us some useful information. However, if this was all we were given, we would not be able to distinguish the data above from a situation where the cost for each of the ten patients was exactly $14,100. Notice that if the cost for each patient was $14,100, the mean would be:\n\\[\\bar x=\\frac{14100 + 14100 + 14100 + 14100 + 14100 + 14100 + 14100 + 14100 + 14100 + 14100}{10} =14,100\\]\nEven though measures of center are important, we need to consider the shape, center and spread of a distribution of data. When evaluating data, it is sometimes tempting to compute a mean but to avoid creating a histogram. This can lead to errant decisions based on a misunderstanding or incorrect transcription of data. If there is a transcription error in the data, it is sometimes easiest to detect it as an outlier in a histogram."
  },
  {
    "objectID": "Textbook/Shape_and_Center.html#summary",
    "href": "Textbook/Shape_and_Center.html#summary",
    "title": "Describing Quantitative Data (Shape & Center)",
    "section": "Summary",
    "text": "Summary\n\nRemember…\n\n\n\n\nHistograms are created by dividing the number line into several equal parts, starting at or below the minimum value occurring in the data and ending at or above the maximum value in the data. The number of data points occurring in each interval (called a bin) are counted. A bar is then drawn for each bin so that the height of the bar shows the number of data points contained in that bin.\nA histogram allows us to visually interpret data to quickly recognize which values are most common and which values are least common in the data.\nHistograms can be left-skewed (the majority of the data is on the right of the histogram, less common values stretch to the left side), right-skewed (majority of the data is on the left side with less common values stretching to the right), or symmetrical and bell-shaped (most data is in the middle with less common values stretching out to either side).\nThe mean, median, and mode are measures of the center of a distribution. The mean is the most common measure of center and is computed by adding up the observed data and dividing by the number of observations in the data set. The median represents the 50th percentile in the data. The mean can be calculated in R using mean(...), the median by using median(...), and the mode by table(...) where the ... in each case consists of the data.\nWhen comparing the centers of distributions using graphical and numerical summaries, the direction of the skew showing in the histogram will generally correspond with the mean being pulled in that direction.\n\n\n\n\nRight-skewed\n\n\nSymmetric & Bell-shaped\n\n\nLeft-skewed\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMean: $10.45\nMedian: $9.04\nMean is to the right of the median.\n\n\nMean: 71.1 inches\nMedian: 71 inches\nMean and median are roughly equal.\n\n\nMean: 3.42\nMedian: 3.45\nMean is to the left of the median.\n\n\n\n\n\nIn a symmetrical and bell-shaped distribution of data, the mean, median, and mode are all roughly the same in value. However, in a skewed distribution, the mean is strongly influenced by outliers and tends to be pulled in the direction of the skew. In a left-skewed distribution, the mean will tend to be to the left of the median. In a right-skewed distribution, the mean will tend to be to the right of the median.\nA parameter is a true (but usually unknown) number that describes a population. A statistic is an estimate of a parameter obtained from a sample of the population."
  },
  {
    "objectID": "Textbook/Stat_Process.html",
    "href": "Textbook/Stat_Process.html",
    "title": "The Statistical Process",
    "section": "",
    "text": "By the end of this lesson, you should be able to:\n\nDescribe the five steps of the Statistical Process\nDistinguish between an observational study and an experiment\nDifferentiate between a population and a sample\nDescribe each of the following sampling schemes:\n\nSimple random sampling\nStratified sampling\nSystematic sampling\nCluster sampling\nConvenience sampling\n\nExplain the importance of using random sampling\nDistinguish between a quantitative and a categorical variable"
  },
  {
    "objectID": "Textbook/Stat_Process.html#lesson-outcomes",
    "href": "Textbook/Stat_Process.html#lesson-outcomes",
    "title": "The Statistical Process",
    "section": "",
    "text": "By the end of this lesson, you should be able to:\n\nDescribe the five steps of the Statistical Process\nDistinguish between an observational study and an experiment\nDifferentiate between a population and a sample\nDescribe each of the following sampling schemes:\n\nSimple random sampling\nStratified sampling\nSystematic sampling\nCluster sampling\nConvenience sampling\n\nExplain the importance of using random sampling\nDistinguish between a quantitative and a categorical variable"
  },
  {
    "objectID": "Textbook/Stat_Process.html#introduction",
    "href": "Textbook/Stat_Process.html#introduction",
    "title": "The Statistical Process",
    "section": "Introduction",
    "text": "Introduction\nStatistics are used in every aspect of society. Every statistical analysis follows a pattern we will call the Statistical Process. This process will be introduced in this lesson and will be used throughout the course.\n\n\nThe Statistical Process and Daniel’s Experiment\n\n\n\nStained-glass depiction of Daniel’s deliverance from the lions’ den. Found in the old Dominican priory church at Hawkesyard in Staffordshire, England. (Photo credit: Fr Lawrence Lew, O.P. Used by permission.)\n\n\n\nThe Old Testament prophet Daniel planned one of the earliest recorded scientific research studies. We will use his example to illustrate the following five steps of The Statistical Process.\nThe following icons can help you remember these steps. Notice that each icon has a letter and an image to help you remember the five steps of the Statistical Process.\n\n\n\n\n\n\n\n\n\n\n\nThe Statistical Process\n\n\n\n\n\n\nDesign the Study\n\n\n\nCollect the Data\n\n\n\nDescribe the Data\n\n\n\nMake Inference\n\n\n\nTake Action\n\n\n\n\n\n\n\nStep 1: Design the Study\n\nAn important step in scientific inquiry or problem solving can be to state a research question such as:\n\nWill internet advertising increase a company’s revenue?\nDoes expressing gratitude increase a person’s satisfaction with life in general?\nDoes a newly developed vaccine prevent the spread of disease?\n\nResearchers also investigate the background of the situation. What have other people discovered about this situation? How can we find the answer to the research question? What do we need to do? What is the population (or total collection of all individuals) under consideration? What kind of data need to be collected?\nBefore collecting data, researchers make a hypothesis, or an educated guess about the outcome of their research. A hypothesis is a statement such as the following:\n\nUsing internet advertising will increase the company’s sales revenue.\nPeople who express gratitude will be more satisfied with life than those who do not.\nA newly-developed vaccine is effective at preventing tuberculosis.\n\n\n\nDaniel’s Experiment\nAfter taking Israel captive, Babylon’s King Nebuchadnezzar asked his chief officer to bring Israelite children who were well favoured, and skilful in all wisdom, and cunning in knowledge, and understanding science to stand in the king’s palaces (Daniel 1:4). To aid their preparation, Nebuchadnezzar planned to feed them his meat and wine for three years (Daniel 1:5).\nDaniel did not want to defile himself by partaking of the king’s meat and wine. He asked permission to eat pulse[^1] and drink water instead. His supervisor, Melzar, was afraid to displease the king. He thought that after eating pulse and water, the selected Israelites would look worse than their peers, and he would be punished (Daniel 1:8-10).\nWith an understanding of the background of the situation, Daniel proposed an experiment. He said, Prove thy servants, I beseech thee, ten days; and let them give us pulse to eat, and water to drink. Then let our countenances be looked upon before thee, and the countenance of the children that eat of the portion of the king’s meat: and as thou seest, deal with thy servants (Daniel 1:12-13.). In short, Daniel’s implied research question can be stated as: Will those who eat pulse and drink water appear healthier than those who eat the king’s meat and drink his wine? Melzar agreed to the experiment.\n\nAnswer the following question:\n\n\n\nWhat is Daniel’s hypothesis?\n\n\n\nSolution\n\nDaniel’s hypothesis is that the Israelite children who eat pulse and drink water will appear healthier in just ten days, compared to those who eat the king’s meat and drink his wine.\n\n\n\n\n\n\nStep 2: Collect Data\n\nWhen designing a study, much attention is given to the process by which data are observed. When examining data, it is also important to understand the data collection procedures. A sample is a subset (a portion) of a population. How is this sample obtained? How are the observations made?\nDaniel’s study design required that data be collected at the end of 10 days. Melzar would compare the appearances of two groups of people: (1) Israelites who ate pulse and drank water versus (2) Israelites who ate the king’s meat and drank his wine.\n\n\n\nStep 3: Describe the Data\n\nWhen we describe data, we use any tools appropriate to the situation. This can include creating graphs or calculating statistics to help understand or visualize the data.\nFor Daniel’s experiment, the data are described in Daniel 1:15: And at the end of ten days [the] countenances [of those who ate pulse] appeared fairer and fatter in flesh than all the children which did eat the portion of the king’s meat.\n\n\n\nStep 4: Make Inferences\n\nInference is the process of using the information contained in a sample from a population to make a general statement (i.e. to infer something) about the entire population. Later in the course we will learn techniques that make this type of analysis possible.\nMelzar made an inference. Based on the results of the sample, he determined that (in general) those who eat pulse and drink water will be healthier than those who eat the king’s meat and drink his wine Daniel 1:15-16.\n\n\n\nStep 5: Take Action\n\nThe goal of a statistical analysis is to determine which action to take in a particular situation. Actions can include many things: launching an internet ad campaign (or not), expressing gratitude (or not), getting vaccinated (or not), etc.\nMelzar took action as described in Daniel 1:16: Thus Melzar took away the portion of their meat, and the wine that they should drink; and gave [all the Israelite children] pulse.\nWas the experiment a success?\n“Now at the end of the days that the king had said he should bring them in… the king communed with them; and among them all was found none like Daniel, Hananiah, Mishael, and Azariah And in all matters of wisdom and understanding, that the king enquired of them, he found them ten times better than all the magicians and astrologers that were in all his realm” Daniel 1:18-20.\n\n\n\nSummary of the Statistical Process\n\nDaniel’s experience can also help you learn the Statistical Process. Look at the first letter of each of the steps in the Statistical Process. You can use the phrase “Daniel Can Discern More Truth” to help you to help you remember the five steps in the Statistical Process.\n\nThe Statistical Process\n\n\n\n\n \nPneumonic\nActual Process Step\n\n\n\n\nStep 1:\nDaniel\nDesign the study\n\n\nStep 2:\nCan\nCollect data\n\n\nStep 3:\nDiscern\nDescribe the data\n\n\nStep 4:\nMore\nMake inferences\n\n\nStep 5:\nTruth\nTake action\n\n\n\n\nThe Statistical Process will be used throughout the course. Take time to memorize the five steps.\n\n\nThe study designed by the Old Testament prophet Daniel provides an ancient example of a designed experiment. Daniel’s experiment included two groups of people: those who had the experimental treatment eating pulse and drinking water (called the treatment group) and those who ate the standard food the king’s meat (called the control group.) The treatment group receives the experimental procedure. The control group is used for comparison.\n\nAnswer the following question:\n\n\n\nWhy was it important that Daniel’s experiment included a control group?\n\n\n\nSolution\n\nIf there was no control group, then there would be no way to compare the effect of the diets (the treatments). Having a control group allows a researcher to see the effect of not taking any action. For Daniel, the control group (who ate the king’s meat and drank his wine) provided a basis for comparing the effect of the new treatment (i.e. eating pulse and drinking water.)"
  },
  {
    "objectID": "Textbook/Stat_Process.html#design-of-studies",
    "href": "Textbook/Stat_Process.html#design-of-studies",
    "title": "The Statistical Process",
    "section": "Design of Studies",
    "text": "Design of Studies\nMost research projects can be classified into one of two basic categories: observational studies or designed experiments. In an experiment, researchers control (to some extent) the conditions under which measurements are made. In an observational study, researchers simply observe what happens, without controlling the conditions under which measurements are made. Both types of study follow the five steps of the Statistical Process.\n\n\nDesigned Experiments\nIn a designed experiment, researchers manipulate the conditions that the participants experience. They often do this by randomly assigning subjects to one of two groups, a “treatment” group (sometimes called the experimental group) and a “control” group (though this could be second treatment group instead of a control group). The experiment is typically conducted by applying some kind of treatment to the subjects in the treatment group and observing the effect of the treatment. Those in the control group do not receive the treatment and are also observed. In this way researchers can determine the effects of the treatment by comparing the treatment group results to the control group results. The following example illustrates the use of these two groups.\nJonas Salk’s First Polio Vaccine Trial\nBeginning around 1916 and through the 1950s, a mysterious plague attacked infants and children. Symptoms included excruciating muscle pain and a stiff neck. This illness, which became known as poliomyelitis or simply “polio,” left children disfigured, paralyzed, and sometimes even dead.\nWhile working as a researcher at the University of Pittsburgh School of Medicine, Dr. Jonas E. Salk developed a vaccine that might help prevent the spread of this disease. He conducted what has become one of the most famous designed experiments in history.\nThis short video below provides a compelling summary of the famous Jonas Salk vaccine experiment. As you watch, notice each of the 5 steps of a statistical study in this study.  \nAs explained in the video, in the first Salk trial almost 1.1 million children participated in the study. Even though the sample size was large, flaws in the study design rendered the results useless.\nUndaunted, Dr. Salk fixed the problems with the design and enrolled hundreds of thousands of additional children for the second phase of his study. In all, over 1.8 million infants and children participated in this experiment, making it the largest drug trial to date.\n\nStep 1: Design the study.\nThe participants in a study are commonly called subjects. Sometimes subjects are called experimental units or simply units. In the Salk trials, the children who participated were the subjects.\nSubjects (the children) were randomly assigned to one of two groups. The first group was given the experimental vaccine, the treatment. The treatment is the new or experimental condition that is imposed on the subjects. The subjects who receive the treatment make up the treatment group.\nThe second group was given a control or placebo. In this study, the control was an injection that looked just like the vaccine, but contained a harmless saline solution. The control group or placebo group is made up of the subjects assigned to receive the control.\nThis study was double blind. Neither the children’s parents nor their doctors knew whether a particular child received the treatment or the control. Both parties were blinded to this information.\nBecause the children were assigned to the groups randomly, the two groups should be similar. If the vaccine is not effective, the number of future cases of polio should be about the same in each group. However, if Salk’s vaccine helped to prevent the spread of polio, then fewer cases should occur in the vaccinated group.\n\nAnswer the following questions:\n\n\n\nSome children can be identified as having a higher risk of developing polio. Would it have been better if they were assigned to the treatment group so they could get the vaccine?\n\n\n\nSolution\n\n\nNo. The two groups need to be as similar as possible. Specifically, the people in the treatment group need to have the same potential (on average) of contracting polio as the people in the control group. If we put the people who are at a higher risk of developing polio in the treatment group, we run the risk of having more people in the treatment group getting polio simply because they are more likely to get it, whether they are vaccinated or not. Likewise, we might have fewer people in the control group getting polio just because they are less likely to get it, whether they are vaccinated or not.\nThese two effects would create a bias against the vaccine, by making the vaccine look like it doesn’t work, or doesn’t work as well as it does. It might also make it appear that people who aren’t vaccinated stay healthy and the vaccine is not needed. There is even a chance that people will conclude that the vaccine actually gives people polio.\nRandomly assigning subjects to the two groups tends to yield groups with similar characteristics—in this example, similar potential for contracting polio. Randomly assigning subjects to groups therefore defends us against problems like those mentioned in the previous paragraph.\n\n\n\nWhy is it important for the subject and those who assess the health of the subject to be unaware of whether or not that child received the vaccine?\n\n\n\nSolution\n\n\nSubjects: Suppose a subject in the study thinks they’re being treated. It has been documented that subjects with such knowledge tend to show improvement whether they are receiving the treatment or not. To see why, consider how you might feel and act if you were told you had been vaccinated. You might have a more hopeful outlook, leading to healthier living habits such as better hygiene and nutrition. Such changes would tend to reduce your chance of contracting polio whether you’ve received the vaccine or not. This might make the vaccine look like it works better than it does. It also might make the vaccine look like it works, even if it doesn’t.\nNow suppose subjects in the control group know they are not being treated. This can also change the way they feel and act, in ways that can make them more likely to contract polio than they would be if they weren’t in the study. This could make it look like the incidence of polio among unvaccinated persons is higher than it is, again making the vaccine look like it works better than it does.\nTo reduce bias caused by such errors, subjects should not know to which group they are assigned.\nResearchers: Suppose a researcher assessing the health of a subject is told that the subject is in the control group. It has been documented that in such a case, the researcher is more likely to record that the subject has symptoms even if the subject is not actually in the control group. This makes it look like unvaccinated persons are more likely to get polio than they really are, which makes it look like the vaccine works better than it does.\nThere are other effects of knowing to which group the subject belongs, such as doctors treating or advising the patient differently than they would without such knowledge. Such differences can make it harder to tell whether the vaccine works, and how well.\nTo reduce bias caused by such effects, those assessing the health of the subjects should not be told to which group the subject belongs.\n\n\n\n\nStep 2: Collect data.\nThe researchers followed up with each child to determine if they contracted polio. They recorded the number of children in each group that developed polio during the study period. Not all of Salk’s experiments were double-blind. Here is a summary of the results from the regions where a double-blind study was conducted (Francis et al., 1955; Brownlee, 1955):\n\n\nChildren Who Developed Polio\n\n\n\n\n\n\n\nYes\n\n\n\n\nNo\n\n\n\n\nTotal\n\n\n\n\n\n\n\n\nTreatment Group\n\n\n\n\n57\n\n\n\n\n200,688\n\n\n\n\n200,745\n\n\n\n\n\n\nPlacebo Group\n\n\n\n\n142\n\n\n\n\n201,087\n\n\n\n\n201,229\n\n\n\n\n\n\nStep 3: Describe the data.\nOne way to summarize the data is to compute the proportion of children in each group that developed polio. The proportion of children in the treatment group that developed polio during the study period is:\n\\[ \\frac{57}{200745} = 0.000~283~9 \\]\n\nAnswer the following questions:\n\n\n\nCalculate the proportion of children in the placebo group that developed polio during the study period.\n\n\n\nSolution\n\n\n\\[ \\displaystyle{\\frac{142}{201229} = 0.000~705~7} \\]\n\n\n\nCompare the two proportions. What do you observe?\n\n\n\nSolution\n\n\nThe proportion of children in the placebo group that develop polio during the study period was more than double the proportion of children in the treatment group that developed polio during the study period. That suggests that the treatment is effective in reducing the proportion of children that will develop polio.\n\n\n\n\nStep 4: Make inferences\nCareful statistical analysis of the records suggested that this difference was so great that it was attributable to the vaccine and not to chance. Assuming that the vaccine had no effect, the probability that the difference in the proportions between the two groups would be at least as extreme as the difference Dr. Salk observed was very low: 0.00000000093. Because this probability is so small, it is highly unlikely that these results are due to chance.\n\n\nStep 5: Take action\nOnce it was clear that the vaccine was effective, children who were unvaccinated or had received the placebo were given Salk’s vaccine. Since 1954, there has been a marked decrease in the number of polio cases worldwide (Offit, 2005). Public health researchers are striving to eradicate this disease entirely.\n\n\n\nObservational Studies\nIn an observational study researchers observe the responses of the individuals, without controlling the conditions experienced by the individuals. Therefore, they do not assign the participants to treatment or control groups.\nObservational studies commonly occur in business settings. One example is a financial audit. The purpose of a financial audit is to assess the accuracy of a company’s financial business practices. ImmunAvance Ltd., a non-government health care organization, hired the Accounting Office at Global Optimization Unlimited to perform an independent audit of their financial practices. ImmunAvance provides inoculation and other preventative health care services in rural African communities.\n\n\n\nStep 1: Design the study\nThe volume of financial transactions conducted by ImmunAvance makes it impossible to conduct a census or an examination of the entire collection of ImmunAvance’s financial documents. Instead, you will collect a manageable group of items (called the sample) from the entire collection of financial documents (called the population.) A sample is a subset or a portion of a population. The information gained from the sample is used to make an inference (or generalization) about the population.\nAuditors typically cannot consider every item in a population, because there are too many. When it is not possible to conduct a census, auditors face sampling risk. Sampling risk is the risk affiliated with not auditing every item in the population. It is the risk that the sample may not adequately reflect the population. The only way to eliminate sampling risk is to conduct a census, which is usually not practical. Auditors can reduce sampling risk by obtaining a sample randomly. This is called random selection. Another way to reduce sampling risk is to increase the sample size, the number of items sampled.\n\n\n\n\nSampling Methods\nStep 2: Collect data\nThere are several procedures that can be used to select a random sample from a population, including: simple random sampling (SRS), stratified sampling, systematic sampling, cluster sampling, , and convenience sampling (or, haphazard sampling). These are examples of sampling methods.\n\nRandom Sampling Methods\nA simple random sample (SRS) is the best method for obtaining a sample from a population. This method allows each possible sample of a certain size an equal chance at being selected as the chosen sample. A difficulty of this method is that a list of all of the items in the population must be accessible before the sample is taken. Often, we obtain a SRS by allowing a computer to randomly select a certain number of items from the full list of the population. It is akin to the idea of putting all of the names into a hat, shaking them up, and randomly drawing out a few.\n\nFor example, suppose there are 18,000 students in the population of a certain university. School officials can use a computer to randomly choose values between 1 and 18,000 to identify which students are to be selected to complete a survey. In Excel, the command to obtain a random number between 1 and 18,000 is sample(1:18000, 1). A simple random sample can be obtained any time there is a complete list of the items to be sampled and they are all accessible. All the statistical procedures in this course assume that simple random sampling has been used. But in practice, the SRS is often difficult (or impossible) to implement.\n\nA stratified sample is when the items to be sampled are organized in groups of homogeneous (similar) items called strata, then a simple random sample is drawn from each of these strata. Stratified sampling works well when the items are similar within each stratum and tend to differ from one stratum to another. We often use stratified sampling in order to obtain a sample in such a way that we can make comparisons between each of the groups (or strata).\n\nFor example, in obtaining a sample of students from a university, school officials could define the strata as: (1) freshman, (2) sophomores, (3) juniors, and (4) seniors. A simple random sample could then be obtained from each of these strata. This would ensure that each class rank of students was represented in the sample. It would also allow the school officials to see how freshman, sophomore, junior, and senior level students compared in their answers to a survey.\n\nA systematic sample is where every \\(k^{\\text{th}}\\) item in the population is selected to be part of the sample, beginning at a random starting point. Systematic sampling works well when the items are in a random, but sequential ordering. If the items are not arranged randomly, a systematic sample can miss important parts of the population.\n\nFor example, consider a fast food company where every 10th customer is given the opportunity to compete a satisfaction survey in exchange for a small discount coupon towards their next purchase. An airport security line also often implements a procedure where every 100th (or so) person is selected for a more “in depth” security examination. Similarly, factories that use assembly lines will pull say every 500th item from the assembly line to perform a quality control check on the item.\n\nA cluster sample (sometimes called a block sample) consists of taking all items in one or more randomly selected clusters, or blocks. When the variation from one block to another is relatively low, compared to the variation within the block, cluster sampling is a reasonable way to get a sample.\n\nFor example, ecologists could draw grids on a map of a forest to create small sampling regions, or sampling clusters. Then, by randomly selecting one or two of these clusters from the map, the ecologists could go to the areas marked on the map and document information on the health of every tree they find in those clusters. This is a practical way to get a sample in this case because the ecologists only have to go to a few areas of the forest, but are still able to obtain a random sample of all of the trees in the forest. It is also worth noting that the ecologists would not be interesting in comparing the health of the trees from the selected clusters to each other like they would in a stratified sample. Instead, they are just looking for a feasible way to obtain a single random sample of all of the trees in the forest, but want to keep their traveling time to a minimum while collecting their sample. In contrast, to obtain a simple random sample of trees from the same forest, the ecologists would first have to go out and number every tree in the entire forest. Then they would need to use a computer to randomly pick which trees to collect data on. Finally, they would then have to go back to the forest and collect data on the selected trees from across the entire forest. Such an approach just isn’t feasible in practice, so we are willing to settle instead for the cluster sample.\n\nA convenience sample involves selecting items that are relatively easy to obtain and does not use random selection to choose the sample. This method of sampling can be assumed to always bring bias into the sample.\n\nAs an example of a convenience sample, an auditor could haphazardly select items from a filing cabinet. This is frequently done when a quick and simple sample is needed, but may not yield a sample that represents the population well. When possible, convenience samples should be avoided.\n\n\n\n\n\nTypes of Data\nWhenever we collect data, we record information about the things we are studying. There are two basic types of data that can be recorded: quantitative measurements and categorical labels. We will call these types of data simply “quantitative” or “categorical” variables. We use the word “variable” to denote the idea that the quantitative measurements or categorical labels can vary from person to person, or item to item, in our study.\nQuantitative variables provide measurement information on each individual (or item) in our study. They represent things that are numeric in nature; things that are measured. They often include units of measurement along with the quantitative value of the measurement. For example, the heights of children measured in inches (or centimeters), or their weight measured in pounds (or kilograms). For a quantitative variable, it makes sense to apply arithmetic operations to the data (such as adding values together, computing the average of the values, or comparing two values). If one child weighs 30 pounds (13.61 kg) and a second child weights 60 pounds (27.22) then the second child is twice as heavy as the first.\nCategorical variables allow us to place each individual (or item) into to a specific category. Categorical variables are labels, and it does not make sense to do arithmetic with them. For example the gender of a newborn child, the ethnicity of an individual, a person’s job title, the brand of phone they own, or the area code of a telephone number, etc are all categorical variables. Notice that although a telephone number consists of numbers, it is not a quantitative measurement. It does not make sense to double someone’s phone number, to average phone numbers together, or to say one phone number is half the size of another. But the area code of the phone number gives information about the region where the phone number was first initiated, which is categorical information.\nIn Unit 3 of this course we will learn more about categorical variables and proportions. Units 1 and 2 of this course focus on studying quantitative variables.\nReturning to the sample accounts receivable record, we find this data to have information on both types of variables.\n\nAnswer the following question:\n\n\n\nFor each of the following variables taken from this accounts receivable record, indicate whether the variable is quantitative or categorical.\n\n\nTerms\n\n\n\nSolution\n\n\nThe variable “Terms” is categorical. It classifies the invoice by the terms of payment for that invoice.\n\n\n\nAccount number\n\n\n\nSolution\n\n\nThe variable “Account number” is categorical. Even though the account number is given a number, it is actually functioning as a label. It is not something that is counted or measured. It does not make sense to do arithmetic operations (like adding 1 or multiplying by 2) to the account number.\n\n\n\nInvoice amount\n\n\n\nSolution\n\n\nThe variable “Invoice amount” is quantitative. It makes sense to do arithmetic operations to this value. For example, the amount of Invoice 5745 (which is $990.00) is somewhat more than twice as much as that of Invoice 2378 (which is $478.00).\n\n\n \n\n\nStep 3: Describe the data\nAfter auditors collect a sample and compile the data, they review the evidence. Auditors may use graphs or compute numbers (such as the average) to summarize the evidence they found."
  },
  {
    "objectID": "Textbook/Stat_Process.html#making-inferences-hypothesis-testing",
    "href": "Textbook/Stat_Process.html#making-inferences-hypothesis-testing",
    "title": "The Statistical Process",
    "section": "Making Inferences: Hypothesis Testing",
    "text": "Making Inferences: Hypothesis Testing\nStep 4: Make inferences\nAuditors use the information drawn from the sample to form an opinion about the population. Whenever sample data is used to infer a characteristic of a population, it is called making an inference. Inferential statistics represents a collection of methods that can be used to make inference about a population. Based on the documents reviewed, the auditors assess if the company is conducting its business in a proper manner.\nWhen conducting an audit, the implicit assumption is that transactions have been posted properly. As auditors sample the company’s records, they are looking to see if everything is consistent with the original assumption that all transactions have been posted properly. It would only be in the case of discovering suspicious activity or evidence of fraudulent reporting that the auditors would change their belief about the company and accuse the company ImmunAvance of falsely reporting on their financial statements.\n\n“Piled Higher and Deeper” by Jorge Cham  \n\nThere is a formal procedure for determining when enough evidence has been found to make accusations of fraud. Later this semester, after we establish some foundational principles of statistics, we will study these statistical methods in depth. Of course, these methods can be used for much more than just determining if a company has reported their financial statements fraudulently. So we will look at many different ways these statistical procedures can be applied to research and industry.\nFor ImmunAvance’s audit, based on the samples of financial statements that had been selected, while there were a few errors in the documents, there was not evidence dramatic enough to claim that the company had been fraudulent. So the company passed their audit.\n\nStep 5: Take Action\nThe auditors prepare a report in which they give their opinion on the status of the company’s current operations.\nSince there was not enough evidence to suggest that ImmunAvance’s financial statements were fraudulent, the auditor’s conclusion is that no adjustment is necessary. The few observed discrepancies were apparently just the result of random chance errors, not the deliberate falsefying of information."
  },
  {
    "objectID": "Textbook/Stat_Process.html#summary",
    "href": "Textbook/Stat_Process.html#summary",
    "title": "The Statistical Process",
    "section": "Summary",
    "text": "Summary\n\nRemember…\n\n\n\nThe Statistical Process has five steps: Design the study, Collect the data, Describe the data, Make inference, Take action. These can be remembered by the pneumonic “Daniel Can Discern More Truth.”\nIn a designed experiment, researchers control the conditions of the study, typically with a treatment group and a control group, and then observe how the treatments impact the subjects. In a purely observational study, researchers don’t control the conditions but only observe what happens.\nThe population is the entire group of all possible subjects that could be included in the study. The sample is the subset of the population that is actually selected to participate in the study. Statistics use information from the sample to make claims about what is true about the entire population.\nThere are many sampling methods used to obtain a sample from a population. The best methods use some sort of randomness (like pulling names out of a hat, rolling dice, flipping coins, or using a computer generated list of random numbers) to avoid bias.\n\n\nA simple random sample (SRS) is a random sample taken from the full list of the population. This is the least biased (best) sampling method, but can only be implemented when a full list of the population is accessible.\nA stratified sample divides the population into similar groups and then takes an SRS from each group. The main reason to use this sampling method is when a study wants to compare and contrast certain groups within the population, say to compare freshman, sophomores, juniors, and seniors at a university.\nA systematic sample samples every kth item in the population, beginning at a random starting point. This is best applied when subjects are lined up in some way, like at a fast food restaurant, an airport security line, or an assembly line in a factory.\nA cluster sample consists of taking all items in one or more randomly selected clusters, or blocks. For example, ecologists could draw grids on a map of a forest to create small sampling regions and then sample all trees they find in a few randomly selected regions. Note that this differs from a stratified sample in that only a few sub-groups (clusters) are selected and that all subjects within the selected clusters are included in the study.\nA convenience sample involves selecting items that are relatively easy to obtain and does not use random selection to choose the sample. This method of sampling can be assumed to always bring bias into the sample.\n\n\nThe best way to avoid bias when trying to make conclusions about a population from a single sample of that population is to use a random sampling method to obtain the sample.\nQuantitative variables represent things that are numeric in nature, such as the value of a car or the number of students in a classroom. Categorical variables represent non-numerical data that can only be considered as labels, such as colors or brands of shoes."
  },
  {
    "objectID": "Textbook/Stat_Process.html#references",
    "href": "Textbook/Stat_Process.html#references",
    "title": "The Statistical Process",
    "section": "References",
    "text": "References\nBible Dictionary, “Pulse” at http://churchofjesuschrist.org/scriptures/bd/pulse.\nBrownlee, K. A. (1955). Statistics of the 1954 polio vaccine trials. Journal of the American Statistical Association, 50(272), pp. 1005-1013.\nFrancis, T., et. al. (1955). An evaluation of the 1954 poliomyletis vaccine trials. American Journal of Public Health and the Nation’s Health, 45(5)\nOffit, P. A. (2005). Why are pharmaceutical companies gradually abandoning vaccines? Health Affairs, 24(3), 622-630. doi:10.1377/hlthaff.24.3.622"
  }
]