[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "BYU-Idaho Math 221: Introduction to Statistics in R",
    "section": "",
    "text": "This course is intended to familiarize students with foundational concepts and vocabulary in statistics and introduce basic data wrangling and visualization in R.\nThose who diligently work through these materials will be well prepared for their next Statistics or Data Science course.\nThis book is designed to be a digital workbook. You can download the notes, practice problems and application activities which can be edited locally on your computer. Completed assignments will be submitted in Canvas as web-page reports.\n\n\nTo download the folder containing all of the exercises, notes and application activities by following these steps:\n\nCreate a class folder where you will save all your work throughout this semester. This can be on your computer locally or in cloud-based storage such as One-Drive or iCloud.\nClick on the link below to download the .zip file containing all contents of the course website:\n\n\n Download Course Files \n\n\nOpen the .zip file\nSave the Student_Work folder to your class folder created in step 1. This can typically be done by right clicking the folder then copying and pasting it into the new location.\n\nThese files will be the basis of your coursework.\n\n\n\nEach unit contains:\n\nPreparation readings\nPractice problems\nApplication Activities\nAssessments\n\nGetting Started guides you through getting R and R-Studio set up, introduces the tools we will use for the class, and introduces basic statistical concepts to help get you started. We learn how to use R-Studio to make lovely reports that can be viewed in a web-browser. It’s a gentle enough introduction to enable even those who have never coded before to start working with R.\nDescriptive Statistics introduces tools to help understand data. We review summary statistics and visualizations for quantitative and categorical data types and learn the basic skills to turn data into information.\nData Wrangling and Visualization takes a step further into dealing with messy data and creating better visualizations.\nFoundations of Statistical Inference introduces the key concepts for using sample data to make generalized conclusions about a population. We cover probability, hypothesis testing, confidence intervals and other concepts necessary for making the leap from sample to population.\nStatistical Tests - Part 1 and 2 cover specific inferential statistics for different data types.\nThe Semester Project is an opportunity for students to demonstrate the skills acquired during the course. You will be able to find and import data into R, clean the data, create top notch visualizations, perform appropriate statistical analysis and create a beautiful report.\nR Help provides additional resources for learning how to use R.\n\n\nBy the end of this semester, students will confidently be able to:\n\nImport data and apply basic data wrangling to moderately messy data\nDescribe data with numerical and graphical summaries\nMake evidence-based decisions regarding situations with inherent randomness\nInvestigate questions through the application of probability distributions\nPerform the appropriate statistical analysis based on specific data being analyzed including hypothesis tests and confidence intervals\nCreate web-based reports to communicate the results of statistical analyses to relevant audiences"
  },
  {
    "objectID": "index.html#an-interactive-textbook",
    "href": "index.html#an-interactive-textbook",
    "title": "BYU-Idaho Math 221: Introduction to Statistics in R",
    "section": "",
    "text": "This course is intended to familiarize students with foundational concepts and vocabulary in statistics and introduce basic data wrangling and visualization in R.\nThose who diligently work through these materials will be well prepared for their next Statistics or Data Science course.\nThis book is designed to be a digital workbook. You can download the notes, practice problems and application activities which can be edited locally on your computer. Completed assignments will be submitted in Canvas as web-page reports.\n\n\nTo download the folder containing all of the exercises, notes and application activities by following these steps:\n\nCreate a class folder where you will save all your work throughout this semester. This can be on your computer locally or in cloud-based storage such as One-Drive or iCloud.\nClick on the link below to download the .zip file containing all contents of the course website:\n\n\n Download Course Files \n\n\nOpen the .zip file\nSave the Student_Work folder to your class folder created in step 1. This can typically be done by right clicking the folder then copying and pasting it into the new location.\n\nThese files will be the basis of your coursework.\n\n\n\nEach unit contains:\n\nPreparation readings\nPractice problems\nApplication Activities\nAssessments\n\nGetting Started guides you through getting R and R-Studio set up, introduces the tools we will use for the class, and introduces basic statistical concepts to help get you started. We learn how to use R-Studio to make lovely reports that can be viewed in a web-browser. It’s a gentle enough introduction to enable even those who have never coded before to start working with R.\nDescriptive Statistics introduces tools to help understand data. We review summary statistics and visualizations for quantitative and categorical data types and learn the basic skills to turn data into information.\nData Wrangling and Visualization takes a step further into dealing with messy data and creating better visualizations.\nFoundations of Statistical Inference introduces the key concepts for using sample data to make generalized conclusions about a population. We cover probability, hypothesis testing, confidence intervals and other concepts necessary for making the leap from sample to population.\nStatistical Tests - Part 1 and 2 cover specific inferential statistics for different data types.\nThe Semester Project is an opportunity for students to demonstrate the skills acquired during the course. You will be able to find and import data into R, clean the data, create top notch visualizations, perform appropriate statistical analysis and create a beautiful report.\nR Help provides additional resources for learning how to use R.\n\n\nBy the end of this semester, students will confidently be able to:\n\nImport data and apply basic data wrangling to moderately messy data\nDescribe data with numerical and graphical summaries\nMake evidence-based decisions regarding situations with inherent randomness\nInvestigate questions through the application of probability distributions\nPerform the appropriate statistical analysis based on specific data being analyzed including hypothesis tests and confidence intervals\nCreate web-based reports to communicate the results of statistical analyses to relevant audiences"
  },
  {
    "objectID": "7-Semester_Project/Semester_Project_Instructions.html",
    "href": "7-Semester_Project/Semester_Project_Instructions.html",
    "title": "Semester Project Instructions",
    "section": "",
    "text": "For this project, you will create an html report that is an original analysis based on data that you find.\nStart with a research question that interests you. It can be about any topic (finance, music, video games, hunting, weather, mental health, AI…seriously anything!)\nFor this project, a good research question will be interesting to you AND feasible to find available data. You can certainly think of exciting research questions for which the data are impossible to find or collect. Expect to refine your research question as you begin the data search.\nOnce you have a research question in mind, start looking for data relating to it. Below are some links that might be helpful for finding datasets.\n\n\nGoogle actually has a search engine specifically for datasets\nStatista has a datasets for a wide range of topics but is particularly well suited for government policy-related data such as health, crime, social science.\nKaggle runs competitions for companies who outsource data challenges. It has also compiled a large library of datasets on a range of topics. Because businesses run competitions through here, there are a lot of datasets related to specific challenges that businesses face."
  },
  {
    "objectID": "7-Semester_Project/Semester_Project_Instructions.html#helpful-links",
    "href": "7-Semester_Project/Semester_Project_Instructions.html#helpful-links",
    "title": "Semester Project Instructions",
    "section": "",
    "text": "Google actually has a search engine specifically for datasets\nStatista has a datasets for a wide range of topics but is particularly well suited for government policy-related data such as health, crime, social science.\nKaggle runs competitions for companies who outsource data challenges. It has also compiled a large library of datasets on a range of topics. Because businesses run competitions through here, there are a lot of datasets related to specific challenges that businesses face."
  },
  {
    "objectID": "7-Semester_Project/Semester_Project_Instructions.html#define-the-problem",
    "href": "7-Semester_Project/Semester_Project_Instructions.html#define-the-problem",
    "title": "Semester Project Instructions",
    "section": "1. Define the Problem",
    "text": "1. Define the Problem\nInclude an introduction that describes why you were interested in the topic and what you envisioned for an analysis. Some questions to address:\n\nWhat is the population of your research?\nWhat do you think is the nature of the relationship you hope to discover?\nWhat type of data are you looking for (quantitative? categorical?)\n\nWhat type of analysis are you expecting to do (t-test, regression? ANOVA? Chi-square test for independence? etc.)"
  },
  {
    "objectID": "7-Semester_Project/Semester_Project_Instructions.html#collect-the-data",
    "href": "7-Semester_Project/Semester_Project_Instructions.html#collect-the-data",
    "title": "Semester Project Instructions",
    "section": "2. Collect the Data",
    "text": "2. Collect the Data\nTalk about the process of how you found the data and whether you had to make adjustments to your research question."
  },
  {
    "objectID": "7-Semester_Project/Semester_Project_Instructions.html#describe-the-data",
    "href": "7-Semester_Project/Semester_Project_Instructions.html#describe-the-data",
    "title": "Semester Project Instructions",
    "section": "3. Describe the Data",
    "text": "3. Describe the Data\nInclude summary statistics (favstats, proportions, contingency tables, etc.)\nInclude GGPlot visualizations (boxplots, histograms/density plots, bar charts, etc.). Make the charts as understandable as possible without having to read the text description. That means make sure:\n\nAxes are labeled appropriately\nIt has a descriptive title\nLabel lines or points that are highlighted in the graph"
  },
  {
    "objectID": "7-Semester_Project/Semester_Project_Instructions.html#analyze-the-data",
    "href": "7-Semester_Project/Semester_Project_Instructions.html#analyze-the-data",
    "title": "Semester Project Instructions",
    "section": "4. Analyze the Data",
    "text": "4. Analyze the Data\nPerform the appropriate analysis for the data collected (t-test, F-test, regression, Chi-square, etc.)"
  },
  {
    "objectID": "7-Semester_Project/Semester_Project_Instructions.html#take-action",
    "href": "7-Semester_Project/Semester_Project_Instructions.html#take-action",
    "title": "Semester Project Instructions",
    "section": "5. Take Action",
    "text": "5. Take Action\nBased on your statistical inference, what recommendations or actions would you take?"
  },
  {
    "objectID": "6-Statistical_Tests_Part2/Summarizing_Categorical_Data.html",
    "href": "6-Statistical_Tests_Part2/Summarizing_Categorical_Data.html",
    "title": "Summarizing Categorical Data",
    "section": "",
    "text": "In this section we will show how to summarize data numerically and visually. We will be using survey responses about Star Wars. The survey was carried out by FiveThirtyEight about the first 6 Star Wars films. The survey contains demographic information as well as movie rankings and character favorability rankings.\n\n\n\nlibrary(tidyverse)\nlibrary(mosaic)\nlibrary(rio)\nlibrary(ggplot2)\n\nsw &lt;- read_csv('https://raw.githubusercontent.com/byuistats/Math221D_Cannon/master/Data/StarWarsData_clean.csv')\n\n\n\n\n\n\nWe typically summarize categorical variables with counts or proportions. When we want to summarize a single categorical variable by itself, we can use the table() function to get counts. For example, if we want to tabulate the favorability of Han Solo:\n\ntable(sw$`Favorability_Han Solo`)\n\n\nNeither favorably nor unfavorably (neutral) \n                                         44 \n                         Somewhat favorably \n                                        151 \n                       Somewhat unfavorably \n                                          8 \n                           Unfamiliar (N/A) \n                                         15 \n                             Very favorably \n                                        610 \n                           Very unfavorably \n                                          1 \n\n\nThis shows us the counts of respondents in each response category.\nWe can also use the prop.table() function to get the proportion of respondents, rather than the counts, by inputing a table.\n\nprop.table(table(sw$`Favorability_Han Solo`)) \n\n\nNeither favorably nor unfavorably (neutral) \n                                0.053075995 \n                         Somewhat favorably \n                                0.182147165 \n                       Somewhat unfavorably \n                                0.009650181 \n                           Unfamiliar (N/A) \n                                0.018094089 \n                             Very favorably \n                                0.735826297 \n                           Very unfavorably \n                                0.001206273 \n\n\nNOTE: The prop.table() function needs a table as an input, not a data column.\nQuestion: What percent of respondents are “Very favorable” towards Han Solo?\nAnswer:\nQuestion: What percent of respondents are “Very unfavorable” towards Han Solo?\nAnswer:\n\n\n\nWe can use Contingency Tables to examine associations between 2 categorical variables. A contingency table displays the counts of combinations of 2 categorical variables each being represented as a row or a column. It is easy to create a contingency table in R by inputting 2 data columns into the table() function. The resulting table will have rows and columns which correspond to the order of input table(row, column).\nLet’s contrast gender with whether or not a respondent is a fan of Star Wars (Are You a Fan of SW):\n\ntable(sw$Gender, sw$`Are You a Fan of SW?`)\n\n        \n          No Yes\n  Female 158 238\n  Male   119 303\n\n\nWe can include row and column totals by wrapping our table in the addmargins() function as follows:\n\naddmargins(table(sw$Gender, sw$`Are You a Fan of SW?`))\n\n        \n          No Yes Sum\n  Female 158 238 396\n  Male   119 303 422\n  Sum    277 541 818\n\n\nThis can be used to get row or column percentages. Alternatively we can use the prop.table() function to get proportions.\n\nprop.table(table(sw$Gender, sw$`Are You a Fan of SW?`))\n\n        \n                No       Yes\n  Female 0.1931540 0.2909535\n  Male   0.1454768 0.3704156\n\n\nThe default for prop.table() is to give the overall percentages (counts / table total). So the proportions add to 1 across the whole table.\nWe can specify row or column percentages by specifying a “margin.” In R, margin=1 corresponds to rows and margin = 2 corresponds to columns.\nCompare the difference:\n\nprop.table(table(sw$Gender, sw$`Are You a Fan of SW?`), margin = 1)\n\n        \n                No       Yes\n  Female 0.3989899 0.6010101\n  Male   0.2819905 0.7180095\n\n\nThis table sums to 1 across the rows, meaning that about 60% of Females are fans of Star Wars and about 72% of Males are fans.\nNow look at margin = 2\n\nprop.table(table(sw$Gender, sw$`Are You a Fan of SW?`), margin = 2)\n\n        \n                No       Yes\n  Female 0.5703971 0.4399261\n  Male   0.4296029 0.5600739\n\n\nQuestion: What does this table show?\nAnswer:\nNOTE: Which margin we choose to evaluate depends on the order we input columns into the table() function. Be sure to double check that you calculate the correct percentages.\n\n\n\n\nVisually, bar charts are the optimal way to express categorical data. Pie charts, while very common, are problematic because of weaknesses in basic human perception.\nWe can use ggplot() with categorical variables to get summaries of counts using the geom_bar() geometry.\n\nggplot(sw, aes(x = `Are You a Fan of SW?`)) + \n  geom_bar()\n\n\n\n\n\n\n\n\nWe can add another variable to the mix to look at things by gender using the fill= argument inside the aesthetics:\n\nggplot(sw, aes(x = who_shot_first, fill = Gender)) + \n  geom_bar()\n\n\n\n\n\n\n\n\nThe default for geom_bar() is to stack bars. If we want side-by-side bars we can add a “position = ‘dodge’” to the geom_bar() function:\n\nggplot(sw, aes(x = who_shot_first, fill = Gender)) + \n  geom_bar(position = \"dodge\")\n\n\n\n\n\n\n\n\n\n\nThe graphs above include missing values as its own category. The easiest way to deal with missing values is to create a subset of the data that is prepared for the graph we are interested in creating.\nYou can filter out the missing values using filter() or use the combination of select() with drop_na() in the following way:\n\nshot_first_data &lt;- sw %&gt;%\n  filter(who_shot_first != \"\",\n         Gender != \"\")\n\n# Or using drop_na()\n\nshot_first_data &lt;- sw %&gt;%\n  select(Gender, who_shot_first) %&gt;%\n  drop_na()\n\nNOTE: The drop_na() function drops all rows with ANY missing values. If we use this function on the dataset with all the columns, we may end up losing information on the analysis of interest. This is why we do a select() first. that way we only delete rows missing relevant information.\nNow look at the graph without missing values.\n\nggplot(shot_first_data, aes(x = who_shot_first, fill = Gender)) + \n  geom_bar(position = \"dodge\")\n\n\n\n\n\n\n\n\n\n\n\nThe default visualization elements in ggplot() can always be improved. Here are some options for making the chart more readable:\n\nggplot(shot_first_data, aes(x = who_shot_first, fill = Gender)) + \n  geom_bar(position = \"dodge\") +\n  theme_bw() +\n  labs(\n    x = \"Which Character Shot First?\",\n    y = \"Count\",\n    title = \"Comparing response to the Question 'Who Shot First' by Gender\" \n  )\n\n\n\n\n\n\n\n\n\n\n\nWith categorical variables, we can group differently depending on which comparisons we would like to emphasize. Above, we grouped by responses to “who shot first” and colored by gender. If we swap the x variable and the color, we get the same bars, but arranged differently.\n\nggplot(shot_first_data, aes(x = Gender, fill = who_shot_first)) + \n  geom_bar(position = \"dodge\") +\n  theme_bw() +\n  labs(\n    x = \"\",\n    y = \"Count\",\n    title = \"Comparing response to the Question 'Who Shot First' by Gender\" \n  )\n\n\n\n\n\n\n\n\nThis different point of view makes it easier to see the breakdown of responses for each gender separately. We can see more clearly that the frequency of Females who do not understand the question is much more pronounced than on the Male side. Males, it seems largely agree that Han shot first.\nAlternatively,\n\nggplot(shot_first_data, aes(x = who_shot_first, fill = Gender)) + \n  geom_bar(position = \"dodge\") +\n  theme_bw() +\n  labs(\n    x = \"\",\n    y = \"Count\",\n    title = \"Comparing response to the Question 'Who Shot First' by Gender\" \n  )\n\n\n\n\n\n\n\n\nAlternative to a side-by-side bar chart, we can “facet” the graph which splits up the panels on one of the variables. This can be useful when we want to emphasize specific comparisons.\n\n\n\nggplot(shot_first_data, aes(x = who_shot_first, fill = Gender)) + \n  geom_bar() +\n  facet_wrap(~Gender)\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(shot_first_data, aes(x = Gender, fill = Gender)) + \n  geom_bar(position = \"dodge\") +\n  facet_wrap(~who_shot_first)\n\n\n\n\n\n\n\n\nHow you split the graph depends on which comparison is the most important. The first graph emphasizes the differences in how the question was answered. The second makes it easy to compare female and male responses."
  },
  {
    "objectID": "6-Statistical_Tests_Part2/Summarizing_Categorical_Data.html#load-the-data-and-libraries",
    "href": "6-Statistical_Tests_Part2/Summarizing_Categorical_Data.html#load-the-data-and-libraries",
    "title": "Summarizing Categorical Data",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(rio)\nlibrary(ggplot2)\n\nsw &lt;- read_csv('https://raw.githubusercontent.com/byuistats/Math221D_Cannon/master/Data/StarWarsData_clean.csv')"
  },
  {
    "objectID": "6-Statistical_Tests_Part2/Summarizing_Categorical_Data.html#numerical-summaries",
    "href": "6-Statistical_Tests_Part2/Summarizing_Categorical_Data.html#numerical-summaries",
    "title": "Summarizing Categorical Data",
    "section": "",
    "text": "We typically summarize categorical variables with counts or proportions. When we want to summarize a single categorical variable by itself, we can use the table() function to get counts. For example, if we want to tabulate the favorability of Han Solo:\n\ntable(sw$`Favorability_Han Solo`)\n\n\nNeither favorably nor unfavorably (neutral) \n                                         44 \n                         Somewhat favorably \n                                        151 \n                       Somewhat unfavorably \n                                          8 \n                           Unfamiliar (N/A) \n                                         15 \n                             Very favorably \n                                        610 \n                           Very unfavorably \n                                          1 \n\n\nThis shows us the counts of respondents in each response category.\nWe can also use the prop.table() function to get the proportion of respondents, rather than the counts, by inputing a table.\n\nprop.table(table(sw$`Favorability_Han Solo`)) \n\n\nNeither favorably nor unfavorably (neutral) \n                                0.053075995 \n                         Somewhat favorably \n                                0.182147165 \n                       Somewhat unfavorably \n                                0.009650181 \n                           Unfamiliar (N/A) \n                                0.018094089 \n                             Very favorably \n                                0.735826297 \n                           Very unfavorably \n                                0.001206273 \n\n\nNOTE: The prop.table() function needs a table as an input, not a data column.\nQuestion: What percent of respondents are “Very favorable” towards Han Solo?\nAnswer:\nQuestion: What percent of respondents are “Very unfavorable” towards Han Solo?\nAnswer:\n\n\n\nWe can use Contingency Tables to examine associations between 2 categorical variables. A contingency table displays the counts of combinations of 2 categorical variables each being represented as a row or a column. It is easy to create a contingency table in R by inputting 2 data columns into the table() function. The resulting table will have rows and columns which correspond to the order of input table(row, column).\nLet’s contrast gender with whether or not a respondent is a fan of Star Wars (Are You a Fan of SW):\n\ntable(sw$Gender, sw$`Are You a Fan of SW?`)\n\n        \n          No Yes\n  Female 158 238\n  Male   119 303\n\n\nWe can include row and column totals by wrapping our table in the addmargins() function as follows:\n\naddmargins(table(sw$Gender, sw$`Are You a Fan of SW?`))\n\n        \n          No Yes Sum\n  Female 158 238 396\n  Male   119 303 422\n  Sum    277 541 818\n\n\nThis can be used to get row or column percentages. Alternatively we can use the prop.table() function to get proportions.\n\nprop.table(table(sw$Gender, sw$`Are You a Fan of SW?`))\n\n        \n                No       Yes\n  Female 0.1931540 0.2909535\n  Male   0.1454768 0.3704156\n\n\nThe default for prop.table() is to give the overall percentages (counts / table total). So the proportions add to 1 across the whole table.\nWe can specify row or column percentages by specifying a “margin.” In R, margin=1 corresponds to rows and margin = 2 corresponds to columns.\nCompare the difference:\n\nprop.table(table(sw$Gender, sw$`Are You a Fan of SW?`), margin = 1)\n\n        \n                No       Yes\n  Female 0.3989899 0.6010101\n  Male   0.2819905 0.7180095\n\n\nThis table sums to 1 across the rows, meaning that about 60% of Females are fans of Star Wars and about 72% of Males are fans.\nNow look at margin = 2\n\nprop.table(table(sw$Gender, sw$`Are You a Fan of SW?`), margin = 2)\n\n        \n                No       Yes\n  Female 0.5703971 0.4399261\n  Male   0.4296029 0.5600739\n\n\nQuestion: What does this table show?\nAnswer:\nNOTE: Which margin we choose to evaluate depends on the order we input columns into the table() function. Be sure to double check that you calculate the correct percentages."
  },
  {
    "objectID": "6-Statistical_Tests_Part2/Summarizing_Categorical_Data.html#visual-summaries",
    "href": "6-Statistical_Tests_Part2/Summarizing_Categorical_Data.html#visual-summaries",
    "title": "Summarizing Categorical Data",
    "section": "",
    "text": "Visually, bar charts are the optimal way to express categorical data. Pie charts, while very common, are problematic because of weaknesses in basic human perception.\nWe can use ggplot() with categorical variables to get summaries of counts using the geom_bar() geometry.\n\nggplot(sw, aes(x = `Are You a Fan of SW?`)) + \n  geom_bar()\n\n\n\n\n\n\n\n\nWe can add another variable to the mix to look at things by gender using the fill= argument inside the aesthetics:\n\nggplot(sw, aes(x = who_shot_first, fill = Gender)) + \n  geom_bar()\n\n\n\n\n\n\n\n\nThe default for geom_bar() is to stack bars. If we want side-by-side bars we can add a “position = ‘dodge’” to the geom_bar() function:\n\nggplot(sw, aes(x = who_shot_first, fill = Gender)) + \n  geom_bar(position = \"dodge\")\n\n\n\n\n\n\n\n\n\n\nThe graphs above include missing values as its own category. The easiest way to deal with missing values is to create a subset of the data that is prepared for the graph we are interested in creating.\nYou can filter out the missing values using filter() or use the combination of select() with drop_na() in the following way:\n\nshot_first_data &lt;- sw %&gt;%\n  filter(who_shot_first != \"\",\n         Gender != \"\")\n\n# Or using drop_na()\n\nshot_first_data &lt;- sw %&gt;%\n  select(Gender, who_shot_first) %&gt;%\n  drop_na()\n\nNOTE: The drop_na() function drops all rows with ANY missing values. If we use this function on the dataset with all the columns, we may end up losing information on the analysis of interest. This is why we do a select() first. that way we only delete rows missing relevant information.\nNow look at the graph without missing values.\n\nggplot(shot_first_data, aes(x = who_shot_first, fill = Gender)) + \n  geom_bar(position = \"dodge\")\n\n\n\n\n\n\n\n\n\n\n\nThe default visualization elements in ggplot() can always be improved. Here are some options for making the chart more readable:\n\nggplot(shot_first_data, aes(x = who_shot_first, fill = Gender)) + \n  geom_bar(position = \"dodge\") +\n  theme_bw() +\n  labs(\n    x = \"Which Character Shot First?\",\n    y = \"Count\",\n    title = \"Comparing response to the Question 'Who Shot First' by Gender\" \n  )\n\n\n\n\n\n\n\n\n\n\n\nWith categorical variables, we can group differently depending on which comparisons we would like to emphasize. Above, we grouped by responses to “who shot first” and colored by gender. If we swap the x variable and the color, we get the same bars, but arranged differently.\n\nggplot(shot_first_data, aes(x = Gender, fill = who_shot_first)) + \n  geom_bar(position = \"dodge\") +\n  theme_bw() +\n  labs(\n    x = \"\",\n    y = \"Count\",\n    title = \"Comparing response to the Question 'Who Shot First' by Gender\" \n  )\n\n\n\n\n\n\n\n\nThis different point of view makes it easier to see the breakdown of responses for each gender separately. We can see more clearly that the frequency of Females who do not understand the question is much more pronounced than on the Male side. Males, it seems largely agree that Han shot first.\nAlternatively,\n\nggplot(shot_first_data, aes(x = who_shot_first, fill = Gender)) + \n  geom_bar(position = \"dodge\") +\n  theme_bw() +\n  labs(\n    x = \"\",\n    y = \"Count\",\n    title = \"Comparing response to the Question 'Who Shot First' by Gender\" \n  )\n\n\n\n\n\n\n\n\nAlternative to a side-by-side bar chart, we can “facet” the graph which splits up the panels on one of the variables. This can be useful when we want to emphasize specific comparisons.\n\n\n\nggplot(shot_first_data, aes(x = who_shot_first, fill = Gender)) + \n  geom_bar() +\n  facet_wrap(~Gender)\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(shot_first_data, aes(x = Gender, fill = Gender)) + \n  geom_bar(position = \"dodge\") +\n  facet_wrap(~who_shot_first)\n\n\n\n\n\n\n\n\nHow you split the graph depends on which comparison is the most important. The first graph emphasizes the differences in how the question was answered. The second makes it easy to compare female and male responses."
  },
  {
    "objectID": "6-Statistical_Tests_Part2/Summarizing_Categorical_Data.html#visualization",
    "href": "6-Statistical_Tests_Part2/Summarizing_Categorical_Data.html#visualization",
    "title": "Summarizing Categorical Data",
    "section": "Visualization",
    "text": "Visualization\nCreate a bar chart for favorability of Han Solo by whether or not they are fans of Star Trek (fan_of_star_trek).\nStart by making a new dataset called trekky that only includes the 2 relevant columns and drops the missing values.\n\ntrekky &lt;- sw %&gt;%\n\nError in parse(text = input): &lt;text&gt;:4:0: unexpected end of input\n2: trekky &lt;- sw %&gt;%\n3: \n  ^\n\n\nQuestion: What observations can you make based on the visualization?\nAnswer:"
  },
  {
    "objectID": "6-Statistical_Tests_Part2/Summarizing_Categorical_Data.html#proportion-table",
    "href": "6-Statistical_Tests_Part2/Summarizing_Categorical_Data.html#proportion-table",
    "title": "Summarizing Categorical Data",
    "section": "Proportion Table",
    "text": "Proportion Table\nWe would like to compare what percent of female respondents do not understand the question compared to the percent of males who do not understand the question.\nCreate a proportion table that can answer this question:\nQuestion: What percent of female respondents do not understand the question?\nAnswer:\nQuestion: What percent of male respondents do not understand the question?\nAnswer:"
  },
  {
    "objectID": "6-Statistical_Tests_Part2/One_and_Two_Sample_Proportion_Practice.html",
    "href": "6-Statistical_Tests_Part2/One_and_Two_Sample_Proportion_Practice.html",
    "title": "One and Two Sample Proportion Practice",
    "section": "",
    "text": "A study was conducted to determine the proportion of American teenagers between 13 and 17 who smoke. A survey from 10 years ago claimed that 15% percent of all teenagers smoke.\nA new Gallup survey interviewed a nationally representative sample of 785 teenagers aged 13 to 17. Seventy-one (71) teenagers in the survey acknowledged having smoked at least once in the past week.\nWe want to see if the new study shows a decrease in the percentage of teenagers who smoke from the 15% reported a decade ago.\nPerform the appropriate hypothesis test and create a confidence interval for the true proportion of teenagers who smoke.\n\n\nState the null and alternative hypotheses:\n\\[H_0: \\]\n\\[H_a: \\] Choose your confidence level:\n\\[\\alpha = 0.\\]\nPerform the appropriate analysis:\n\nprop.test(x = , n = , alternative = \"\")\n\nError in prop.test(x = , n = , alternative = \"\"): argument \"x\" is missing, with no default\n\n\nQuestion: What is the P-value?\nAnswer:\nQuestion: Based on your chosen \\(\\alpha\\) and P-value, what is your conclusion?\nAnswer:\n\n\nRecall that we must check that we have a big enough sample size to trust our p-value. To do this, we check that there are at least 10 expected “success” and “failures” for a given sample size, n:\nCheck:\n\\[np \\ge 10\\]\n\\[n(1-p) \\ge 10\\]\n\n# Fill in n and the hypothesized p\nn &lt;- \np &lt;- \n\nn*p &gt;= 10\n\nError: object 'n' not found\n\nn*(1-p) &gt;= 10\n\nError: object 'n' not found\n\n\nQUESTION: Are the requirements for a hypothesis test satisfied?\nANSWER:\n\n\n\n\nCreate a 99% confidence interval for the true population proportion of teenagers who smoke?\nQUESTION: Interpret the confidence interval in context of the research question:\nANSWER:\n\n\nRecall that Confidence Intervals do not depend on a hypothesized proportion, so the requirements are a little different. For Confidence Intervals we check:\n\\[n\\hat{p} \\ge 10\\]\n\\[n(1-\\hat{p}) \\ge 10\\]\n\n# Fill in X and N and check that there are enough \"successes\" and \"failures\"\n\nx &lt;- \nn &lt;- \nphat &lt;- x/n\n\nError: object 'x' not found\n\nn*phat &gt;= 10\n\nError: object 'n' not found\n\nn*(1-phat) &gt;= 10\n\nError: object 'n' not found"
  },
  {
    "objectID": "6-Statistical_Tests_Part2/One_and_Two_Sample_Proportion_Practice.html#teenage-smoking-habits",
    "href": "6-Statistical_Tests_Part2/One_and_Two_Sample_Proportion_Practice.html#teenage-smoking-habits",
    "title": "One and Two Sample Proportion Practice",
    "section": "",
    "text": "A study was conducted to determine the proportion of American teenagers between 13 and 17 who smoke. A survey from 10 years ago claimed that 15% percent of all teenagers smoke.\nA new Gallup survey interviewed a nationally representative sample of 785 teenagers aged 13 to 17. Seventy-one (71) teenagers in the survey acknowledged having smoked at least once in the past week.\nWe want to see if the new study shows a decrease in the percentage of teenagers who smoke from the 15% reported a decade ago.\nPerform the appropriate hypothesis test and create a confidence interval for the true proportion of teenagers who smoke.\n\n\nState the null and alternative hypotheses:\n\\[H_0: \\]\n\\[H_a: \\] Choose your confidence level:\n\\[\\alpha = 0.\\]\nPerform the appropriate analysis:\n\nprop.test(x = , n = , alternative = \"\")\n\nError in prop.test(x = , n = , alternative = \"\"): argument \"x\" is missing, with no default\n\n\nQuestion: What is the P-value?\nAnswer:\nQuestion: Based on your chosen \\(\\alpha\\) and P-value, what is your conclusion?\nAnswer:\n\n\nRecall that we must check that we have a big enough sample size to trust our p-value. To do this, we check that there are at least 10 expected “success” and “failures” for a given sample size, n:\nCheck:\n\\[np \\ge 10\\]\n\\[n(1-p) \\ge 10\\]\n\n# Fill in n and the hypothesized p\nn &lt;- \np &lt;- \n\nn*p &gt;= 10\n\nError: object 'n' not found\n\nn*(1-p) &gt;= 10\n\nError: object 'n' not found\n\n\nQUESTION: Are the requirements for a hypothesis test satisfied?\nANSWER:\n\n\n\n\nCreate a 99% confidence interval for the true population proportion of teenagers who smoke?\nQUESTION: Interpret the confidence interval in context of the research question:\nANSWER:\n\n\nRecall that Confidence Intervals do not depend on a hypothesized proportion, so the requirements are a little different. For Confidence Intervals we check:\n\\[n\\hat{p} \\ge 10\\]\n\\[n(1-\\hat{p}) \\ge 10\\]\n\n# Fill in X and N and check that there are enough \"successes\" and \"failures\"\n\nx &lt;- \nn &lt;- \nphat &lt;- x/n\n\nError: object 'x' not found\n\nn*phat &gt;= 10\n\nError: object 'n' not found\n\nn*(1-phat) &gt;= 10\n\nError: object 'n' not found"
  },
  {
    "objectID": "6-Statistical_Tests_Part2/One_and_Two_Sample_Proportion_Practice.html#a-penny-saved",
    "href": "6-Statistical_Tests_Part2/One_and_Two_Sample_Proportion_Practice.html#a-penny-saved",
    "title": "One and Two Sample Proportion Practice",
    "section": "A Penny Saved?",
    "text": "A Penny Saved?\nA random sample of BYU-Idaho students was surveyed and asked if they were in favor of retaining the penny as a form of currency in the United States. Out of the 116 women surveyed, 80 said that they were in favor of retaining the penny as a form of currency. Of the 137 men surveyed, 91 said that they were in favor of retaining the penny.\nFor these questions, let group 1 represent women and group 2 represent men.\n\nHypothesis Test\nTest to see if there is a difference between the proportion of women who want to keep the penny and the proportion of men who want to keep the penny. Use a level of significance of \\(\\alpha = 0.05\\).\nState your null and alternative hypotheses (replace the question marks with the appropriate symbols):\n\\[H_0: p_{female} ???  p_{male}\\]\n\\[H_a: p_{female} ??? p_{male}\\]\nPerform the appropriate test:\n\nprop.test(x = c(), n = c(), alternative = \"\")\n\nError in complete.cases(x, n): no input has determined the number of cases\n\n\nQuestion: What is the P-value?\nAnswer:\nQuestion: Based on \\(\\alpha = 0.05\\), state your conclusion in context of the research question:\nAnswer:\n\n\nConfidence Interval\nCreate a 95% confidence interval for the difference in the proportion of females to males who prefer to keep the penny:\nQuestion: Interpret the confidence interval in context of the research question:\nAnswer:\n\n\nTest Requirements\nRecall that the requirements for Hypothesis Testing and Confidence Intervals for 2-sample proportions are the same. We must check that there are more than 10 “successes” and “failures” in both samples:\n\\[ n_1\\hat{p}_1 \\ge 10\\] \\[n_1(1-\\hat{p}_1) \\ge 10\\] \\[ n_2\\hat{p}_2 \\ge 10\\] \\[n_2(1-\\hat{p}_2) \\ge 10\\]\nUse the following calculator to check the above requirements:\n\n# All must be true:\n\nx1 &lt;- \nn1 &lt;- \nphat1 &lt;- x1/n1\n\nError: object 'x1' not found\n\nn1*phat1 &gt;= 10\n\nError: object 'n1' not found\n\nn1*(1-phat1) &gt;=10\n\nError: object 'n1' not found\n\nx2 &lt;- \nn2 &lt;- \nphat2 &lt;- x2 / n2\n\nError: object 'x2' not found\n\nn2*phat2 &gt;= 10\n\nError: object 'n2' not found\n\nn2*(1-phat2) &gt;=10\n\nError: object 'n2' not found\n\n\nQuestion: Are the requirements satisfied for assuming normality?\nAnswer:"
  },
  {
    "objectID": "6-Statistical_Tests_Part2/One_and_Two_Sample_Proportion_Practice.html#hypothesis-test-2",
    "href": "6-Statistical_Tests_Part2/One_and_Two_Sample_Proportion_Practice.html#hypothesis-test-2",
    "title": "One and Two Sample Proportion Practice",
    "section": "Hypothesis Test",
    "text": "Hypothesis Test\nConstruct a null and alternative hypothesis for the study:\n\\[H_0: \\]\n\\[H_a: \\]\nPerform the appropriate analysis:\n\nprop.test()\n\nError in prop.test(): argument \"x\" is missing, with no default\n\n\nQuestion: What is the P-value?\nAnswer:\nQuestion: Based on your decision rule, state your conclusion in context of the research question:\nAnswer:"
  },
  {
    "objectID": "6-Statistical_Tests_Part2/One_and_Two_Sample_Proportion_Practice.html#confidence-interval-2",
    "href": "6-Statistical_Tests_Part2/One_and_Two_Sample_Proportion_Practice.html#confidence-interval-2",
    "title": "One and Two Sample Proportion Practice",
    "section": "Confidence Interval",
    "text": "Confidence Interval\nCreate a 95% confidence interval for the difference in the proportion of divorces between communicative and non-communicative couples:\nQuestion: Interpret the confidence interval in context of the question:\nAnswer:\n\nTest Requirements\nUse the following calculator to check the requirements for the hypothesis test and confidence interval:\n\n# All must be true:\n\nx1 &lt;- \nn1 &lt;- \nphat1 &lt;- x1/n1\n\nError: object 'x1' not found\n\nn1*phat1 &gt;= 10\n\nError: object 'n1' not found\n\nn1*(1-phat1) &gt;=10\n\nError: object 'n1' not found\n\nx2 &lt;- \nn2 &lt;- \nphat2 &lt;- x2 / n2\n\nError: object 'x2' not found\n\nn2*phat2 &gt;= 10\n\nError: object 'n2' not found\n\nn2*(1-phat2) &gt;=10\n\nError: object 'n2' not found\n\n\nQUESTION: Are the requirements for the hypothesis test and confidence interval satisfied?\nANSWER:"
  },
  {
    "objectID": "6-Statistical_Tests_Part2/13-AA_Who_Shot_First_Cat_Vars.html",
    "href": "6-Statistical_Tests_Part2/13-AA_Who_Shot_First_Cat_Vars.html",
    "title": "Who Shot First?",
    "section": "",
    "text": "Use the Star Wars dataset to answer the following questions:\n\nDo less than 20% of respondents feel Very Favorably towards Emperor Palpatine? (1-sample Z test for proportion)\nWhat is the difference in proportions of females and males who are Very Favorable towards Jar-Jar Binks? (2-sample Proportion)\nCome up with one other 2-sample proportion test using anything from the Star Wars dataset.\nTest to see if income and response to “Which Character Shot First?” are Independent (Chi-square)\n\nFor the proportion tests:\n\nDefine the null and alternative hypotheses\nInclude an explanation and conclusion for hypothesis tests\nInclude Confidence intervals and a sentence explaining each\nCheck the requirements for the hypothesis test and the confidence intervals\n\nFor the Chi-square test:\n\nDefine the null and alternative hypotheses\nInclude an explanation of the conclusion for\nBe sure to check the hypothesis requirements for a test of independence"
  },
  {
    "objectID": "6-Statistical_Tests_Part2/13-AA_Who_Shot_First_Cat_Vars.html#explore-the-data",
    "href": "6-Statistical_Tests_Part2/13-AA_Who_Shot_First_Cat_Vars.html#explore-the-data",
    "title": "Who Shot First?",
    "section": "Explore the data",
    "text": "Explore the data\n\nnames(sw)\n\n [1] \"Are You a Fan of SW?\"               \"Favorability_Han Solo\"             \n [3] \"Favorability_Luke Skywalker\"        \"Favorability_Princess Leia Organa\" \n [5] \"Favorability_Anakin Skywalker\"      \"Favorability_Obi Wan Kenobi\"       \n [7] \"Favorability_Emperor Palpatine\"     \"Favorability_Darth Vader\"          \n [9] \"Favorability_Lando Calrissian\"      \"Favorability_Boba Fett\"            \n[11] \"Favorability_C-3P0\"                 \"Favorability_R2 D2\"                \n[13] \"Favorability_Jar Jar Binks\"         \"Favorability_Padme Amidala\"        \n[15] \"Favorability_Yoda\"                  \"who_shot_first\"                    \n[17] \"Familiar_with_expanded_universe\"    \"are_you_a_fan_of_expanded_universe\"\n[19] \"fan_of_star_trek\"                   \"Gender\"                            \n[21] \"Age\"                                \"Household.Income\"                  \n[23] \"Education\"                          \"Location\"                          \n\ntable(sw$`Favorability_Han Solo`)\n\n\n                                            \n                                          5 \nNeither favorably nor unfavorably (neutral) \n                                         44 \n                         Somewhat favorably \n                                        151 \n                       Somewhat unfavorably \n                                          8 \n                           Unfamiliar (N/A) \n                                         15 \n                             Very favorably \n                                        610 \n                           Very unfavorably \n                                          1 \n\naddmargins(table(sw$`Favorability_Han Solo`, sw$Gender))\n\n                                             \n                                                  Female Male Sum\n                                                0      2    3   5\n  Neither favorably nor unfavorably (neutral)   1     22   21  44\n  Somewhat favorably                            4     71   76 151\n  Somewhat unfavorably                          1      3    4   8\n  Unfamiliar (N/A)                              0      9    6  15\n  Very favorably                               10    289  311 610\n  Very unfavorably                              0      0    1   1\n  Sum                                          16    396  422 834\n\naddmargins(table(sw$`Favorability_Emperor Palpatine`))\n\n\n                                            \n                                         20 \nNeither favorably nor unfavorably (neutral) \n                                        213 \n                         Somewhat favorably \n                                        143 \n                       Somewhat unfavorably \n                                         68 \n                           Unfamiliar (N/A) \n                                        156 \n                             Very favorably \n                                        110 \n                           Very unfavorably \n                                        124 \n                                        Sum \n                                        834"
  },
  {
    "objectID": "6-Statistical_Tests_Part2/13-AA_Who_Shot_First_Cat_Vars.html#one-sample-proportion-test",
    "href": "6-Statistical_Tests_Part2/13-AA_Who_Shot_First_Cat_Vars.html#one-sample-proportion-test",
    "title": "Who Shot First?",
    "section": "One-sample Proportion Test",
    "text": "One-sample Proportion Test\nWhat proportion of respondents are very favorable towards Emperor Palpatine?\nIs this significantly less than 20%?"
  },
  {
    "objectID": "6-Statistical_Tests_Part2/13-AA_Who_Shot_First_Cat_Vars.html#two-sample-proportion-test",
    "href": "6-Statistical_Tests_Part2/13-AA_Who_Shot_First_Cat_Vars.html#two-sample-proportion-test",
    "title": "Who Shot First?",
    "section": "Two-sample Proportion Test",
    "text": "Two-sample Proportion Test\nWhat percent of female respondents are favorable towards Jar-Jar Binks?\nWhat percent of male respondents are favorable towards Jar-Jar Binks?\nAre they significantly different?"
  },
  {
    "objectID": "6-Statistical_Tests_Part2/13-AA_Who_Shot_First_Cat_Vars.html#choose-your-own-adventure",
    "href": "6-Statistical_Tests_Part2/13-AA_Who_Shot_First_Cat_Vars.html#choose-your-own-adventure",
    "title": "Who Shot First?",
    "section": "Choose your own adventure",
    "text": "Choose your own adventure\nCompare 2 proportions of your choosing and perform a prop.test()."
  },
  {
    "objectID": "6-Statistical_Tests_Part2/13-AA_Who_Shot_First_Cat_Vars.html#chi-square-test-for-independence",
    "href": "6-Statistical_Tests_Part2/13-AA_Who_Shot_First_Cat_Vars.html#chi-square-test-for-independence",
    "title": "Who Shot First?",
    "section": "Chi-square Test for Independence",
    "text": "Chi-square Test for Independence\nTest to see if how you responded to the question “Who Shot First” is independent of income category.\nState your conclusion:"
  },
  {
    "objectID": "6-Statistical_Tests_Part2/11-Chi_Square_Intro.html",
    "href": "6-Statistical_Tests_Part2/11-Chi_Square_Intro.html",
    "title": "Chi-square Test of Independence",
    "section": "",
    "text": "When working through statistical inference for means, we progressed from learning 1-sample t-tests to 2-sample t-tests. When we wanted to compare a quantitative variable between multiple groups, we introduced ANOVA. The hypothesis test changed and we introduced the F-statistic.\nRecall that the F-statistics was based on a ratio of squared quantities and was therefore always positive and skewed right.\nSimilarly, when we want to compare a categorical variable across multiple groups, we must modify the hypothesis test from the 2-sample proportion test and introduce a new test statistic: \\(\\chi^2\\). The Greek letter, \\(\\chi\\), is pronounced like “ki” in “kite”, not like “chi” in “tai chi”.\nAs can be seen from its name, \\(\\chi^2\\) is a squared value and is thus always positive and right skewed like the F-statistic.\n\n\nThe \\(\\chi^2\\) distribution also has degrees of freedom that determine its shape.\n\\[df = (r-1)(c-1)\\] Where \\(r\\) is the number of rows and \\(c\\) is the number of columns in a summary table.\n\n\n\nThe null and alternative hypotheses test for \\(\\chi^2\\) test for independence are always the same.\n\\[H_0: \\text{The row variable is independent of the column variable}\\] \\[H_A: \\text{The row variable is not independent of the column variable}\\] While not a fan of the double negative, it serves a technical purpose. Mathematically, we get the same test statistic and p-value if we swap rows and columns. We cannot say the row variable depends on the column variable without also saying that the column variable depends on the row variable.\nThink of Alice at the Mad Hatter’s tea party:\n\n“Then you should say what you mean,” the March Hare went on. “I do,” Alice hastily replied; “at least-at least I mean what I say-that’s the same thing, you know.”\n“Not the same thing a bit!” said the Hatter. “Why, you might just as well say that ‘I see what I eat’ is the same thing as ‘I eat what I see’!”\n“You might just as well say,” added the March Hare, “that ‘I like what I get’ is the same thing as ‘I get what I like’!”\n“You might just as well say,” added the Dormouse, which seemed to be talking in its sleep, “that ‘I breathe when I sleep’ is the same thing as ‘I sleep when I breathe’!”\n“It is the same thing with you.” said the Hatter,”\n\nSo we are resigned to conclude that we have sufficient/insufficient evidence that they are not independent."
  },
  {
    "objectID": "6-Statistical_Tests_Part2/11-Chi_Square_Intro.html#degrees-of-freedom",
    "href": "6-Statistical_Tests_Part2/11-Chi_Square_Intro.html#degrees-of-freedom",
    "title": "Chi-square Test of Independence",
    "section": "",
    "text": "The \\(\\chi^2\\) distribution also has degrees of freedom that determine its shape.\n\\[df = (r-1)(c-1)\\] Where \\(r\\) is the number of rows and \\(c\\) is the number of columns in a summary table."
  },
  {
    "objectID": "6-Statistical_Tests_Part2/11-Chi_Square_Intro.html#hypothesis-test",
    "href": "6-Statistical_Tests_Part2/11-Chi_Square_Intro.html#hypothesis-test",
    "title": "Chi-square Test of Independence",
    "section": "",
    "text": "The null and alternative hypotheses test for \\(\\chi^2\\) test for independence are always the same.\n\\[H_0: \\text{The row variable is independent of the column variable}\\] \\[H_A: \\text{The row variable is not independent of the column variable}\\] While not a fan of the double negative, it serves a technical purpose. Mathematically, we get the same test statistic and p-value if we swap rows and columns. We cannot say the row variable depends on the column variable without also saying that the column variable depends on the row variable.\nThink of Alice at the Mad Hatter’s tea party:\n\n“Then you should say what you mean,” the March Hare went on. “I do,” Alice hastily replied; “at least-at least I mean what I say-that’s the same thing, you know.”\n“Not the same thing a bit!” said the Hatter. “Why, you might just as well say that ‘I see what I eat’ is the same thing as ‘I eat what I see’!”\n“You might just as well say,” added the March Hare, “that ‘I like what I get’ is the same thing as ‘I get what I like’!”\n“You might just as well say,” added the Dormouse, which seemed to be talking in its sleep, “that ‘I breathe when I sleep’ is the same thing as ‘I sleep when I breathe’!”\n“It is the same thing with you.” said the Hatter,”\n\nSo we are resigned to conclude that we have sufficient/insufficient evidence that they are not independent."
  },
  {
    "objectID": "6-Statistical_Tests_Part2/11-Chi_Square_Intro.html#test-requirements",
    "href": "6-Statistical_Tests_Part2/11-Chi_Square_Intro.html#test-requirements",
    "title": "Chi-square Test of Independence",
    "section": "Test Requirements",
    "text": "Test Requirements\nRecall that 2-sample tests for proportions needed an at least 10 expected successes and at least 10 expected failures (\\(np \\ge 10\\) and \\(n(1-p)\\ge10\\)) for the test statistic to be valid.\nFor a \\(\\chi^2\\) test we need to check the expected counts for all the different combinations.\nWe don’t need to fret about the math behind the expected count calculation. Intuitively, if there was no relationship between the two variables you would expect all the row totals to be proportionally distributed across the column groups.\nWe only need to check that all expected counts are greater than 5.\n\nchisq.test(chiro_table)$expected &gt;= 5\n\n               \n                At Risk Prevention Self Care Sick Role Wellness\n  Australia        TRUE       TRUE      TRUE      TRUE     TRUE\n  Europe           TRUE       TRUE      TRUE      TRUE     TRUE\n  United States    TRUE       TRUE      TRUE      TRUE     TRUE"
  },
  {
    "objectID": "6-Statistical_Tests_Part2/11-Chi_Square_Intro.html#visualization",
    "href": "6-Statistical_Tests_Part2/11-Chi_Square_Intro.html#visualization",
    "title": "Chi-square Test of Independence",
    "section": "Visualization",
    "text": "Visualization\nWe can use ggplot() to create nice bar charts to help interpret the results.\n\nggplot(chiropractic, aes(x = location, fill = motivation)) +\n  geom_bar(position = \"dodge\") +\n  theme_minimal()"
  },
  {
    "objectID": "6-Statistical_Tests_Part2/11-Chi_Square_Intro.html#homework-grades-and-classroom-type",
    "href": "6-Statistical_Tests_Part2/11-Chi_Square_Intro.html#homework-grades-and-classroom-type",
    "title": "Chi-square Test of Independence",
    "section": "Homework: Grades and Classroom Type",
    "text": "Homework: Grades and Classroom Type\nThis is the raw data for the Homework Quiz. Use it to answer the homework questions, but also create a Bar Chart using ggplot().\n\ncourse &lt;- import('https://raw.githubusercontent.com/byuistats/Math221D_Cannon/master/Data/course_type_by_grade.csv')\n\nCreate a Bar Chart using ggplot()\nPerform the \\(\\chi^2\\) test of independence.\nQuestion: Are the requirements for a \\(\\chi^2\\) test satisfied?\nAnswer:\nQuestion: What are the null and alternative hypotheses?\nAnswer:\nQuestion: What is the test statistic, \\(\\chi^2\\)?\nAnswer:\nQuestion: How many degrees of freedom does this test have?\nAnswer:\nQuestion: What is the P-value?\nAnswer:"
  },
  {
    "objectID": "6-Statistical_Tests_Part2/11-Chi_Square_Intro.html#heart-disease-among-australian-women-and-men",
    "href": "6-Statistical_Tests_Part2/11-Chi_Square_Intro.html#heart-disease-among-australian-women-and-men",
    "title": "Chi-square Test of Independence",
    "section": "Heart Disease among Australian Women and Men",
    "text": "Heart Disease among Australian Women and Men\nIn 1982 in Western Australia, 1317 males and 854 females died of ischemic heart disease, 1119 males and 828 females died of cancer, 371 males and 460 females died of cerebral vascular disease, and 346 males and 147 females died of accidents. A medical researcher wanted to see if gender and cause of death are independent using a level of significance of 0.05.\nThe data read in below are a summary table of counts. One way to create a bar chart to compare heart disease deaths between men and women is to take this data and make it “longer”. This stacks the columns with count data in them and makes 2 new columns, one for the counts and one for the category of cardiovascular death.\nI will comment the code below to walk through each step.\nThe %&gt;% “pipe” below comes from the tidyverse library. You can think of this like making a series of steps where everything before the %&gt;% is pushed to the next step. For example, we assign aussie_death the original data table then move the table into the pivot_long() function which is the function that stacks the data. The output of that function becomes a new data shape, aussie_death.\n\n# Import the table data\naussie_death_table &lt;- import(\"https://byuistats.github.io/M221R/Data/quiz/R/aussie_death.csv\") \naussie_death_table\n\n      V1 heart_disease cancer vascular_disease accident\n1 female           854    828              460      147\n2   male          1371   1119              371      346\n\n# Pipe the original table into the pivot_longer() function\n# Pivot_longer needs to know which columns to \"stack\" which is input by the 'cols = ' argument.\n# We also need to give a name to the new column containing the count information.  The 'values_to=' argument names the column that will have the values, in this case we use \"count\".  \n# The 'names_to = ' argument names the column that will contain the labels of each category\n\n# Run the code and see if you can follow what happened\n\naussie_death &lt;- aussie_death_table %&gt;% \n  pivot_longer(cols = c('heart_disease', 'cancer', 'vascular_disease', 'accident'), values_to = 'count', names_to = \"reason\")\n\naussie_death\n\n# A tibble: 8 × 3\n  V1     reason           count\n  &lt;chr&gt;  &lt;chr&gt;            &lt;int&gt;\n1 female heart_disease      854\n2 female cancer             828\n3 female vascular_disease   460\n4 female accident           147\n5 male   heart_disease     1371\n6 male   cancer            1119\n7 male   vascular_disease   371\n8 male   accident           346\n\n# V1 was the default label and really represents \"Gender\" in this study.  We could change the name or leave it as is and fix it in the graphs\n\n\n# When using raw data, 'geom_bar()' creates the counts automatically from the categorical columns in a raw dataset.  The data in aussie_death is still a summary.  If we have the counts already, then we use the `geom_col()` function and have to specify a y variable to define how high to make the bars (in our case \"count\")\n\nggplot(aussie_death, aes(x = reason, y = count, fill = V1)) +\n  geom_col(position=\"dodge\")"
  },
  {
    "objectID": "6-Statistical_Tests_Part2/09-Two_Sample_Proportion_Ztest.html",
    "href": "6-Statistical_Tests_Part2/09-Two_Sample_Proportion_Ztest.html",
    "title": "Two-sample Proportion Tests",
    "section": "",
    "text": "In statistics, two sample proportion tests are used to compare proportions or percentages between two independent groups. We here discuss these tests and provide examples of their application in R.\nThe hypothesis test should not be surprising:\n\\[H_0: p_1 = p_2\\] \\[H_a: p_1\\; (&lt;,&gt;,\\neq) \\: p_2\\] where \\(p_1\\) represents the unknown population proportion for group 1 and \\(p_2\\) represents the unknown population proportion for group 2."
  },
  {
    "objectID": "6-Statistical_Tests_Part2/09-Two_Sample_Proportion_Ztest.html#example-1-voting-behaviour-by-gender",
    "href": "6-Statistical_Tests_Part2/09-Two_Sample_Proportion_Ztest.html#example-1-voting-behaviour-by-gender",
    "title": "Two-sample Proportion Tests",
    "section": "Example 1: Voting Behaviour by Gender",
    "text": "Example 1: Voting Behaviour by Gender\nSuppose we want to test if women are more likely to identify as Democrat than men. We sample 250 men and 250 women and measure their political affiliation. We find that 80 men identify as Democrat and 102 females identify as Democrat.\nJust as with the two-sample t-test for means, we must define a reference group. In this example, we will use females as the reference group so that our alternative will be relative to that group.\n\\[H_0: p_{femaleDem} = p_{maleDem}\\] \\[H_a: p_{femaleDem} &gt; p_{maleDem}\\] We will use \\(\\alpha = 0.05\\)\n\nprop.test(x = c(102, 80), n = c(250, 250), alternative = \"greater\")\n\n\n    2-sample test for equality of proportions with continuity correction\n\ndata:  c(102, 80) out of c(250, 250)\nX-squared = 3.8099, df = 1, p-value = 0.02548\nalternative hypothesis: greater\n95 percent confidence interval:\n 0.01350993 1.00000000\nsample estimates:\nprop 1 prop 2 \n 0.408  0.320 \n\n\nWe can also create a confidence interval for the difference:\n\nprop.test(x = c(102, 80), n = c(250, 250))$conf.int\n\n[1] 5.904086e-06 1.759941e-01\nattr(,\"conf.level\")\n[1] 0.95\n\n\nConfidence intervals for differences can be positive and negative. In this example, a negative number would indicate that Females are less likely to be Democrat and a positive number means they are more likely to be Democrat.\nOur confidence interval is just above zero on the lower end. We are 95% confident that females are between 0.000% and 17.6% more likely to be Democrat than men.\n\nTest Requirments\nJust as with 1-sample proportion tests, we must validate that we have a large enough sample size to ensure that \\(\\hat{p}\\) is approximately normally distributed. When we have 2 samples, however, we must check both \\(\\hat{p}\\)’s. For both hypothesis testing and confidence intervals we check:\nRequirements for Hypothesis Testing and Confidence Intervals\n\\[ n_1\\hat{p}_1 \\ge 10\\] \\[n_1(1-\\hat{p}_1) \\ge 10\\] \\[ n_2\\hat{p}_2 \\ge 10\\] \\[n_2(1-\\hat{p}_2) \\ge 10\\]\nAn easy R calculator to check this is:\n\n# All must be true:\n\nx1 &lt;- 102\nn1 &lt;- 250\nphat1 &lt;- x1/n1\n\nn1*phat1 &gt;= 10\n\n[1] TRUE\n\nn1*(1-phat1) &gt;=10\n\n[1] TRUE\n\nx2 &lt;- 80\nn2 &lt;- 250\nphat2 &lt;- x2 / n2\n\nn2*phat2 &gt;= 10\n\n[1] TRUE\n\nn2*(1-phat2) &gt;=10\n\n[1] TRUE\n\n\nIf all conditions are greater than or equal to 10, we can trust our p-values and confidence intervals."
  },
  {
    "objectID": "6-Statistical_Tests_Part2/09-Two_Sample_Proportion_Ztest.html#example-2-favorite-sports",
    "href": "6-Statistical_Tests_Part2/09-Two_Sample_Proportion_Ztest.html#example-2-favorite-sports",
    "title": "Two-sample Proportion Tests",
    "section": "Example 2: Favorite Sports",
    "text": "Example 2: Favorite Sports\nSoccer is becoming much more popular in the United States. We would like to test if this is being driven by demographic shifts in the population where the younger generation is more likely to favor soccer.\nA researcher samples 524 individuals under 40 and 655 individuals older than 40 and asks what their preferred sport is. Of the 524 respondents under 40, 44 identified soccer as their favorite sport. Of the 655 respondents over 40, 27 identified soccer as their favorite sport.\nPerform a 2-sample proportion test to determine if significantly more younger people identify soccer as their favorite sport.\nCreate and interpret the confidence interval for the difference in the proportions.\nAre the requirements for the hypothesis test and confidence interval satisfied?"
  },
  {
    "objectID": "6-Statistical_Tests_Part2/07-One_Sample_Proportion_Ztest.html",
    "href": "6-Statistical_Tests_Part2/07-One_Sample_Proportion_Ztest.html",
    "title": "One-sample Proportion Tests",
    "section": "",
    "text": "In statistics, one-sample proportion tests are used to compare proportions or percentages to a hypothesized value. These tests are useful when dealing with categorical data. We here discuss these tests and provide examples of their application in R.\nThe hypothesis test should look very familiar:\n\\[H_0: p = p_0\\] \\[H_a: p\\; (&lt;,&gt;,\\neq) \\: p_0\\] where \\(p_0\\) is some hypothesized value for the population proportion.\nSo far, we have been using Greek letters to represent population parameters. We deviate from that now due to the fact that the Greek letter for p, \\(\\pi\\), already has a long-established meaning in mathematics. In the hypothesis definition above, \\(p\\) represents the population proportion."
  },
  {
    "objectID": "6-Statistical_Tests_Part2/07-One_Sample_Proportion_Ztest.html#example-1-one-sample-proportion-test",
    "href": "6-Statistical_Tests_Part2/07-One_Sample_Proportion_Ztest.html#example-1-one-sample-proportion-test",
    "title": "One-sample Proportion Tests",
    "section": "Example 1: One Sample Proportion Test",
    "text": "Example 1: One Sample Proportion Test\nSuppose we want to test whether the proportion of students who passed an exam is significantly less than 0.75. We have a sample of 100 students, of which 72 passed.\n\\[\\hat{p} = \\frac{X}{N} = \\frac{72}{100} = .72\\]\nIf we want to test if this is significantly less than 75%, we can use the prop.test() which is very similar to t.test(). Instead of putting in a sample mean, \\(\\bar{x}\\), with a hypothesized \\(\\mu\\), we put in \\(X\\) and \\(N\\) and a hypothesized \\(p\\). Setting the alternative and confidence level operates the same as t.test().\nConfidence intervals for proportions for proportions can also be obtained just as with t.test().\n\n# One Sample Proportion Test Example\n# Hypothesized proportion: 0.75\n# Sample size: 100\n# Number of successes: 72\n\nprop.test(x = 72, n = 100, p = 0.75, alternative = \"less\", conf.level = .9)\n\n\n    1-sample proportions test with continuity correction\n\ndata:  72 out of 100, null probability 0.75\nX-squared = 0.33333, df = 1, p-value = 0.2819\nalternative hypothesis: true p is less than 0.75\n90 percent confidence interval:\n 0.0000000 0.7782396\nsample estimates:\n   p \n0.72 \n\nprop.test(x = 72, n = 100, conf.level = .9)$conf.int\n\n[1] 0.6358512 0.7917861\nattr(,\"conf.level\")\n[1] 0.9\n\n\n\nThe distribution of \\(\\hat{p}\\)\nRecall that the sampling distribution for \\(\\hat{p}\\) is approximately normally distributed when our sample has more than 10 expeted “successes” and more than 10 expected “failures”. We test this by looking at\nHypothesis Test Requirements:\n\\[np \\geq 10\\] \\[n(1-p) \\geq 10\\]\nWe use p, not \\(\\hat{p}\\) for hypothesis testing because hypothesis testing always assumes the null hypothesis is true. Confidence intervals, on the other hand, make no such assumption.\nTo see if the calculated confidence interval is appropriate, we use \\(\\hat{p}\\).\nConfidence Interval Requirements: \\[n\\hat{p} \\geq 10\\] \\[n(1-\\hat{p}) \\geq 10\\] Can we trust the p-value and confidence interval?\nYou can use a calculator for this, or simply use R as a calculator:\n\nx &lt;- 72\nn &lt;- 100\np_hat &lt;- x/n\np &lt;- .75\n\n# For Hypothesis Testing:\nn*p &gt;= 10\n\n[1] TRUE\n\nn*(1-p) &gt;= 10\n\n[1] TRUE\n\n# For Confidence Intervals:\nn*p_hat &gt;= 10\n\n[1] TRUE\n\nn*(1-p_hat) &gt;=10\n\n[1] TRUE"
  },
  {
    "objectID": "6-Statistical_Tests_Part2/07-One_Sample_Proportion_Ztest.html#example-2-handedness",
    "href": "6-Statistical_Tests_Part2/07-One_Sample_Proportion_Ztest.html#example-2-handedness",
    "title": "One-sample Proportion Tests",
    "section": "Example 2: Handedness",
    "text": "Example 2: Handedness\nSuppose the United States national average percent of left-handed people is 11%. A researcher wants to know if visual arts majors are significantly more likely to be left handed. She samples 250 visual arts majors and finds that 36 are left handed.\nPerform a one-sample proportion to see if visual arts majors are significantly more left-handed than the general population.\nState the null and alternative hypotheses and your significance level.\n\\[H_0: p = \\] \\[H_a: p \\] \\[\\alpha = \\]\nQuestion: What is the value of the test statistics for this test?\nAnswer:\nQuestion: What is the P-Value?\nAnswer:\nQuestion: State your conclusion in context of this problem:\nAnswer:\nMake a \\((1-\\alpha)\\) level confidence interval for the true population proportion.\nQuestion: Interpret the confidence interval in context of the question:\nAnswer:\nQuestion: Are the test requirements for the normality of \\(\\hat{p}\\) satisfied?\nAnswer:\nQuestion: Are the requirements for a confidence interval for \\(p\\) satisfied?\nAnswer:"
  },
  {
    "objectID": "6-Statistical_Tests_Part2/05-Distribution_of_Phat.html",
    "href": "6-Statistical_Tests_Part2/05-Distribution_of_Phat.html",
    "title": "Sampling Distribution of P_hat",
    "section": "",
    "text": "Categorical data is often summarized as a percent. If we randomly select 500 students and find 276 have brown hair, we can estimate the population proportion using \\(\\hat{p} = \\frac{X}{N}\\) where \\(X\\) is the number with brown hair and N is the number in our sample. We often interchange percents and proportions, but strictly speaking, a proportion is a number between 0 and 1. This can be interpreted as a probability as well.\nIf another researcher were to collect a different sample of 500 from the same population, they would almost certainly get a different number of people with brown hair than the first study. We can imagine taking many samples of 500 students and imagine the theoretical distribution of all possible \\(\\hat{p}\\). If we are taking good samples, most of these \\(\\hat{p}\\)’s should be near the population proportion, \\(p\\).\nIn fact, under certain conditions we can know the distribution of \\(\\hat{p}\\). As you probably guessed, the distribution of \\(\\hat{p}\\) is approximately normal with:\n\\[\\mu_{\\hat{p}} = p\\] \\[\\sigma_{\\hat{p}} = \\sqrt{\\frac{p(1-p)}{N}}\\]\nThat is to say, the mean of all sample proportions is the population proportion.\nJust as it was with a sample mean, \\(\\bar{x}\\), we have to check certain conditions to assume the distribution is approximately normal. For \\(\\bar{x}\\), we needed the population to be normally distributed or to have a sample size greater than 30. The principle of having a large enough sample applies, but it’s different for a proportion.\nWe can assume the distribution is approximately normal if:\n\\[np \\geq 10\\] \\[n(1-p) \\geq 10\\]\nIn plain English, this means our sample size has to be big enough to have at least 10 “successes” and 10 “failures”. For example, if we’re estimating the proportion of left handed people, we would need a sample size large enough to have at least 10 left handed people and 10 right handed people.\nIf the distribution of sample means is approximately normal according to the conditions above, we can calculate a z-score as we did in Unit 1 and 2:\n\\[z = \\frac{\\hat{p}-p}{\\sqrt{\\frac{p(1-p)}{N}}}\\]\nthen use pnorm() as before."
  },
  {
    "objectID": "6-Statistical_Tests_Part2/05-Distribution_of_Phat.html#example",
    "href": "6-Statistical_Tests_Part2/05-Distribution_of_Phat.html#example",
    "title": "Sampling Distribution of P_hat",
    "section": "Example",
    "text": "Example\nSuppose we have a population where the true proportion of success is \\(p = 0.6\\). We take a random sample of size \\(n = 100\\) from this population. We want to find the probability that the sample proportion \\(\\hat{p}\\) is less than \\(0.55\\).\n\n# Given data\nx &lt;- 55\nn &lt;- 100\np_hat &lt;- x/n\n\n# population proportion\np &lt;- 0.6\n\n# Calculate standard deviation\nsigma_phat &lt;- sqrt(p * (1 - p) / n)\n\n# Calculate z-score\nz &lt;- (p_hat - p) / sigma_phat\n\n# Left Tail (lower than p)\npnorm(z)\n\n[1] 0.1537171\n\n# Right Tail (greater than p)\n1-pnorm(z)\n\n[1] 0.8462829"
  },
  {
    "objectID": "6-Statistical_Tests_Part2/05-Distribution_of_Phat.html#your-turn",
    "href": "6-Statistical_Tests_Part2/05-Distribution_of_Phat.html#your-turn",
    "title": "Sampling Distribution of P_hat",
    "section": "Your Turn",
    "text": "Your Turn\nThe nationwide, fully-vaccinated rate if November 2021 was 58%. Based on survey responses of 150, 81% of all BYU-I students on campus during Fall 2021 semester had received at least one vaccination dose against COVID-19 with 74% being fully vaccinated.\nWhat is the probability that we get a random sample of 150 individuals with a \\(\\hat{p}\\) higher than the fully vaccinated rate that we observed?\n\n# given data\n\n# Population Proportion\n\n# Calculate Z\n\n# P-value"
  },
  {
    "objectID": "6-Statistical_Tests_Part2/02-Linear_Regression_Intro.html",
    "href": "6-Statistical_Tests_Part2/02-Linear_Regression_Intro.html",
    "title": "Simple Linear Regression",
    "section": "",
    "text": "Consider the relationship between Score on a math exam and a student’s self-reported Confidence Rating.\n\nlibrary(tidyverse)\nlibrary(mosaic)\nlibrary(rio)\nlibrary(car)\n\nmath &lt;- import('https://byuistats.github.io/BYUI_M221_Book/Data/MathSelfEfficacy.xlsx')\n\nQuestion: What is the explanatory (aka independent) variable, \\(x\\)?\nAnswer:\nQuestion: What is the response (aka the dependent) variable, \\(y\\)?\nAnswer:\nPlot the relationship:\n\nplot(Score ~ ConfidenceRatingMean, data = math)\n\n\n\n\n\n\n\nggplot(math, aes(x = ConfidenceRatingMean, y = Score )) +\n  geom_point(color = \"darkblue\") +\n  theme_bw() +\n  labs(\n    title = \"Relationship between Student Confidence Rating in Math and Test Score\"\n  ) \n\n\n\n\n\n\n\n\nQuestion: Does the relationship appear linear?\nAnswer:\nQuestion: What is the direction of the relationship? Answer:\nQuestion: What do you think is the strength of the relationship? (Strong/Moderate/Weak) Answer:\nQuestion: What is the correlation coefficient, r? Answer:\n\ncor(Score ~ ConfidenceRatingMean, data = math)\n\n[1] 0.7278648"
  },
  {
    "objectID": "6-Statistical_Tests_Part2/02-Linear_Regression_Intro.html#plotting-the-regression-line",
    "href": "6-Statistical_Tests_Part2/02-Linear_Regression_Intro.html#plotting-the-regression-line",
    "title": "Simple Linear Regression",
    "section": "Plotting the Regression Line",
    "text": "Plotting the Regression Line\n\nBase R\nScatter plots by themselves are nice, but we would also like to see the regression line. Simple graphics in R can be augmented by using some functions. The abline() function in base R, when executed right after a graphing function can add lines. We’ve used this to add vertical lines and horizontal line already in class. We can also use this function to add a regression line. We simply insert our linear model output into the abline() function as follows:\n\nplot(Score ~ ConfidenceRatingMean, data = math)\nabline(math_lm)\n\n\n\n\n\n\n\n\nJust as with the other plotting functions we’ve used, we can change the color, type and width of the line:\n\nplot(Score ~ ConfidenceRatingMean, data = math, pch = 16, main = \"Title\")\nabline(math_lm, col = \"purple\", lwd = 3, lty = 3)"
  },
  {
    "objectID": "6-Statistical_Tests_Part2/02-Linear_Regression_Intro.html#ggplot",
    "href": "6-Statistical_Tests_Part2/02-Linear_Regression_Intro.html#ggplot",
    "title": "Simple Linear Regression",
    "section": "GGPlot",
    "text": "GGPlot\nUsing ggplot(), we can simply add a geom_smooth() geometry and specify the method of “smoothing” as a linear model:\n\nggplot(math, aes(x = ConfidenceRatingMean, y = Score )) +\n  geom_point(color = \"darkblue\") +\n  theme_bw() +\n  labs(\n    title = \"Relationship between Student Confidence Rating in Math and Test Score\"\n  ) +\n  geom_smooth(method=\"lm\")\n\n\n\n\n\n\n\n\nBy default, this gives us a confidence interval for the slope of the regression line. We can turn that off by forcing the “standard error” to be FALSE:\n\nggplot(math, aes(x = ConfidenceRatingMean, y = Score )) +\n  geom_point(color = \"darkblue\") +\n  theme_bw() +\n  labs(\n    title = \"Relationship between Student Confidence Rating in Math and Test Score\"\n  ) +\n  geom_smooth(method=\"lm\", se = FALSE)"
  },
  {
    "objectID": "6-Statistical_Tests_Part2/02-Linear_Regression_Intro.html#hypothesis-testing-for-regression",
    "href": "6-Statistical_Tests_Part2/02-Linear_Regression_Intro.html#hypothesis-testing-for-regression",
    "title": "Simple Linear Regression",
    "section": "Hypothesis Testing for Regression",
    "text": "Hypothesis Testing for Regression\nA linear equation has 2 parameters: Slope and Intercept. In most situations, the intercept isn’t very interesting by itself and is often absurd. We are most often only interested in the slope\n\\[H_o: \\beta_1 = 0\\] \\[H_a: \\beta_1 \\neq 0\\]\nThese are the same for all simple linear regression questions.\nTo get the p-value and test statistics, we use the summary() function as we did with aov:\n\nsummary(math_lm)\n\n\nCall:\nlm(formula = Score ~ ConfidenceRatingMean, data = math)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-38.200  -6.163   1.292   7.567  23.422 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)            18.690      4.610   4.054  8.4e-05 ***\nConfidenceRatingMean   12.695      1.022  12.424  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.27 on 137 degrees of freedom\nMultiple R-squared:  0.5298,    Adjusted R-squared:  0.5264 \nF-statistic: 154.4 on 1 and 137 DF,  p-value: &lt; 2.2e-16\n\n\nWe can also calculate confidence intervals for the slope by using the confint() function. This function requires you to tell it which model to extract a confidence intervals from. You can specify which parameter you’re interested in, and the level of confidence:\n\n# input the model into the following function:\nconfint(math_lm, level = .95)\n\n                         2.5 %   97.5 %\n(Intercept)           9.573588 27.80654\nConfidenceRatingMean 10.674297 14.71535\n\n\nHow do we interpret this confidence interval for a slope?\nTechnically correct: 95% Confident that the true population slope is within (10.674297, 14.71535)\nContextual Explanation: For every 1 unit increase in Confidence Rating, test scores go up by between (10.674297, 14.71535) on average."
  },
  {
    "objectID": "6-Statistical_Tests_Part2/02-Linear_Regression_Intro.html#regression-requirements-is-our-p-value-sus",
    "href": "6-Statistical_Tests_Part2/02-Linear_Regression_Intro.html#regression-requirements-is-our-p-value-sus",
    "title": "Simple Linear Regression",
    "section": "Regression Requirements (Is our P-value sus?)",
    "text": "Regression Requirements (Is our P-value sus?)\nThere are certain requirements for all statistical tests to be valid. For means, we needed to make sure that the Central Limit Theorem applied. This meant that we had a large enough sample size (N&gt;30) or that the population itself was normally distributed.\nFor ANOVA, we had to check that the residuals were normally distributed and that the population standard deviations were the same.\nRegression analysis has 5 requirements to be valid. While this sounds daunting, in practice we can check most of them very quickly.\n\nRelationship between X and Y is Linear\nThe residuals, \\(\\epsilon\\), are normally distributed\nThe Variance of the error terms is constant for all values of X\nThe X’s are fixed and measured without error (i.e. X’s can be considered as known constants)\nThe observations are independent\n\nThe linear relationship is assessed visually with the scatter plot. If there is obvious curvature or non-linearity then fitting a line isn’t the best model.\nWe check the normality of the residuals with a qqPlot() exactly as with the aov() output.\nRecall that with ANOVA, we had to check that the variation in each group was roughly the same (largest standard deviation was less than twice as large as the smallest standard deviation). For a quantitative explanatory variable, we can’t calculate the standard deviation for a specified level of \\(x\\) because \\(x\\) can be any number.\nConstant variance in regression is checked with a new plot that looks at how the predicted values relate to the residuals. This is important because we want our predictions to be “wrong” about the same regardless of the value of the prediction. We’re looking for random scatter.\nRequirement 4 cannot be analyzed directly. It is important because because \\(x\\) is the independent variable. If there is uncertainty about the input, then the simple linear regression might not be the most appropriate model.\nRequirement 5 also cannot be analyzed, but random sampling usually satisfies this requirement.\n\n# Requirement 1:  Check for linear relationship\nggplot(math, aes(x=ConfidenceRatingMean, y = Score)) + \n  geom_point()\n\n\n\n\n\n\n\n# Req 2: Normality of residuals:\nqqPlot(math_lm$residuals)\n\n\n\n\n\n\n\n\n[1] 37 89\n\n# Req 3: Constant variance (look odd patterns). When you put lm() output into the plot function it gives you several different plots. The residual plots we're most interested in are 1 and 2\n\nplot(math_lm, which = 1)"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/09-AA_Unit4_Review.html",
    "href": "5-Statistical_Tests_Part1/09-AA_Unit4_Review.html",
    "title": "Comparing Means",
    "section": "",
    "text": "In this activity, you will use everything we’ve covered up to this point including:\n\nData manipulation using tidyverse functions\nHypothesis tests\n\n1-Sample t-test\n2-sample dependent t-test\n2-sample independent t-test\nANOVA\n\nConfidence Intervals where applicable\n\nWe will be using data collected about students in 2 Portuguese schools including their final grade. The goal is to answer research questions using statistical methods to see what factors significantly impact final grades.\n\n\nIn class, we have reinforced a process for approaching a new dataset. The following is a summary of activities that help us conduct good research:\n\nRead in the data\n\nExplore the dataset as a whole:\n\nWhat are the column names? What do they mean? Where can I find information about them?\nWhat is the response/dependent variable? Could there be more than one?\nWhat are some factors that may impact the response variable? Which are likely the most important?\n\nExplore specific columns\n\nStart with the response variable. Are there any outliers? Obtain summary statistics (favstats()), visualize the data (histogram(), boxplot()).\nExplore the explanatory variables you think are most impact to the response variable. What type of data are they (categorical, quantitative)? For categorical variables, what are all the levels (unique())\n\nFormalize statistical hypotheses. If your factors are categorical, how many groups will you be comparing? Is it a 1-sample t-test, 2-sample t-test, ANOVA?\nPrepare data for analysis. You may need to clean the data (eg. data %&gt;% filter() %&gt;% select())\nPerform the appropriate analysis (t.test(), aov())\n\nAll these activities are important, but we may spend more or less time on any one of them depending on the state of the data."
  },
  {
    "objectID": "5-Statistical_Tests_Part1/09-AA_Unit4_Review.html#getting-to-know-a-new-dataset",
    "href": "5-Statistical_Tests_Part1/09-AA_Unit4_Review.html#getting-to-know-a-new-dataset",
    "title": "Comparing Means",
    "section": "",
    "text": "In class, we have reinforced a process for approaching a new dataset. The following is a summary of activities that help us conduct good research:\n\nRead in the data\n\nExplore the dataset as a whole:\n\nWhat are the column names? What do they mean? Where can I find information about them?\nWhat is the response/dependent variable? Could there be more than one?\nWhat are some factors that may impact the response variable? Which are likely the most important?\n\nExplore specific columns\n\nStart with the response variable. Are there any outliers? Obtain summary statistics (favstats()), visualize the data (histogram(), boxplot()).\nExplore the explanatory variables you think are most impact to the response variable. What type of data are they (categorical, quantitative)? For categorical variables, what are all the levels (unique())\n\nFormalize statistical hypotheses. If your factors are categorical, how many groups will you be comparing? Is it a 1-sample t-test, 2-sample t-test, ANOVA?\nPrepare data for analysis. You may need to clean the data (eg. data %&gt;% filter() %&gt;% select())\nPerform the appropriate analysis (t.test(), aov())\n\nAll these activities are important, but we may spend more or less time on any one of them depending on the state of the data."
  },
  {
    "objectID": "5-Statistical_Tests_Part1/09-AA_Unit4_Review.html#categories-labeled-as-numbers",
    "href": "5-Statistical_Tests_Part1/09-AA_Unit4_Review.html#categories-labeled-as-numbers",
    "title": "Comparing Means",
    "section": "Categories Labeled as Numbers",
    "text": "Categories Labeled as Numbers\nSometimes even correct data can have issues that must be addressed. For example, categories are often labeled as numbers. Software can’t guess when numbers are supposed to be categories, so we have to tell R when a number should be treated as a category.\nTo force a variable to be a category, we use the factor() function in R. We can change the variable type in the data itself or change it in the analysis. We demonstrate both methods below.\n\nChanging a Column Type in a dataset\nFather’s education, Fedu, shows up as a number in R. The website suggests that the numbers represent categories (0 = none, 1 = primary education (4th grade), 2 = 5th to 9th grade, 3 = secondary education or 4 = higher education).\nTo change the data type in the data itself, we can use a mutate statement in the following manner:\n\n# Create a new dataset called fedu_data that begins with the clean data and adds a column that we called Fedu_factor, which is the factorized column, Fedu:\n\nnew_data &lt;- student %&gt;%\n  mutate(Fedu_factor = factor(Fedu))\n\n# Check the column names of the new dataset.  Notice the new column\nnames(new_data)\n\n# glimpse() shows us data types.  Notice after Fedu_factor, the &lt;fct&gt;, which shows us that this is in fact, a factor variable type.  &lt;dbl&gt; stands for \"double\" and is a numeric variable type\n\nglimpse(new_data)\n\n\n\nChanging the Variable Type “on the fly”\nYou may not want to bother changing all the variable types for each potential analysis. Fortunately, you can create a factor “on the fly” within the analysis function itself.\nBecause there are more than 2 levels of Father’s Education, I will demonstrate how this is done in an ANOVA:\n\n# Force Fedu to be treated like a category in ANOVA:\nfedu_anova &lt;- aov(student$G3 ~ factor(student$Fedu))\n\nsummary(fedu_anova)\n\n                      Df Sum Sq Mean Sq F value Pr(&gt;F)  \nfactor(student$Fedu)   4    238   59.53   2.891 0.0222 *\nResiduals            390   8032   20.59                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThis works with most analysis functions including t.test() and aov().\nNOTE: You only have to do this for variables in a dataset that are categories labeled as numbers. If the categories are text, t.test() and aov() automatically recognizes the variable as categorical. However, it does no harm to put a column with text into a factor() statement."
  },
  {
    "objectID": "5-Statistical_Tests_Part1/09-AA_Unit4_Review.html#cleaning-the-data",
    "href": "5-Statistical_Tests_Part1/09-AA_Unit4_Review.html#cleaning-the-data",
    "title": "Comparing Means",
    "section": "Cleaning the Data",
    "text": "Cleaning the Data\nWhile exploring the data, you may have noticed a few students ended up with a final grade of zero. While it may be interesting to explore what factors lead to an incomplete grade, we want to make conclusions about students who completed the course.\nCreate a clean dataset called, clean, that excludes zeros for G3. This will be used for the following analyses."
  },
  {
    "objectID": "5-Statistical_Tests_Part1/09-AA_Unit4_Review.html#comparing-schools",
    "href": "5-Statistical_Tests_Part1/09-AA_Unit4_Review.html#comparing-schools",
    "title": "Comparing Means",
    "section": "Comparing Schools",
    "text": "Comparing Schools\nSuppose the Gabriel Pereira school (GP) has more stringent admissions requirements. We suspect this would lead to higher grades, on average.\nCreate a side-by-side boxplot of the final grades for each school. Change the y-axis label to read “Final Grade out of 20”, the x-axis label to read “School”, and add a title.\nQuestion: What do you notice?\nAnswer:\nCreate a table of summary statistics of final grade for each school:\n\nHypothesis Test\nCreate a qqPlot to look at the normality of both groups:\nQuestion: Do the grades look normally distributed for both groups? If not, should we be concerned?\nAnswer:\nQuestion: Can we trust the P-value?\nAnswer:\nState your null and alternative hypotheses and significance level.\nNOTE: Recall that R uses alphabetical order to determine which group is the reference group. It is useful to put this group on the left side of the null hypothesis and set your alternative hypothesis accordingly.\n\\[H_o: \\]\n\\[H_a: \\]\n\\[\\alpha = 0.\\]\nPerform the appropriate statistical test:\nQuestion: What is the P-value?\nAnswer:\nQuestion: What is your conclusion in context of the research question?\nAnswer:\n\n\nConfidence Interval\nCreate a \\((1-\\alpha)\\)% confidence interval and explain it in context of the research question.\nExplanation:"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/09-AA_Unit4_Review.html#comparing-second-period-grade-with-final-grade",
    "href": "5-Statistical_Tests_Part1/09-AA_Unit4_Review.html#comparing-second-period-grade-with-final-grade",
    "title": "Comparing Means",
    "section": "Comparing Second Period Grade with Final Grade",
    "text": "Comparing Second Period Grade with Final Grade\nWe suspect there is a difference between the second period and the final grade, though we do not know if they go up or down. Carry out a hypothesis test to evaluate this suspicion.\n\nHypothesis Test\nChoose how you will define the difference between final grade and second period grade, and create a new object called diff:\n\ndiff &lt;- \n\nError in parse(text = input): &lt;text&gt;:2:0: unexpected end of input\n1: diff &lt;- \n   ^\n\n\nQuestion: What does a negative number mean?\nAnswer:\nCreate a qqPlot() of diff and check for normality:\nQuestion: Do the grade differences look normally distributed? If not, should we be concerned?\nAnswer:\nQuestion: Can we trust the P-value?\nAnswer:\nState your null and alternative hypothesis and choose a significance level:\n\\[H_o: \\]\n\\[Ha:  \\]\n\\[\\alpha = 0.\\]\nPerform the appropriate analysis.\nQuestion: What is the P-value?\nAnswer:\nQuestion: What conclusion do you make in context of this research question?\nAnswer:\n\n\nConfidence Interval\nCreate a \\((1-\\alpha)\\)% confidence interval for the differences and explain it in context of the research question.\nExplanation:"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/09-AA_Unit4_Review.html#absenteeism-in-portugal",
    "href": "5-Statistical_Tests_Part1/09-AA_Unit4_Review.html#absenteeism-in-portugal",
    "title": "Comparing Means",
    "section": "Absenteeism in Portugal",
    "text": "Absenteeism in Portugal\nIn 2021, Portugal reported having 0% absenteeism for 15-year-olds. We suspect that the actual absenteeism is higher than the reported value (zero).\nCreate a dataset that includes only students who are 15 years old.\n\nfifteen &lt;- \n\nError in parse(text = input): &lt;text&gt;:2:0: unexpected end of input\n1: fifteen &lt;- \n   ^\n\n\n\nHypothesis Test\nCreate a qqPlot() for absences.\nQuestion: Do absences look normally distributed? If not, should we be concerned?\nAnswer:\nQuestion: Can we trust the P-value?\nAnswer:\nState your null and alternative hypotheses and choose a significance level:\n\\[H_o: \\]\n\\[H_a: \\]\n\\[\\alpha = 0.\\]\nPerform the appropriate analysis.\nQuestion: What is the P-value?\nAnswer:\nQuestion: What conclusion do you make in context of this research question?\nAnswer:\n\n\nConfidence Interval\nCreate a \\((1-\\alpha)\\)% confidence interval for average absences and interpret it in context of the problem.\nExplanation:"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/09-AA_Unit4_Review.html#the-impact-of-mothers-education-level",
    "href": "5-Statistical_Tests_Part1/09-AA_Unit4_Review.html#the-impact-of-mothers-education-level",
    "title": "Comparing Means",
    "section": "The Impact of Mother’s Education Level",
    "text": "The Impact of Mother’s Education Level\nThe level of education of the mother in the home is thought to have a significant impact on student success.\nCreate a side-by-side boxplot of final grades for each level of mother’s education.\nCreate a table of summary statistics of final grades for each level of mother’s education.\nQuestion: How many respondents have a mother with no formal education (level 0)?\nAnswer:\nCreate a new dataset, clean_medu, that does not include mother’s education level 0.\n\nclean_medu &lt;- clean %&gt;% \n\nError in parse(text = input): &lt;text&gt;:4:0: unexpected end of input\n2: clean_medu &lt;- clean %&gt;% \n3: \n  ^\n\n\nCreate another boxplot with the new dataset that excludes level 0.\nCreate a summary table of final grades for each level of a mother’s education with the new dataset.\nQuestion: What is the maximum standard deviation?\nAnswer:\nQuestion: What is the minimum standard deviation?\nAnswer:\nQuestion: Verify that the maximum is less than twice as large as the minimum to check the “equality of standard deviations”.\nAnswer:\n\nHypothesis Test\nState your null and alternative hypotheses and pick alpha:\n\\[H_o: \\]\n\\[H_a: \\]\n\\[\\alpha = 0.\\]\nPerform the appropriate statistical test.\nQuestion: What is the test statistic?\nAnswer:\nQuestion: What is the P-value?\nAnswer:\nCheck the normality of the residuals.\nQuestion: Do the residuals appear roughly normally distributed?\nAnswer:\nQuestion: Can we trust the P-value.\nAnswer:\nState your conclusion."
  },
  {
    "objectID": "5-Statistical_Tests_Part1/09-AA_Unit4_Review.html#choose-your-own-adventure",
    "href": "5-Statistical_Tests_Part1/09-AA_Unit4_Review.html#choose-your-own-adventure",
    "title": "Comparing Means",
    "section": "Choose your own adventure",
    "text": "Choose your own adventure\nPick another variable that was not analyzed above.\nCreate a side-by-side boxplot. Be sure to properly label the graph and add sufficient information so readers can know what they are looking at without having to search through the report or code.\nPerform the appropriate analysis. Be sure to include a concise conclusion in the context of the research question, including a hypothesis test (and confidence interval if applicable.)"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/07-ANOVA_Intro.html",
    "href": "5-Statistical_Tests_Part1/07-ANOVA_Intro.html",
    "title": "Analysis of Variance (ANOVA)",
    "section": "",
    "text": "When we want to compare 3 or more groups, the math get’s more complicated. Analysis of Variance (ANOVA) compares how spread out the means are relative to the within group variation."
  },
  {
    "objectID": "5-Statistical_Tests_Part1/07-ANOVA_Intro.html#test-statistic",
    "href": "5-Statistical_Tests_Part1/07-ANOVA_Intro.html#test-statistic",
    "title": "Analysis of Variance (ANOVA)",
    "section": "Test Statistic",
    "text": "Test Statistic\nThe Test Statistic for comparing more than 2 groups is called \\(F\\). Just like the \\(z\\) and the \\(t\\) statistics, \\(F\\) has a probability distribution.\nThe formula for the new test statistic, \\(F\\), is messy, but we can get a sense for what it’s doing visually.\n\nThe test statistic is the ratio of the variation between groups and the average variation within groups. The further spread out the sample means are relative to the noise within the groups, the more significant the result."
  },
  {
    "objectID": "5-Statistical_Tests_Part1/07-ANOVA_Intro.html#the-f-distribution",
    "href": "5-Statistical_Tests_Part1/07-ANOVA_Intro.html#the-f-distribution",
    "title": "Analysis of Variance (ANOVA)",
    "section": "The F-Distribution",
    "text": "The F-Distribution\nRecall that the shape of the \\(t\\)-distribution depended on how much data was in the sample. The t-distribution was fatter tailed than the standard normal distribution when n was small. The F-statistic also changes shape. Its shape depends on how many data points are in the sample and how many groups we are comparing. This means the F-distribution has 2 sets of degrees of freedom.\nThe numerator, or between groups degrees of freedom is \\(df_{between}=k-1\\), where k is the number of groups you are comparing.\nThe denominator, or within groups degrees of freedom is \\(df_{within}=n-k\\) where n is the total number of data points and k is the number of groups.\nWe will get the values of both degrees of freedom directly from the R output.\nUnlike the \\(t\\)-distribution, the \\(F\\)-distribution is not centered around zero and can never be negative because \\(F\\) is the ratio of 2 positive numbers and is, therefore, always positive.\nTo summarize:\n\n\\(F\\) is always positive because it is the ratio of 2 positive numbers\n\\(F\\) is always right skewed\n\\(F\\) changes shape depending on the number of groups (numerator degrees of freedom) and the number of total data points (denominator degrees of freedom)\n\n\nThe P-value for an F-statistic is always one-tailed. The probability of observing a test statistic, \\(F\\), if the null hypothesis is true can be visualized:\n\nIn practice, the computer calculates the test statistic, P-value, and degrees of freedom and we interpret the output as with other statistical tests."
  },
  {
    "objectID": "5-Statistical_Tests_Part1/07-ANOVA_Intro.html#test-requirements",
    "href": "5-Statistical_Tests_Part1/07-ANOVA_Intro.html#test-requirements",
    "title": "Analysis of Variance (ANOVA)",
    "section": "Test Requirements",
    "text": "Test Requirements\nJust as with other statistical tests we’ve done so far, the \\(F\\)-test has certain requirements we must check to validate our P-values. Because of the way we calculate F, we are less concerned with the normality of the individual groups as we are with the variation within the groups.\nWhat to check:\n\nAre the standard deviations of each group “equal”?\nAre the residuals normally distributed?\n\n\nTest Equal Variances\nTo check the first requirement we can compare the biggest standard deviation to the smallest. If the ratio of the biggest to the smallest is less than 2, we conclude that the population standard deviations are “equal”.\nNOTE: Intuitively, this means that if the biggest standard deviation is more than twice as big as the smallest, then we might have cause for concern.\nThis can be checked using the standard deviations from the favstats() output (see Analysis in R below)\n\n\nNormality of Residuals\nResiduals are defined as the difference between an observation and it’s “expected” value:\n\\[residual = \\text{observed}-\\text{expected}\\]\nIn the case of ANOVA, that means we get a residual for every observation by taking the observed value and subtracting its group mean. Once again, R will provide this for us.\nFor our analysis to be valid, we need to see if the residuals are normally distributed. We can do a qqPlot() of the residuals to assess this requirement.\nIf both requirements are met, we can trust the P-value."
  },
  {
    "objectID": "5-Statistical_Tests_Part1/07-ANOVA_Intro.html#analysis-in-r",
    "href": "5-Statistical_Tests_Part1/07-ANOVA_Intro.html#analysis-in-r",
    "title": "Analysis of Variance (ANOVA)",
    "section": "Analysis in R",
    "text": "Analysis in R\nMuch of the syntax for ANOVA will look familiar, but we will be using a new function, aov() instead of t.test().\nThe aov() function by itself isn’t as useful as t.test(). However, we can use the summary() function to give us everything we need.\nWe typically name our output using the assignment operator &lt;- to make it easier to get the information we would like.\nThe inside of aov() will look familiar, using the same ~ notation we’ve used all semester.\nThe generic process for performing an ANOVA is:\n\n# Name the ANOVA output:\noutput &lt;- aov(data$response_variable ~ data$categorical_variable)\n\n# Summarise the ANOVA output to get test statistics, DF, P-value, etc:\nsummary(output)\n\n\nCheck ANOVA Requirments:\nJust as with the other statistical tests, ANOVA has certain requirements for us to be able to trust the P-value.\n\nChecking Equal Standard Deviations:\nWe can use favstats() to extract the standard deviations of each group, then find the ratio of the max/min to see if it is less than 2.\n# extract only the standard deviations from favstats output using th `$`:\n\nsds &lt;- favstats(data$response_variable ~ data$categorical_variable)$sd\n\n# Compare the max/min to 2\n\nmax(sds) / min(sds)\n\n# if max/min &lt; 2, then we're ok\n\n\n\nAssessing Normality of the Residuals\nWe can assess normality of the residuals with a qqPlot(). We first need to extract the residuals from our output:\n# Name the output of the aov() function `output`:\n\noutput &lt;- aov(data$response_variable ~ data$categorical_variable)\n\n# do a qqPlot of the residuals (observation values - group mean) that R calculates.  We can use th `$` to tell R what part of the output we want to make into a qqPlot():  \n\nqqPlot(output$residuals)\n\nIf most of the points fall within the blue zone, we can be confident that the residuals are normally distributed."
  },
  {
    "objectID": "5-Statistical_Tests_Part1/07-ANOVA_Intro.html#step-1-read-in-data",
    "href": "5-Statistical_Tests_Part1/07-ANOVA_Intro.html#step-1-read-in-data",
    "title": "Analysis of Variance (ANOVA)",
    "section": "Step 1: Read in data",
    "text": "Step 1: Read in data\nFor this demonstration we will be exploring the iris data. This dataset is built in to base R libraries, so we can access it without reading it in using “import”."
  },
  {
    "objectID": "5-Statistical_Tests_Part1/07-ANOVA_Intro.html#step-2-review-the-data",
    "href": "5-Statistical_Tests_Part1/07-ANOVA_Intro.html#step-2-review-the-data",
    "title": "Analysis of Variance (ANOVA)",
    "section": "Step 2: Review the data",
    "text": "Step 2: Review the data\nThe iris data contains multiple measures on flowers that might be of interest to compare across species.\nLet’s first compare sepal lengths between species."
  },
  {
    "objectID": "5-Statistical_Tests_Part1/07-ANOVA_Intro.html#step-2-explore-the-data",
    "href": "5-Statistical_Tests_Part1/07-ANOVA_Intro.html#step-2-explore-the-data",
    "title": "Analysis of Variance (ANOVA)",
    "section": "Step 2: Explore the Data",
    "text": "Step 2: Explore the Data\nHow many species do we have in our dataset?\n\ntable(iris$Species)\n\n\n    setosa versicolor  virginica \n        50         50         50 \n\n\nCreate a side-by-side boxplot of species and Sepal Length.\n\nboxplot(Sepal.Length ~ Species, data=iris, col = c(2,3,4), ylab=\"Sepal Length\", main = \"Sepal Length by Species\")\n\n\n\n\n\n\n\n\nUsing GGPlot:\n\nggplot(iris, aes(x=Species, y = Sepal.Length)) +\n  geom_boxplot(fill=c(2,3,4)) +\n  theme_bw() +\n  labs(\n    y = \"Sepal Length\",\n    title = \"Sepal Length by Species\"\n  )"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/07-ANOVA_Intro.html#step-4-perform-the-appropriate-analysis",
    "href": "5-Statistical_Tests_Part1/07-ANOVA_Intro.html#step-4-perform-the-appropriate-analysis",
    "title": "Analysis of Variance (ANOVA)",
    "section": "Step 4: Perform the appropriate analysis",
    "text": "Step 4: Perform the appropriate analysis\n\naov_sepal &lt;- aov(Sepal.Length ~ Species, data=iris)\nsummary(aov_sepal)\n\n             Df Sum Sq Mean Sq F value Pr(&gt;F)    \nSpecies       2  63.21  31.606   119.3 &lt;2e-16 ***\nResiduals   147  38.96   0.265                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nBefore we make a conclusion, we want to check that we can trust our output. Every statistical test has requirements that must be satisfied if we are to trust our conclusions. For ANOVA, we need to check the normality and that the variation within groups is roughly the same.\nWe use a QQ-plot to check for normality and the ratio of the largest to the smallest standard deviation to check “equal” variation.\n\n# QQ plots show how closely the the residuals are to a normal distribution\n\nqqPlot(aov_sepal$residuals)\n\n\n\n\n\n\n\n\n[1] 107 132\n\n# Check that there is less than a 2X difference between the largest and smallest standard deviations\n\n# We can assign favstats()$sd to a variable to make it easier to use. Recall the \"$\" can also be used to extract specific output from functions\n\nsds &lt;- favstats(Sepal.Length ~ Species, data=iris)$sd\n\nmax(sds) / min(sds)\n\n[1] 1.803967\n\n\nQuestion: What is the F-statistics?\nAnswer: 119.3\nQuestion: What are the between-groups degrees of freedom?\nAnswer: 2\nQuestion: What are the within-groups degrees of freedom?\nAnswer: 147\nQuestion: What is the P-value?\nAnswer: &lt;2e-16\nQuestion: What is your conclusion?\nAnswer: Because p is less than alpha, we reject the null hypothesis. We conclude that at least one of the means of Sepal Length is different from the other group means."
  },
  {
    "objectID": "5-Statistical_Tests_Part1/05-Independent_2_sample_ttest.html",
    "href": "5-Statistical_Tests_Part1/05-Independent_2_sample_ttest.html",
    "title": "Independent Two-sample T-test",
    "section": "",
    "text": "In many situations we would like to compare averages from different populations. In these situations, we take 2 random samples from each population and perform statistical tests to determine if the population means are significantly different. Because these two groups of individuals are sampled independently, we call this analysis Independent 2-Sample t-test.\nAlternatively, in many experimental designs, participants are randomly assigned into a treatment and a control group. The randomization process ensures that there is no association between participants in either group. They are independent.\nWhen 2 random samples are taken from 2 separate populations, or when a group of people are randomly assigned into treatment groups, the samples are independent.\nSome examples include:\n\nComparing salaries of men and women (randomly sample men and women separately)\nTesting a new medication compared to a placebo (participants randomly assigned to treatment groups)\nComparing average GPA of Math majors and Economics majors (randomly select from each population)"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/05-Independent_2_sample_ttest.html#independent-t-test-in-r",
    "href": "5-Statistical_Tests_Part1/05-Independent_2_sample_ttest.html#independent-t-test-in-r",
    "title": "Independent Two-sample T-test",
    "section": "Independent t-test in R",
    "text": "Independent t-test in R\nA 2-sample independent t-test in R requires a slight modification to the 1-sample t-test previously discussed. However, the syntax should look familiar.\nRecall that when we created a boxplot() or did favstats() for one set of data it looked like:\nboxplot(data$response_variable)\nfavstats(data$response_variable)\n\nwith data$response_variable corresponding to our quantitative variable of interest.\nWhen we wanted to break the analysis down by a grouping factor we used the ~ notation to add a group variable:\nboxplot(data$response_variable ~ data$grouping_variable)\nfavstats(data$response_variable ~ data$grouping_variable)\nWe use the exact same modification for a t-test with 2 groups:\nt.test(data$response_variable ~ data$grouping_variable, alterntive = \"greater\")\nThe t.test() function uses mu=0 as a default, we do not need to specify it in the function because when comparing 2 groups, the difference is assumed to be 0 in most cases.\nWhen specifying an alternative hypothesis in R, a reference group must be defined. This serves as the baseline for comparison. R will assess how the mean of the second group differs from that of the first.\nBy default, R decides which group is the reference group alphabetically. Whichever category label is first alphabetically is the reference group.\nNOTE: In R, group 1 and 2 are determined alphabetically according to the labels in the explanatory variable column.\nEXAMPLE: If my explanatory variable is sex, and they are labeled “Female” and “Male”, then “Female” would be the reference group. If I suspect that Females had smaller feet than males I would use alternative = \"less\"."
  },
  {
    "objectID": "5-Statistical_Tests_Part1/05-Independent_2_sample_ttest.html#checking-requirements",
    "href": "5-Statistical_Tests_Part1/05-Independent_2_sample_ttest.html#checking-requirements",
    "title": "Independent Two-sample T-test",
    "section": "Checking Requirements",
    "text": "Checking Requirements\nWhen we looked at 1-sample t-tests, we had to check that the sample mean was normally distributed. Recall that the sample mean is approximately normal if:\n\nThe source population is normally distributed\nThe sample size, \\(n \\ge 30\\)\n\nFor a 2-sample t-test, we must check that this is true for both samples. If both sample sizes are bigger than 30 then we can trust the CLT and the statistics based on it.\nIf the sample sizes are small, we can make a qqPlot() for both groups:\nqqPlot(data$response_variable ~ data$explanatory_variable)"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/05-Independent_2_sample_ttest.html#confidence-intervals",
    "href": "5-Statistical_Tests_Part1/05-Independent_2_sample_ttest.html#confidence-intervals",
    "title": "Independent Two-sample T-test",
    "section": "Confidence Intervals",
    "text": "Confidence Intervals\nRecall that confidence intervals are necessarily two-sided. So the code for a 99% confidence interval looks like:\nt.test(data$response_variable ~ data$grouping_variable, conf.level = .99)$conf.int\nWe interpret a confidence interval for the difference of means as follows:\n\nI am 99% confident that the true difference of the means is between [lower limit] and [upper limit].\n\nWe can usually do better within the context of a research question:\n\nClass 1 did, on average, between 3.21 and 5.67 percent better than class 2 on the last exam.\n\nStore A outperforms Store B by between $27,022 and $36,977 on average"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/05-Independent_2_sample_ttest.html#step-1-read-in-data",
    "href": "5-Statistical_Tests_Part1/05-Independent_2_sample_ttest.html#step-1-read-in-data",
    "title": "Independent Two-sample T-test",
    "section": "Step 1: Read in Data",
    "text": "Step 1: Read in Data\n\n# Load Libraries\n\nlibrary(tidyverse)\nlibrary(mosaic)\nlibrary(rio)\nlibrary(car)\n\n\n# Load Data\n\nfifa_heart_attacks &lt;- import(\"https://byuistats.github.io/M221R/Data/fifa_heart_attacks.xlsx\")"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/05-Independent_2_sample_ttest.html#step-2-review-data",
    "href": "5-Statistical_Tests_Part1/05-Independent_2_sample_ttest.html#step-2-review-data",
    "title": "Independent Two-sample T-test",
    "section": "Step 2: Review Data",
    "text": "Step 2: Review Data\nLook at the data.\nCreate summary statistics tables of the number of heart attacks for each group.\nCreate a side-by-side boxplot for the during the World Cup and the Control.\nDo you notice any outliers or data that may need to be omitted for analysis?\nCheck to see if the means from both groups are normally distributed:\n\nIs n &gt; 30 for both groups?\nCreate a qqPlot()\n\n\nqqPlot(fifa_heart_attacks$heart_attacks ~ fifa_heart_attacks$time_period)\n\n\n\n\n\n\n\n\nCan we trust that the central limit theorem applies?"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/05-Independent_2_sample_ttest.html#step-3-prepare-data-for-analysis",
    "href": "5-Statistical_Tests_Part1/05-Independent_2_sample_ttest.html#step-3-prepare-data-for-analysis",
    "title": "Independent Two-sample T-test",
    "section": "Step 3: Prepare Data for Analysis",
    "text": "Step 3: Prepare Data for Analysis\nThese data look ready for analysis."
  },
  {
    "objectID": "5-Statistical_Tests_Part1/05-Independent_2_sample_ttest.html#step-4-perform-the-appropriate-analysis",
    "href": "5-Statistical_Tests_Part1/05-Independent_2_sample_ttest.html#step-4-perform-the-appropriate-analysis",
    "title": "Independent Two-sample T-test",
    "section": "Step 4: Perform the appropriate analysis",
    "text": "Step 4: Perform the appropriate analysis\n\nHypothesis Test\nAre the individuals in each group dependent or independent of each other?\nWrite out your null and alternative hypotheses.\nHo: Ha:\nWhich group is considered group 1 and which is group 2 in R?\nCheck the alphabetical order:\n\nunique(fifa_heart_attacks$time_period)\n\n[1] \"Control\"   \"World Cup\"\n\n\nPerform the appropriate t-test.\nWhat is your test statistic?\nWhat is your p-value?\nState your conclusion:\n\n\n97% Confidence interval\nCalculate the 97% confidence interval for the difference of the means.\nIn context of the research question, interpret the confidence interval."
  },
  {
    "objectID": "5-Statistical_Tests_Part1/05-Independent_2_sample_ttest.html#new-zealand-rugby",
    "href": "5-Statistical_Tests_Part1/05-Independent_2_sample_ttest.html#new-zealand-rugby",
    "title": "Independent Two-sample T-test",
    "section": "New Zealand Rugby",
    "text": "New Zealand Rugby\nRugby is a popular sport in the United Kingdom, France, Australia, New Zealand and South Africa. It is gaining popularity in the US, Canada, Japan and parts of Europe. Some of the rules of the game have recently been changed to make play more exciting. In a study to examine the effects of the rule changes, Hollings and Triggs (1993) collected data on some recent games.\nTypically, a game consists of bursts of activity that terminate when points are scored, if the ball is moved out of the field of play or if a violation of the rules occurs. In 1992, the investigators gathered data on ten international matches which involved the New Zealand national team, the All Blacks. The first five games were the last international games played under the old rules, and the second set of five were the first internationals played under the new rules.\nFor each of the ten games, the data give the successive times (in seconds) of each passage of play in that game.\nYou will investigate whether the mean duration of the passages has dropped under the new rules.\nUse a level of significance of 0.01.\n\nLoad the Data\n\nrugby &lt;- import(\"https://byuistats.github.io/M221R/Data/quiz/R/nz_rugby.csv\")\n\n\n\nExplore the Data\nCreate a side-by-side boxplot for the amount of reported passage of play before and after the rule changes.\nAdd a title and change the colors of the boxes.\nWhat do you observe?\nCreate a table of summary statistics of play time for before and after the rule change. (favstats()):\n\n\nPerform the Appropriate Analysis\n\nHypothesis Test\nState your null and alternative hypotheses:\nNOTE: The default for R is to set group order alphabetically. This means Group 1 = NewRules\nCompare the the time per play under the new and old rules:\n\nqqPlot(rugby$time, groups = rugby$period)\n\n\n\n\n\n\n\n\nDo the data for each group appear normally distributed?\nWhy is it OK to continue with the analysis?\nPerform a t-test.\nWhat is the value of the test statistic?\nHow many degrees of freedom for this test?\nWhat is the p-value?\nWhat do you conclude?\n\n\nConfidence Interval\nCreate a confidence interval for the difference of the average Importance Score between both groups:"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/05-Independent_2_sample_ttest.html#step-1-read-in-the-data",
    "href": "5-Statistical_Tests_Part1/05-Independent_2_sample_ttest.html#step-1-read-in-the-data",
    "title": "Independent Two-sample T-test",
    "section": "Step 1: Read in the data",
    "text": "Step 1: Read in the data\n\ncopd &lt;- import(\"https://byuistats.github.io/M221R/Data/copd_rehab.xlsx\") %&gt;% pivot_longer(cols=c(\"community\", \"hospital\"), names_to = \"Treatment\", values_to = \"Steps\") %&gt;% select(Treatment, Steps) %&gt;% arrange(Treatment)"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/05-Independent_2_sample_ttest.html#step-2-review-the-data",
    "href": "5-Statistical_Tests_Part1/05-Independent_2_sample_ttest.html#step-2-review-the-data",
    "title": "Independent Two-sample T-test",
    "section": "Step 2: Review the data",
    "text": "Step 2: Review the data\nCreate side-by-side boxplots and summary statistics for the community and hospital groups:\nCheck to see if the means are expected to be normally distributed.\nCan trust the CLT for our test statistic and P-value?"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/05-Independent_2_sample_ttest.html#step-3-prepare-data-for-analysis-1",
    "href": "5-Statistical_Tests_Part1/05-Independent_2_sample_ttest.html#step-3-prepare-data-for-analysis-1",
    "title": "Independent Two-sample T-test",
    "section": "Step 3: Prepare Data for Analysis",
    "text": "Step 3: Prepare Data for Analysis\nThe data cleansing has been performed for you. You’re welcome."
  },
  {
    "objectID": "5-Statistical_Tests_Part1/05-Independent_2_sample_ttest.html#step-4-perform-the-appropriate-analysis-1",
    "href": "5-Statistical_Tests_Part1/05-Independent_2_sample_ttest.html#step-4-perform-the-appropriate-analysis-1",
    "title": "Independent Two-sample T-test",
    "section": "Step 4: Perform the appropriate analysis",
    "text": "Step 4: Perform the appropriate analysis\n\nHypothesis Test\nState your null and alternative hypotheses.\nHo:\nHa:\nWhich group is considered group 1 in this data?\nRun the appropriate t-test.\n\n#t.test()\n\nState your conclusion about the hypothesis test.\n\n\nConfidence Interval\nCreate a 95% confidence interval for the difference between the means\nInterpret the 95% confidence interval for the mean difference between the community-based and hospital-based groups."
  },
  {
    "objectID": "5-Statistical_Tests_Part1/03-Paired_ttest.html",
    "href": "5-Statistical_Tests_Part1/03-Paired_ttest.html",
    "title": "Paired T-test Class Notes",
    "section": "",
    "text": "A matched pairs design is used in statistics to compare 2 treatments or conditions measured on subjects that are logically connected in a meaningful way. In the simplest case, measurements are collected on the same subject as in a before-and-after evaluation.\nMatched pairs designs are often called “dependent samples” because knowing who or what is in the first treatment group determines who will be in the second. In the case of a before-and-after situation, if you are selected to be in the “pre” group, then you will also be in the “post” group. But pairs are not always the same subjects.\n\n\n\nYou want to study the difference in salaries between husbands and wives. If one spouse is selected for the study it automatically determines that the other spouse will be in the other group.\nAn ACT preparation course gives you a test before you take the course and after to see if the course improved test score\nA weight loss program takes your weight at the beginning and after the 12 weeks in the program to see if the program reduced weight.\nComparing prices of a specific set of items between Walmart and Broulims. Because we are comparing the same items, we can take the difference between prices for each item.\n\n\n\n\nAs with the one-sample t-test, we have to make sure that the either the pairs are normally distributed or we have a large enough sample size.\nWith smaller sample sizes, create a qqPlot() using the car library to check for normality.\n\n\n\nWhen we perform a matched pairs analysis, we will be doing a 1-sample t-test on the differences between the connected observations. One challenge is that we can define the difference either way: before - after or after - before.\nA good practice is to define differences so that a negative number means “loss” and a positive number means “gain”.\nFor example, if we believe our weight loss program reduces weight, then defining post_weight - pre_weight should give a negative number, meaning weight lost during the program.\nIf you believe Walmart is cheaper than Broulims, defining the difference Broulims - Walmart gives a positive number, meaning how much you can save, on average, for shopping at Walmart.\nMathematically, it doesn’t matter which way we define the difference as long as we keep track of what a negative number and a positive number mean. This will define which alternative hypothesis we use."
  },
  {
    "objectID": "5-Statistical_Tests_Part1/03-Paired_ttest.html#examples",
    "href": "5-Statistical_Tests_Part1/03-Paired_ttest.html#examples",
    "title": "Paired T-test Class Notes",
    "section": "",
    "text": "You want to study the difference in salaries between husbands and wives. If one spouse is selected for the study it automatically determines that the other spouse will be in the other group.\nAn ACT preparation course gives you a test before you take the course and after to see if the course improved test score\nA weight loss program takes your weight at the beginning and after the 12 weeks in the program to see if the program reduced weight.\nComparing prices of a specific set of items between Walmart and Broulims. Because we are comparing the same items, we can take the difference between prices for each item."
  },
  {
    "objectID": "5-Statistical_Tests_Part1/03-Paired_ttest.html#requirements-for-a-matched-pairs-analysis",
    "href": "5-Statistical_Tests_Part1/03-Paired_ttest.html#requirements-for-a-matched-pairs-analysis",
    "title": "Paired T-test Class Notes",
    "section": "",
    "text": "As with the one-sample t-test, we have to make sure that the either the pairs are normally distributed or we have a large enough sample size.\nWith smaller sample sizes, create a qqPlot() using the car library to check for normality."
  },
  {
    "objectID": "5-Statistical_Tests_Part1/03-Paired_ttest.html#watchouts",
    "href": "5-Statistical_Tests_Part1/03-Paired_ttest.html#watchouts",
    "title": "Paired T-test Class Notes",
    "section": "",
    "text": "When we perform a matched pairs analysis, we will be doing a 1-sample t-test on the differences between the connected observations. One challenge is that we can define the difference either way: before - after or after - before.\nA good practice is to define differences so that a negative number means “loss” and a positive number means “gain”.\nFor example, if we believe our weight loss program reduces weight, then defining post_weight - pre_weight should give a negative number, meaning weight lost during the program.\nIf you believe Walmart is cheaper than Broulims, defining the difference Broulims - Walmart gives a positive number, meaning how much you can save, on average, for shopping at Walmart.\nMathematically, it doesn’t matter which way we define the difference as long as we keep track of what a negative number and a positive number mean. This will define which alternative hypothesis we use."
  },
  {
    "objectID": "5-Statistical_Tests_Part1/03-Paired_ttest.html#step-1-read-in-data",
    "href": "5-Statistical_Tests_Part1/03-Paired_ttest.html#step-1-read-in-data",
    "title": "Paired T-test Class Notes",
    "section": "Step 1: Read in Data",
    "text": "Step 1: Read in Data\n\n# Load Libraries\n\nlibrary(tidyverse)\nlibrary(mosaic)\nlibrary(rio)\nlibrary(car)\n\n\n# Load Data\nweight_loss &lt;- import(\"https://byuistats.github.io/M221R/Data/weight_loss.xlsx\")"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/03-Paired_ttest.html#step-2-explore-the-data-and-generate-hypotheses",
    "href": "5-Statistical_Tests_Part1/03-Paired_ttest.html#step-2-explore-the-data-and-generate-hypotheses",
    "title": "Paired T-test Class Notes",
    "section": "Step 2: Explore the Data and Generate Hypotheses",
    "text": "Step 2: Explore the Data and Generate Hypotheses\nCreate histograms summary statistics for the pre and post weight measurements:\n\nglimpse(weight_loss)\n\nRows: 27\nColumns: 4\n$ subject  &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18…\n$ pre      &lt;dbl&gt; 62.5, 88.8, 74.7, 98.6, 78.1, 73.8, 87.0, 62.1, 60.7, 76.2, 8…\n$ post     &lt;dbl&gt; 56.1, 80.2, 70.8, 97.0, 69.1, 63.8, 81.9, 53.3, 56.5, 71.8, 7…\n$ comments &lt;chr&gt; \"Data collected by Annie Mahon in a\", \"weight loss study. Sub…\n\n# Pre-weight histogram\nhistogram(weight_loss$pre)\n\n\n\n\n\n\n\nfavstats(weight_loss$pre)\n\n  min    Q1 median   Q3  max mean       sd  n missing\n 60.7 72.75     77 80.7 99.6 76.9 10.20671 27       0\n\n# Post-weight histogram\nhistogram(weight_loss$post)\n\n\n\n\n\n\n\nfavstats(weight_loss$post)\n\n  min    Q1 median   Q3 max    mean       sd  n missing\n 53.3 63.45   69.9 75.2  97 70.0963 10.63273 27       0"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/03-Paired_ttest.html#step-3-prepare-the-data-for-analysis",
    "href": "5-Statistical_Tests_Part1/03-Paired_ttest.html#step-3-prepare-the-data-for-analysis",
    "title": "Paired T-test Class Notes",
    "section": "Step 3: Prepare the data for analysis",
    "text": "Step 3: Prepare the data for analysis\nDecide how you’re going to define the difference (\\(post - pre\\) or \\(pre - post\\)).\nQuestion: What does a negative number mean based on your definition?\nAnswer:\n\n# Decide which column to subtract from the other\ndiff &lt;- weight_loss$post - weight_loss$pre\n\nCreate a histogram and a qqPlot of the differences to determine if you will be able to trust the statsitical analyses:\n\n# histogram of the differences\nhistogram(diff)\n\n\n\n\n\n\n\nfavstats(diff)\n\n   min   Q1 median   Q3  max      mean       sd  n missing\n -13.6 -8.9   -6.7 -4.2 -1.6 -6.803704 3.172051 27       0\n\n# Check if the differences are normally distributed:\nqqPlot(diff)\n\n\n\n\n\n\n\n\n[1] 14 18"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/03-Paired_ttest.html#step-4-perform-the-appropriate-analysis",
    "href": "5-Statistical_Tests_Part1/03-Paired_ttest.html#step-4-perform-the-appropriate-analysis",
    "title": "Paired T-test Class Notes",
    "section": "Step 4: Perform the appropriate analysis",
    "text": "Step 4: Perform the appropriate analysis\n\nHypothesis Test\nState your null and alternative hypotheses.\n\\[ H_0: \\mu_{differences} = 0\\]\n\\[H_a:  \\mu_{differences} &lt; 0\\]\n\\[ \\alpha = 0.01\\]\nQuestions to consider:\n\nHow did you define your difference?\nBased on your decision for the difference, what does a negative number mean? a positive number?\nAre you expecting the difference to be greater than, less than, or not equal to 0?\n\nNOTE: One way to verify that you have used the correct alternative is to look at the difference as defined (weight_loss$post - weight_loss$pre) and decide which one you think is supposed to be bigger. Swap out the subtraction sign for the inequality that makes sense (weight_loss$post &lt; weight_loss$pre). Therefore, your alternative should be “less” because \\(&lt;\\) corresponds to “less than”.\nPerform a t-test of the differences:\n\n# Hypothesis t.test()\nt.test(diff, mu = 0, alternative = \"less\")\n\n\n    One Sample t-test\n\ndata:  diff\nt = -11.145, df = 26, p-value = 1.059e-11\nalternative hypothesis: true mean is less than 0\n95 percent confidence interval:\n     -Inf -5.76249\nsample estimates:\nmean of x \n-6.803704 \n\n\nState your conclusion in context of the research question:\nBecause P-value &lt; 0.01 we reject the null hypothesis. We have SUFFICIENT evidence to suggest that participation in the weight loss program led to weight loss.\n\n\nConfidence Interval\n\n# Confidence interval using t.test()\nt.test(diff, conf.level = .99)$conf.int\n\n[1] -8.500002 -5.107405\nattr(,\"conf.level\")\n[1] 0.99\n\n\nI am 99% confident that the true average weight loss during the program was between -8.5 and -5.11 kilograms."
  },
  {
    "objectID": "5-Statistical_Tests_Part1/03-Paired_ttest.html#step-1-read-in-data-1",
    "href": "5-Statistical_Tests_Part1/03-Paired_ttest.html#step-1-read-in-data-1",
    "title": "Paired T-test Class Notes",
    "section": "Step 1: Read in Data",
    "text": "Step 1: Read in Data\n\ncholesterol &lt;- import(\"https://byuistats.github.io/M221R/Data/quiz/R/cholesterol.csv\")"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/03-Paired_ttest.html#step-2-check-data",
    "href": "5-Statistical_Tests_Part1/03-Paired_ttest.html#step-2-check-data",
    "title": "Paired T-test Class Notes",
    "section": "Step 2: Check Data",
    "text": "Step 2: Check Data\nReview the data to see if the variables look correct. Calculate summary statistics and histograms for chol_day2 and chol_day4."
  },
  {
    "objectID": "5-Statistical_Tests_Part1/03-Paired_ttest.html#step-3-prepare-data-for-analysis",
    "href": "5-Statistical_Tests_Part1/03-Paired_ttest.html#step-3-prepare-data-for-analysis",
    "title": "Paired T-test Class Notes",
    "section": "Step 3: Prepare Data for Analysis",
    "text": "Step 3: Prepare Data for Analysis\nDecide how to define your difference. What does a negative number indicate? a positive number?\nCreate a histogram and summary statistics of the differences:\nBecause we \\(n &lt; 30\\) we need to check the qqPlot() to assess the normality of the differences."
  },
  {
    "objectID": "5-Statistical_Tests_Part1/03-Paired_ttest.html#step-4-perform-the-appropriate-analysis-1",
    "href": "5-Statistical_Tests_Part1/03-Paired_ttest.html#step-4-perform-the-appropriate-analysis-1",
    "title": "Paired T-test Class Notes",
    "section": "Step 4: Perform the appropriate analysis",
    "text": "Step 4: Perform the appropriate analysis\n\nHypothesis Test\nState your null and alternative hypotheses.\nWhat is your confidence level, ($1-= $)?\nPerform a matched pairs t-test for the difference in cholesterol at day 2 and day 4:\nState your conclusion:\n\n\nConfidence Interval\nCalculate a confidence interval for the average difference.\nState your conclusions and interpret the confidence interval in context of the research question."
  },
  {
    "objectID": "5-Statistical_Tests_Part1/01-Inference_for_mean_sigma_unknown.html",
    "href": "5-Statistical_Tests_Part1/01-Inference_for_mean_sigma_unknown.html",
    "title": "Inference for a Mean - Sigma Unknown",
    "section": "",
    "text": "By the end of this lesson, you should be able to:\n\nRecognize when a one mean (sigma unknown) inferential procedure is appropriate\n\nPerform a hypothesis test for one mean (sigma unknown) using the following steps:\n\nState the null and alternative hypotheses\n\nCalculate the test-statistic, degrees of freedom and P-value using R\n\nAssess statistical significance in order to state the conclusion for the hypothesis test in context of the research question\n\nCheck the requirements for the hypothesis test\n\n\nCreate a confidence interval for one mean (sigma unknown) using the following steps:\n\nCalculate a confidence interval for a given level of confidence using R\n\nInterpret the confidence interval\n\nCheck the requirements of the confidence interval\n\n\nState the properties of the Student’s t-distribution"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/01-Inference_for_mean_sigma_unknown.html#confidence-interval",
    "href": "5-Statistical_Tests_Part1/01-Inference_for_mean_sigma_unknown.html#confidence-interval",
    "title": "Inference for a Mean - Sigma Unknown",
    "section": "Confidence Interval",
    "text": "Confidence Interval\nWe can also calculate a confidence interval for the above example.\nRecall the formula for a confidence interval is, for a given \\(z^*\\) associated with a desired level of confidence:\n\\[ CI = \\bar{x} \\pm z^*\\frac{\\sigma}{\\sqrt{n}}\\]\nRecall the \\(z^*\\) for common confidence levels:\n\nlibrary(tidyverse)\nlibrary(pander)\ntibble(`Conf. Level` = c(0.99, 0.95, 0.90), `Z*` = c(2.576, 1.96, 1.645)) %&gt;% pander()\n\n\n\n\n\n\n\n\nConf. Level\nZ*\n\n\n\n\n0.99\n2.576\n\n\n0.95\n1.96\n\n\n0.9\n1.645"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/01-Inference_for_mean_sigma_unknown.html#calculating-p-values-by-hand-with-the-t-distribution-one-out-of-ten.-would-not-recommended",
    "href": "5-Statistical_Tests_Part1/01-Inference_for_mean_sigma_unknown.html#calculating-p-values-by-hand-with-the-t-distribution-one-out-of-ten.-would-not-recommended",
    "title": "Inference for a Mean - Sigma Unknown",
    "section": "Calculating P-values by hand with the T-distribution (One out of ten. Would NOT RECOMMENDED)",
    "text": "Calculating P-values by hand with the T-distribution (One out of ten. Would NOT RECOMMENDED)\nSuppose we have 25 test scores defined as “data” in the code chunk below. We can calculate the \\(t\\)-statistic just like we did with the \\(z\\)-score.\nSuppose we believe that the mean score of these 25 students is significantly higher than 50. Our null and alternative hypothesis are as follows:\n\\[ H_0:  \\mu = 50 \\] \\[ H_a: \\mu &gt; 50 \\]\n\n# we can use the t-distribution, pt(), just like pnorm() but must also add the degrees of freedom\n\ndata &lt;- c(88,81,27,92,46,79,67,44,46,88,21,60,71,81,79,52,100,44,42,58,52,48,83,65,98)\n\n# Hypothesized Mean:\nmu_0 &lt;- 50\n\n# Sample size, sample Mean and sample SD\n#  The length function tells us how many data points are in the list.  \nn &lt;- length(data)\nxbar &lt;- mean(data)\ns &lt;- sd(data)\n\ns_xbar = s / sqrt(n)\n\nt &lt;- (xbar - mu_0) / s_xbar\n\n# Probability of getting a test statistic at least as extreme as the one we observed if the null hypothesis is true\n1-pt(t, n-1)\n\n[1] 0.00151866\n\n\nThis means that if the true population mean was 50, then there is only a 0.00152 probability of observing a test statistic, t, as high as the one we got (P-value)."
  },
  {
    "objectID": "5-Statistical_Tests_Part1/01-Inference_for_mean_sigma_unknown.html#step-1-read-in-the-data",
    "href": "5-Statistical_Tests_Part1/01-Inference_for_mean_sigma_unknown.html#step-1-read-in-the-data",
    "title": "Inference for a Mean - Sigma Unknown",
    "section": "Step 1: Read in the data",
    "text": "Step 1: Read in the data\n\n# Load Libraries\n\nlibrary(tidyverse)\nlibrary(mosaic)\nlibrary(rio)\nlibrary(car)\n\n\n# Read in data\nbig5 &lt;- import('https://raw.githubusercontent.com/byuistats/Math221D_Cannon/master/Data/All_class_combined_personality_data.csv')"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/01-Inference_for_mean_sigma_unknown.html#step-2-review-the-data",
    "href": "5-Statistical_Tests_Part1/01-Inference_for_mean_sigma_unknown.html#step-2-review-the-data",
    "title": "Inference for a Mean - Sigma Unknown",
    "section": "Step 2: Review the Data",
    "text": "Step 2: Review the Data\nIn this step, we are looking to see if the data are as expected. Are the columns we’re interested in numeric? Categorical? and do these match expectations. We can also start to look for strange data and outliers. Visualizations can help.\nOther common issues to look for include: negative numbers that should only be positive, date values that shouldn’t exist, missing values, character variables inside what should be a number.\nIn the real world, data are messy. Reviewing the data is a critical part of an analysis.\nTake a look through the personality dataset and see if there are any anomalies that might need to be addressed."
  },
  {
    "objectID": "5-Statistical_Tests_Part1/01-Inference_for_mean_sigma_unknown.html#step-3-visulize-the-data",
    "href": "5-Statistical_Tests_Part1/01-Inference_for_mean_sigma_unknown.html#step-3-visulize-the-data",
    "title": "Inference for a Mean - Sigma Unknown",
    "section": "Step 3: Visulize the data",
    "text": "Step 3: Visulize the data\nSo far we have been discussing a single, quantitative variable of interest, like test scores, reaction times, heights, etc. When we start looking at more complicated data, we will expand our repertoire of visualizations, but Histograms are very good when looking at one variable at a time, and boxplots are very good for comparison between groups.\nCreate a histogram of Extroversion. Describe some features of the data. Is it symmetric? Skewed? Are there outliers?\n\nhistogram(big5$Extroversion, main = \"Extroversion Scores\", xlab = \"Extroversion\")\n\n\n\n\n\n\n\nggplot(big5, aes(x = Extroversion)) +\n  geom_histogram(bins = 15, fill=\"lightblue\") +\n  labs(\n    x = \"Extroversion\",\n    y = \"\",\n    title = \"Extroversion Scores\"\n  ) +\n  theme_bw()"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/01-Inference_for_mean_sigma_unknown.html#step-4-perform-the-appropriate-analysis",
    "href": "5-Statistical_Tests_Part1/01-Inference_for_mean_sigma_unknown.html#step-4-perform-the-appropriate-analysis",
    "title": "Inference for a Mean - Sigma Unknown",
    "section": "Step 4: Perform the appropriate analysis",
    "text": "Step 4: Perform the appropriate analysis\nIn this example, we will be testing the hypothesis that Brother Cannon’s students are similar to the general population. We suspect that these youthful BYU-I students are, on average, more extroverted.\n\nHypothesis Test\nWrite out the Null and alternative hypotheses.\n\\[ H_0: \\mu_{extroversion} = 50\\]\n\\[ H_A: \\mu_{extroversion}  &gt; 50\\]\n\\[\\alpha = 0.05\\]\nPerform the one-sample t-test using the “t.test()” function in R. If you ever get stuck remembering how to use a function in R, you can run ?t.test to see documentation. The question mark will open up the help files for any given function in R.\nThe t.test() function takes as input, the data, the hypothesized mean, mu, and the direction of the alternative hypothesis.\nThe default parameters for the t.test() function are: t.test(data, mu = 0, alternative = \"two.sided\").\n\n#?t.test\n\n# One-sided Hypothesis Test\nt.test(big5$Extroversion, mu = 50, alternative = \"greater\")\n\n\n    One Sample t-test\n\ndata:  big5$Extroversion\nt = 6.6529, df = 403, p-value = 4.697e-11\nalternative hypothesis: true mean is greater than 50\n95 percent confidence interval:\n 55.25232      Inf\nsample estimates:\nmean of x \n 56.98267 \n\n\nQuestion: What is the P-value?\nAnswer: p-value = 4.697e-11\nQUESTION: What is your conclusion based on \\(\\alpha=0.05\\)?\nANSWER: Because P-value &lt; 0.05 we reject the null hypothesis in favor of the alternative.\nQuestion: State your conclusion in context of our research question?\nAnswer: We have sufficient evidence to conclude that Brother Cannon’s students are more extroverted than the general population, on average.\n\n\nConfidence Intervals\nWe can also use the t.test() function to create confidence intervals. Recall that confidence intervals are always 2-tailed. Confidence intervals are typically written in the form: (lower limit, upper limit).\n\n# Confidence Intervals are by definition 2-tailed\n# We can also change the confidence level\n\nt.test(big5$Extroversion, mu = 50, alternative = \"two.sided\", conf.level = .99)\n\n\n    One Sample t-test\n\ndata:  big5$Extroversion\nt = 6.6529, df = 403, p-value = 9.394e-11\nalternative hypothesis: true mean is not equal to 50\n99 percent confidence interval:\n 54.26631 59.69903\nsample estimates:\nmean of x \n 56.98267 \n\n\nTo get only the output for the confidence interval to be shown, we can use a $ to select specific output. Much like when pulling a specific column from a dataset, the $ can pull specific output from an analysis.\nBecause the option for the alternative = in the t.test function is “two.sided”, we don’t actually need to include it when getting confidence intervals.\nAlso, confidence intervals do not require an assumed mu value. So a more efficient way to get a confidence interval for a given set of data is:\n\nt.test(big5$Extroversion, conf.level = .99)$conf.int\n\n[1] 54.26631 59.69903\nattr(,\"conf.level\")\n[1] 0.99\n\n\nQuestion: Describe in words the interpretation of the confidence interval in context of Extroversion.\nAnswer: I am 95% confident that the true population mean of extroversion scores for Brother Cannon’s students is between 54.266 and 59.699.\n\n\nChecking Requirements\nWe still rely on the assumption that the distribution of sample means is normally distributed.\nRecall that the mean is normally distributed if:\n\nThe underlying population is normally distributed\n\nWe have a sufficiently large sample size (\\(n&gt;30\\))\n\nFor the above Extroversion data, we have \\(n=404\\) which is much larger than 40.\nIf my sample size was small, I could check the qqPlot(), which I demonstrate here:\n\nqqPlot(big5$Extroversion)\n\n\n\n\n\n\n\n\n[1] 143 163"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/01-Inference_for_mean_sigma_unknown.html#body-temperature-data",
    "href": "5-Statistical_Tests_Part1/01-Inference_for_mean_sigma_unknown.html#body-temperature-data",
    "title": "Inference for a Mean - Sigma Unknown",
    "section": "Body Temperature Data",
    "text": "Body Temperature Data\nThe dataset below contains information about body temperatures of healthy adults.\n\nLoad the data:\n\n# These lines load the data into the data frame body_temp:\n\nbody_temp &lt;- import(\"https://byuistats.github.io/M221R/Data/body_temp.xlsx\")\n\n\n\nReview the Data\nCreate a table of summary statistics for temperature:\n\n\nVisualize the Data\nCreate a histogram to visualize the body temperature data.\nQuestion: Describe the general shape of the distribution.\nAnswer:\n\n\nAnalyze the Data\nIt’s widely accepted that normal body temperature for healthy adults is 98.6 degrees Fahrenheit.\nSuppose we suspect that the average temperature is different than 98.6\nUse a significance level of \\(\\alpha = 0.01\\) to test whether the mean body temperature of healthy adults is equal to 98.6 degrees Fahrenheit.\nQuestion: What is the P-value?\nAnswer:\nQUESTION: What is your conclusion?\nANSWER:\n\nConfidence Interval\nCreate a 99% confidence interval for the true population average temperature of healthy adults.\nCheck the requirements for the t-test (\\(n&gt;30\\) or qqPlot()):\nQUESTION: Are the requirements for the t-test satisfied?\nANSWER:"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/002-Confidence_Intervals.html",
    "href": "5-Statistical_Tests_Part1/002-Confidence_Intervals.html",
    "title": "Confidence Intervals for a Mean",
    "section": "",
    "text": "By the end of this lesson, you should be able to:\n\nRecognize when a one mean (sigma known) confidence interval is appropriate\nExplain the meaning of a level of confidence\nCreate a confidence interval for a single mean with \\(\\sigma\\) known using the following steps:\n\nFind the point estimate (\\(\\bar{x}\\))\nCalculate the margin of error for the given level of confidence\nCalculate a confidence interval from the point estimate and the margin of error\nInterpret the confidence interval\nCheck the requirements for the confidence interval\n\nExplain how the margin of error is affected by the sample size and level of confidence\n\nStatistical Inference is the practice of using data sampled from a population to make conclusions about population parameters.\nThe two primary methods of statistical inference are:\n\nHypothesis Testing\nConfidence Intervals\n\nThis chapter lays the foundation for confidence intervals."
  },
  {
    "objectID": "5-Statistical_Tests_Part1/002-Confidence_Intervals.html#lesson-outcomes",
    "href": "5-Statistical_Tests_Part1/002-Confidence_Intervals.html#lesson-outcomes",
    "title": "Confidence Intervals for a Mean",
    "section": "",
    "text": "By the end of this lesson, you should be able to:\n\nRecognize when a one mean (sigma known) confidence interval is appropriate\nExplain the meaning of a level of confidence\nCreate a confidence interval for a single mean with \\(\\sigma\\) known using the following steps:\n\nFind the point estimate (\\(\\bar{x}\\))\nCalculate the margin of error for the given level of confidence\nCalculate a confidence interval from the point estimate and the margin of error\nInterpret the confidence interval\nCheck the requirements for the confidence interval\n\nExplain how the margin of error is affected by the sample size and level of confidence\n\nStatistical Inference is the practice of using data sampled from a population to make conclusions about population parameters.\nThe two primary methods of statistical inference are:\n\nHypothesis Testing\nConfidence Intervals\n\nThis chapter lays the foundation for confidence intervals."
  },
  {
    "objectID": "5-Statistical_Tests_Part1/002-Confidence_Intervals.html#point-estimators",
    "href": "5-Statistical_Tests_Part1/002-Confidence_Intervals.html#point-estimators",
    "title": "Confidence Intervals for a Mean",
    "section": "Point Estimators",
    "text": "Point Estimators\nWe have learned about several statistics. Remember, a statistic is any number computed based on data. The sample statistics we have discussed are used to estimate population parameters.\n\n\n\n\n\n\n\n\nSample Statistic\n\n\n\n\nPopulation Parameter\n\n\n\n\n\n\n\n\nMean\n\n\n\n\n\\(\\bar x\\)\n\n\n\n\n\\(\\mu\\)\n\n\n\n\n\n\nStandard Deviation\n\n\n\n\n\\(s\\)\n\n\n\n\n\\(\\sigma\\)\n\n\n\n\n\n\nVariance\n\n\n\n\n\\(s^2\\)\n\n\n\n\n\\(\\sigma^2\\)\n\n\n\n\n\n\n\\(\\vdots\\)\n\n\n\n\n\\(\\vdots\\)\n\n\n\n\n\\(\\vdots\\)\n\n\n\n\n\n\nThe statistics above are called point estimators because they are just one number (one point on a number line) that is used to estimate a parameter. Parameters are generally unknown values. Think about the mean. If \\(\\mu\\) is unknown, how do we know if \\(\\bar{x}\\) is close to it?\nThe short answer is that we will never know for sure if \\(\\bar{x}\\) is close to \\(\\mu\\). This does not mean that we are helpless. The normal distribution provides a way for us to create a range of plausible values for a parameter (e.g. \\(\\mu\\)) based on a statistic (e.g. \\(\\bar{x}\\))."
  },
  {
    "objectID": "5-Statistical_Tests_Part1/002-Confidence_Intervals.html#review-distribution-of-sample-means",
    "href": "5-Statistical_Tests_Part1/002-Confidence_Intervals.html#review-distribution-of-sample-means",
    "title": "Confidence Intervals for a Mean",
    "section": "Review: Distribution of Sample Means",
    "text": "Review: Distribution of Sample Means\nConfidence intervals rely on the validity of the assumption that the distribution of the sample mean is normally distributed.\nRecall that the distribution of sample means is normal when:\n\nThe underlying population is normally distributed\nThe sample size, n, is sufficiently large (\\(n&lt;30\\) for this class) for the Central Limit Theorem to apply\n\nThought Question: If we have a good sample from a population and can trust that the sampling distribution of the mean is approximately normal, how frequently would a sample mean be within 2 standard deviations from the true population mean?\nRemember, the standard deviation of \\(\\bar x\\) is \\(\\frac{\\sigma}{\\sqrt{n}}\\). For the variable \\(\\bar x\\), two standard deviations would be equal to \\(2 \\frac{\\sigma}{\\sqrt{n}}\\).\nANSWER: If we collect a random sample from a population and \\(\\bar x\\) is normally distributed, then about 95% of the time the sample mean \\(\\bar x\\) will be less than \\(2 \\frac{\\sigma}{\\sqrt{n}}\\) units away from the population mean \\(\\mu\\). Notice that this is true, whether or not we know \\(\\mu\\).\n\n\n\n\n\n\n\n\n\nThis means the 95% of the time, we will get a sample mean within 2 Standard Deviations of the true population mean.\nFlipping this around, if we take our sample mean and make an interval 2 standard deviations above the mean and 2 below, the interval will overlap with the true population mean about 95% of the time."
  },
  {
    "objectID": "5-Statistical_Tests_Part1/002-Confidence_Intervals.html#an-approximate-95-confidence-interval",
    "href": "5-Statistical_Tests_Part1/002-Confidence_Intervals.html#an-approximate-95-confidence-interval",
    "title": "Confidence Intervals for a Mean",
    "section": "An Approximate 95% Confidence Interval",
    "text": "An Approximate 95% Confidence Interval\nThe equation of an approximate 95% confidence interval would be:\n\\[ CI = \\bar{x} \\pm 2 \\frac{\\sigma}{\\sqrt{n}}\\]\nThe part that we are adding and subtracting from our point estimate is called the Margin of Error. We use the letter \\(m\\) to denote the margin of error:\n\\[m = 2 \\frac{\\sigma}{\\sqrt{n}}\\]\nUsing this definition for \\(m\\), our confidence interval can be written as\n\\[( \\bar x - m, ~ \\bar x + m )\\]"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/002-Confidence_Intervals.html#interpretation",
    "href": "5-Statistical_Tests_Part1/002-Confidence_Intervals.html#interpretation",
    "title": "Confidence Intervals for a Mean",
    "section": "Interpretation",
    "text": "Interpretation\nConfidence intervals are typically reported using parentheses like: (lower limit, upper limit). We say that we are \\(100*(1-\\alpha)\\%\\) confident that the true population mean is between [lower limit] and [upper limit].\n\nAverage GRE Scores of BYU-I Students\nThe published population standard deviation of the quantitative portion of the Graduate Record Examination (GRE) scores is \\(\\sigma=8.3\\).\nSuppose we take a random sample of \\(n=100\\) BYU-I students who have taken the GRE and find that their average score was \\(\\bar{x}=162.1\\)\nWe can calculate the 99% confidence interval:\n\\[ 162.1 \\pm 2.576\\frac{8.3}{\\sqrt{100}} = (159.96, 164.24)\\]\nThe interpretation of the above confidence interval would be:\nI am 99% confident that the true population mean GRE score for BYU-I students is between 159.96 and 164.24."
  },
  {
    "objectID": "4-Foundations_Statistical_Inference/Sampling_Distributions_CLT_LLN_Reading.html",
    "href": "4-Foundations_Statistical_Inference/Sampling_Distributions_CLT_LLN_Reading.html",
    "title": "Distribution of Sample Means",
    "section": "",
    "text": "This lesson explores the idea of a sampling distribution which is the theoretical distribution of a sample statistic. We discuss the theoretical concepts that allow us to make confident conclusions about an entire population based on a single sample.\n\n\nBy the end of this lesson, you should be able to:\n\nDescribe the concept of a sampling distribution of the sample mean\nState the Central Limit Theorem\nDetermine the mean of the sampling distribution for a given parent population\nDetermine the standard deviation of the sampling distribution of the sample mean for a given parent population\nDetermine the shape of the sampling distribution of the sample mean for a given parent population\nState the Law of Large Numbers"
  },
  {
    "objectID": "4-Foundations_Statistical_Inference/Sampling_Distributions_CLT_LLN_Reading.html#when-is-bar-x-normal",
    "href": "4-Foundations_Statistical_Inference/Sampling_Distributions_CLT_LLN_Reading.html#when-is-bar-x-normal",
    "title": "Distribution of Sample Means",
    "section": "When is \\(\\bar x\\) Normal?",
    "text": "When is \\(\\bar x\\) Normal?\nHow many observations are required so that the Central Limit Theorem will assure that the distribution of sample means will be approximately normal? The answer is, “it depends.”\n\nIf the parent population is normal, then the sampling distribution of \\(\\bar x\\) will always be normally distributed, no matter how many observations are selected.\nIf the parent population is not normal, then the sampling distribution \\(\\bar x\\) will be approximately normally distributed, if the sample size is large enough.\n\nIf the parent population is almost normal (e.g. mound-shaped and nearly symmetrical), then a sample of size n = 5 will probably be sufficient to assure that \\(\\bar x\\) will be approximately normally distributed.\nIf the parent population is heavily skewed, then it will require a larger sample size to be assured that \\(\\bar x\\) will be normally distributed. For most moderately skewed distributions, a sample size of around 30 is traditionally thought to be sufficiently large to assure that \\(\\bar x\\) will be approximately normally distributed. This is not a definitive number but is a rule of thumb.\nFor tremendously skewed distributions (e.g., the distribution of lottery payouts), a much larger sample will be required before the distribution of sample means is approximately normal. This may require billions of observations. Simulation can be used to determine if a particular sample size is sufficient.\n\n\nFor this course, if the sample size is at least 30, we will conclude that the sampling distribution of \\(\\bar x\\) will be approximately normal.\n\n\nAnswer the following question:\n\n\n\nThere are two ways that \\(\\bar x\\) can be (approximately) normally distributed. What are they?\n\n\n\nShow/Hide Solution\n\n\n\\(\\bar x\\) will be normally distributed if the data were drawn from a normal population.\n\\(\\bar x\\) will be approximately normally distributed if the sample size is sufficiently large."
  },
  {
    "objectID": "4-Foundations_Statistical_Inference/Sampling_Distributions_CLT_LLN_Reading.html#mean-and-standard-deviation-of-the-distribution-of-sample-means",
    "href": "4-Foundations_Statistical_Inference/Sampling_Distributions_CLT_LLN_Reading.html#mean-and-standard-deviation-of-the-distribution-of-sample-means",
    "title": "Distribution of Sample Means",
    "section": "Mean and Standard Deviation of the Distribution of Sample Means",
    "text": "Mean and Standard Deviation of the Distribution of Sample Means\nThe following facts are always true. They do not depend on the Central Limit Theorem. They do not depend on the sample size. These facts hold for the sample mean \\(\\bar x\\) of any simple random sample of size \\(n\\) drawn from a population with mean \\(\\mu\\) and standard deviation \\(\\sigma\\).\n\nThe mean of the sample means is \\(\\mu\\)\nThe standard deviation of the sample means is \\(\\sigma / \\sqrt{n}\\)\n\nRemember, these facts are always true. They do not depend on normality or the sample size. So, even though these facts are often used in conjunction with the Central Limit Theorem, they do not depend on it.\nThink about the simulations you have observed. The mean of the sample means was always aligned with \\(\\mu\\). Also, if you took samples larger than \\(n = 1\\), the standard deviation of sample means was always smaller than \\(\\sigma\\).\n\nAnswer the following questions:\n\n\n\nThe amount of time passengers spend waiting for a bus on a particular urban route follows a distribution that has a mean of 8.7 minutes with a standard deviation of 2.2 minutes. Transportation officials observed the waiting times for a random sample of \\(n=121\\) individual passengers and recorded the sample mean, \\(\\bar x\\). We can think of this sample mean as one value observed out of all the possible sample means that could have been observed. What is the mean of the distribution of all possible sample means?\n\n\n\nShow/Hide Solution\n\n\\[\n\\mu = 8.7~\\text{minutes}\n\\]\n\n\nUse the information in the previous problem to answer this question: What is the standard deviation of the distribution of all possible sample means?\n\n\n\nShow/Hide Solution\n\n\\[\n\\frac{\\sigma}{\\sqrt{n}} = \\frac{2.2}{\\sqrt{121}} = 0.2~\\text{minutes}\n\\]"
  },
  {
    "objectID": "4-Foundations_Statistical_Inference/Sampling_Distributions_CLT_LLN_Reading.html#review-of-key-concepts",
    "href": "4-Foundations_Statistical_Inference/Sampling_Distributions_CLT_LLN_Reading.html#review-of-key-concepts",
    "title": "Distribution of Sample Means",
    "section": "Review of Key Concepts",
    "text": "Review of Key Concepts\nIn order to “do statistics” we need to compute probabilities. This requires we know the distribution of the population of interest. The most important distribution in this course is the normal distribution. In this lesson, we are interested in another very important distribution: the distribution of all the sample means, \\(\\bar{x}\\). In order to do statistics with \\(\\bar{x}\\) we need the sampling distribution of \\(\\bar{x}\\) to be normal. There are two situations when the distribution of \\(\\bar{x}\\) is guaranteed to be normal (or at least very close to normal). They are:\n\nIf the parent population is normal, the distribution of the sample means \\(\\bar{x}\\) will be normal, for every sample size \\(n\\).\nEven if the parent population is not normal, the Central Limit Theorem guarantees that the distribution of the sample mean \\(\\bar{x}\\) will be approximately normal if the sample size \\(n\\) is large enough. For this course, if \\(n \\geq 30\\), we will say the distribution of the sample means will be approximately normal (even if the parent population is not normal).\n\nThe mean and standard deviation of \\(\\bar{x}\\) are:\n\nThe mean \\(\\mu_{\\bar{x}}\\) of the sample means is the population mean \\(\\mu\\).\nThe standard deviation \\(\\sigma_{\\bar{x}}\\) of the sample means is the population standard deviation \\(\\sigma\\) divided by the square root of \\(n\\), \\(\\sigma / \\sqrt{n}\\)"
  },
  {
    "objectID": "4-Foundations_Statistical_Inference/Sampling_Distributions_CLT_LLN_Reading.html#summary",
    "href": "4-Foundations_Statistical_Inference/Sampling_Distributions_CLT_LLN_Reading.html#summary",
    "title": "Distribution of Sample Means",
    "section": "Summary",
    "text": "Summary\n\nRemember…\n\n\n\nThe distribution of sample means is a distribution of all possible sample means (\\(\\bar x\\)) for a particular sample size.\nThe Central Limit Theorem states that the sampling distribution of the sample mean will be approximately normal if the sample size \\(n\\) of a sample is sufficiently large. In this class, \\(n\\ge 30\\) is considered to be sufficiently large.\nThe mean of the distribution of sample means is the mean \\(\\mu\\) of the population: \\(\\mu_{\\bar{x}} = \\mu\\).\nThe standard deviation of the distribution of sample means is the standard deviation \\(\\sigma\\) of the population divided by the square root of \\(n\\): \\(\\sigma_{\\bar{x}} = \\sigma/\\sqrt{n}\\).\nThe distribution of sample means is normal in either of two situations: (1) when the data is normally distributed or (2) when, thanks to the Central Limit Theorem (CLT), our sample size (\\(n\\)) is large.\nThe Law of Large Numbers states that as the sample size (\\(n\\)) gets larger, the sample mean (\\(\\bar x\\)) will get closer to the population mean (\\(\\mu\\)). This can be seen in the equation for \\(\\sigma_{\\bar{x}} = \\sigma/\\sqrt{n}\\). Notice as \\(n\\) increases, then \\(\\sigma_\\bar{x}\\) will get smaller."
  },
  {
    "objectID": "4-Foundations_Statistical_Inference/CLT_Class_Exercise.html",
    "href": "4-Foundations_Statistical_Inference/CLT_Class_Exercise.html",
    "title": "Distribution of Sample Statistics",
    "section": "",
    "text": "This in-class activity will explore the distribution of sample statistics and the Central Limit Theorem.\nTo demonstrate, we will be exploring a dataset about different species of penguins collected on different islands. Background about the study and data can be found here.\nPractice the process for approaching a dataset outlined in class:\n\nLoad the data and libraries\nExplore the data and generate hypotheses\nPrepare the data for analysis\nPerform the appropriate analysis\n\nData preparation will include using the filter() function. For now, analysis means creating good visualizations that tell a story using ggplot() and base R."
  },
  {
    "objectID": "4-Foundations_Statistical_Inference/CLT_Class_Exercise.html#explore-the-data-on-your-own",
    "href": "4-Foundations_Statistical_Inference/CLT_Class_Exercise.html#explore-the-data-on-your-own",
    "title": "Distribution of Sample Statistics",
    "section": "Explore the data on your own",
    "text": "Explore the data on your own\nQUESTION: What is/are some potential dependent/response variables?\nANSWER:\nQUESTION: What are some potential explanatory variables?\nANSWER:\nLet’s look deeper into bill_depth_mm."
  },
  {
    "objectID": "4-Foundations_Statistical_Inference/CLT_Class_Exercise.html#create-a-table-of-summary-statistics-favstats-for-bill_depth_mm",
    "href": "4-Foundations_Statistical_Inference/CLT_Class_Exercise.html#create-a-table-of-summary-statistics-favstats-for-bill_depth_mm",
    "title": "Distribution of Sample Statistics",
    "section": "Create a table of summary statistics (favstats()) for bill_depth_mm:",
    "text": "Create a table of summary statistics (favstats()) for bill_depth_mm:\nQUESTION: What is your sample mean?\nANSWER:\nQUESTION: What is your sample standard deviation?\nANSWER:\nQUESTION: What is your sample median?\nANSWER:"
  },
  {
    "objectID": "4-Foundations_Statistical_Inference/CLT_Class_Exercise.html#create-a-frequency-table-of-sex-table",
    "href": "4-Foundations_Statistical_Inference/CLT_Class_Exercise.html#create-a-frequency-table-of-sex-table",
    "title": "Distribution of Sample Statistics",
    "section": "Create a frequency table of sex (table()):",
    "text": "Create a frequency table of sex (table()):\nQUESTION: How many females in your sample?\nANSWER:\nQUESTION: How many males in your sample?\nANSWER:"
  },
  {
    "objectID": "4-Foundations_Statistical_Inference/CLT_Class_Exercise.html#create-a-table-of-species",
    "href": "4-Foundations_Statistical_Inference/CLT_Class_Exercise.html#create-a-table-of-species",
    "title": "Distribution of Sample Statistics",
    "section": "Create a table of species",
    "text": "Create a table of species\nQUESTION: How many Adelie penguins do you have in your sample?\nANSWER:\nQUESTION: How many Chinstrap penguins do you have in your sample?\nANSWER:\nQUESTION: How many Gentoo penguins do you have in your sample?\nANSWER:"
  },
  {
    "objectID": "4-Foundations_Statistical_Inference/CLT_Class_Exercise.html#create-a-side-by-side-boxplot-of-bill_depth_mm-for-all-species",
    "href": "4-Foundations_Statistical_Inference/CLT_Class_Exercise.html#create-a-side-by-side-boxplot-of-bill_depth_mm-for-all-species",
    "title": "Distribution of Sample Statistics",
    "section": "Create a side-by-side boxplot of bill_depth_mm for all species:",
    "text": "Create a side-by-side boxplot of bill_depth_mm for all species:"
  },
  {
    "objectID": "4-Foundations_Statistical_Inference/CLT_Class_Exercise.html#create-a-side-by-side-boxplot-of-bill_depth_mm-for-all-sex",
    "href": "4-Foundations_Statistical_Inference/CLT_Class_Exercise.html#create-a-side-by-side-boxplot-of-bill_depth_mm-for-all-sex",
    "title": "Distribution of Sample Statistics",
    "section": "Create a side-by-side boxplot of bill_depth_mm for all sex:",
    "text": "Create a side-by-side boxplot of bill_depth_mm for all sex:"
  },
  {
    "objectID": "4-Foundations_Statistical_Inference/09-Prob_Calculations_for_Means_Practice.html",
    "href": "4-Foundations_Statistical_Inference/09-Prob_Calculations_for_Means_Practice.html",
    "title": "Probability Calculations for Means (Practice)",
    "section": "",
    "text": "Answer the following questions, render the document and submit the .html report."
  },
  {
    "objectID": "4-Foundations_Statistical_Inference/09-Prob_Calculations_for_Means_Practice.html#gpa",
    "href": "4-Foundations_Statistical_Inference/09-Prob_Calculations_for_Means_Practice.html#gpa",
    "title": "Probability Calculations for Means (Practice)",
    "section": "GPA",
    "text": "GPA\nSuppose the mean GPA of BYU-Idaho students is 3.5 and the standard deviation is 0.7. It is well known that this distribution is left-skewed. A random sample of n = 81 students will be drawn.\nUse the following R code to answer the questions below:\n\nxbar &lt;- 3.3\nmu &lt;- 3.5\nsigma &lt;- 0.7\nn &lt;- 81\nsigma_xbar &lt;- sigma / sqrt(n)\nsigma_xbar\n\n[1] 0.07777778\n\nz &lt;- (xbar-mu) / sigma_xbar\nz\n\n[1] -2.571429\n\n# Area to the left:\npnorm(z)  \n\n[1] 0.005063995\n\n# Area to the right:\n1-pnorm(z)\n\n[1] 0.994936\n\n\nQuestion: What is the mean of the distribution of the sample means (\\(\\mu_{\\bar{x}}\\)) for all possible samples of size 81 that could be drawn from the parent population of GPAs?\nAnswer: 3.5\nQuestion: What is the standard deviation of the distribution of the sample means (\\(\\sigma_{\\bar{x}}\\)) for all possible samples of size 81 that could be drawn from the parent population of GPAs?\nAnswer: 0.07777778\nQuestion: What is the shape of the distribution of the sample means for all possible samples of size 81 that could be drawn from the parent population of GPAs?\nAnswer: Normal because CLT (n &gt; 30)\nQuestion: What is the probability that the mean GPA for 81 randomly selected BYU-Idaho students will be less than 3.3?\nAnswer:"
  },
  {
    "objectID": "4-Foundations_Statistical_Inference/09-Prob_Calculations_for_Means_Practice.html#gre",
    "href": "4-Foundations_Statistical_Inference/09-Prob_Calculations_for_Means_Practice.html#gre",
    "title": "Probability Calculations for Means (Practice)",
    "section": "GRE",
    "text": "GRE\nScores on the quantitative portion of the GRE are approximately normally distributed with mean, \\(\\mu = 150.8\\) and standard deviation, \\(\\sigma = 8.8\\).\nUse the following R code to answer the questions below:\n\nxbar &lt;- \nmu &lt;- \nsigma &lt;- \nn &lt;- \nsigma_xbar &lt;- \nsigma_xbar\n\nz &lt;- \nz\n\n# Area to the left:\n  \n# Area to the right:\n\n# Percentile (qnorm())\n\nQuestion: Dianne earned a score of 160 on the quantitative portion of the GRE. What is the z-score corresponding to Dianne’s score?\nAnswer:\nQuestion: What is the probability that a randomly selected student will score above 160 on the quantitative portion of the GRE?\nAnswer:\nQuestion: What GRE score (n=1) corresponds to the 95th percentile?\nAnswer:\nQuestion: What is the probability that the average GRE score of 5 randomly selected students will be above 160?\nAnswer:"
  },
  {
    "objectID": "4-Foundations_Statistical_Inference/09-Prob_Calculations_for_Means_Practice.html#tankers",
    "href": "4-Foundations_Statistical_Inference/09-Prob_Calculations_for_Means_Practice.html#tankers",
    "title": "Probability Calculations for Means (Practice)",
    "section": "Tankers",
    "text": "Tankers\nTanker trucks are designed to carry huge quantities of gasoline from refineries to filling stations. A factory that manufactures the tank of the trucks claims to manufacture tanks with a capacity of 8550 gallons of gasoline. The actual capacity of the tanks is normally distributed with mean, \\(\\mu = 8544\\) gallons, and standard deviation, \\(\\sigma=12\\) gallons.\nUse the following R code to answer the questions below:\n\nxbar &lt;- 8550\nmu &lt;- 8544\nsigma &lt;- 12\nn &lt;- 1\nsigma_xbar &lt;- sigma / sqrt(n)\nsigma_xbar\n\n[1] 12\n\nz &lt;- (xbar - mu) / sigma_xbar\nz\n\n[1] 0.5\n\n# Area to the left:\npnorm(z)\n\n[1] 0.6914625\n\n#Area to the right\n1-pnorm(z)\n\n[1] 0.3085375\n\n## Area between A and B\nA &lt;- \nB &lt;- \n\npnorm(B, mu, sigma_xbar) - pnorm(A, mu, sigma_xbar)\n\nError: object 'B' not found\n\n\nQuestion: Find the z-score corresponding to a single tank (\\(n=1\\)) with a capacity of 8550 gallons. Round your answer to one decimal place.\nAnswer: 0.5\nQuestion: What is the probability that a randomly selected tank will have a capacity of less than 8550 gallons?\nAnswer:\nQuestion: A simple random sample of \\(n = 20\\) tanks was selected. Find the z-score corresponding to a sample mean capacity for 20 tanks of 8550.\nAnswer:\nQuestion: What is the probability that the sample mean of \\(n=20\\) randomly selected tanks will be between 8541 and 8547?\nAnswer:"
  },
  {
    "objectID": "4-Foundations_Statistical_Inference/07-Prob_Calculations_for_Means.html",
    "href": "4-Foundations_Statistical_Inference/07-Prob_Calculations_for_Means.html",
    "title": "Probability Calculations for Means (Reading)",
    "section": "",
    "text": "When is the sample mean normally distributed? This happens when either of the two conditions are satisfied:\n\nThe population is normally distributed, so the sample mean is automatically normally distributed.\nThe sample size is large, and the Central Limit Theorem implies that the sample mean is normally distributed.\n\nFor the mean of draws from a random variable with mean \\(\\mu\\) and standard deviation \\(\\sigma\\), the following are true:\n\nThe mean of the random variable \\(\\bar X\\) is \\(\\mu\\).\nThe standard deviation of the random variable \\(\\bar X\\) is \\(\\displaystyle{\\frac{\\sigma}{\\sqrt{n}}}\\).\n\nPreviously, you learned how to use the normal probability applet and R to convert a \\(z\\)-score to a corresponding area under the curve. These skills will be applied again in this activity. The following questions will help you brush up on how to calculate probabilities for a normal distribution.\n\n\n\n\n\nAnswer the following questions:\n\n\n\nLook at the output from the applet, given above. What is the probability that a standard normal random variable will be below 1.25?\n\n\n\nShow/Hide Solution\n\n\n\\(0.8944\\)\n\n\n1a. Compare this to the R output using pnorm()\n\n\nShow/Hide Solution\n\n\npnorm(1.25)\n\n[1] 0.8943502\n\n\n\n\nLook at the output from the applet given above. What is the probability that a standard normal random variable will be above 1.25?\n\n\n\nShow/Hide Solution\n\n\n\\(1 - 0.8944 = 0.1056\\)\n\n\n\nChallenge problem: Use R to determine the probability that a standard normal random variable will be between -1.3 and 1.25.\n\n\n\nShow/Hide Solution\n\n\npnorm(1.25) - pnorm(-1.3)\n\n[1] 0.7975497\n\n\n\n\nUse R to find the probability that a standard normal random variable will be greater than -0.75.\n\n\n\nShow/Hide Solution\n\n\n1-pnorm(-.75)\n\n[1] 0.7733726\n\n# Equivalantly \npnorm(-.75, lower.tail = FALSE)\n\n[1] 0.7733726\n\n\n\n\nIf the mean of a normally distributed random variable is -23 and the standard deviation is 7, what is the probability that the random variable will have a value that is less than -25?\n\n\n\nShow/Hide Solution\n\n\nThe \\(z\\)-score is: \n\n\\(\\displaystyle{z = \\frac{-25 - (-23)}{7} = -0.2857}\\)\n\n Using R:\n\n\nx &lt;- -25\nmu &lt;- -23\nsigma &lt;- 7\n\nz &lt;- (x-mu)/sigma\nz\n\n[1] -0.2857143\n\npnorm(z)\n\n[1] 0.3875485\n\n\n*We find that the area to the left of \\(z\\) = -0.2857 is \\(0.3876\\)."
  },
  {
    "objectID": "4-Foundations_Statistical_Inference/07-Prob_Calculations_for_Means.html#review-of-sampling-distributions",
    "href": "4-Foundations_Statistical_Inference/07-Prob_Calculations_for_Means.html#review-of-sampling-distributions",
    "title": "Probability Calculations for Means (Reading)",
    "section": "",
    "text": "When is the sample mean normally distributed? This happens when either of the two conditions are satisfied:\n\nThe population is normally distributed, so the sample mean is automatically normally distributed.\nThe sample size is large, and the Central Limit Theorem implies that the sample mean is normally distributed.\n\nFor the mean of draws from a random variable with mean \\(\\mu\\) and standard deviation \\(\\sigma\\), the following are true:\n\nThe mean of the random variable \\(\\bar X\\) is \\(\\mu\\).\nThe standard deviation of the random variable \\(\\bar X\\) is \\(\\displaystyle{\\frac{\\sigma}{\\sqrt{n}}}\\).\n\nPreviously, you learned how to use the normal probability applet and R to convert a \\(z\\)-score to a corresponding area under the curve. These skills will be applied again in this activity. The following questions will help you brush up on how to calculate probabilities for a normal distribution.\n\n\n\n\n\nAnswer the following questions:\n\n\n\nLook at the output from the applet, given above. What is the probability that a standard normal random variable will be below 1.25?\n\n\n\nShow/Hide Solution\n\n\n\\(0.8944\\)\n\n\n1a. Compare this to the R output using pnorm()\n\n\nShow/Hide Solution\n\n\npnorm(1.25)\n\n[1] 0.8943502\n\n\n\n\nLook at the output from the applet given above. What is the probability that a standard normal random variable will be above 1.25?\n\n\n\nShow/Hide Solution\n\n\n\\(1 - 0.8944 = 0.1056\\)\n\n\n\nChallenge problem: Use R to determine the probability that a standard normal random variable will be between -1.3 and 1.25.\n\n\n\nShow/Hide Solution\n\n\npnorm(1.25) - pnorm(-1.3)\n\n[1] 0.7975497\n\n\n\n\nUse R to find the probability that a standard normal random variable will be greater than -0.75.\n\n\n\nShow/Hide Solution\n\n\n1-pnorm(-.75)\n\n[1] 0.7733726\n\n# Equivalantly \npnorm(-.75, lower.tail = FALSE)\n\n[1] 0.7733726\n\n\n\n\nIf the mean of a normally distributed random variable is -23 and the standard deviation is 7, what is the probability that the random variable will have a value that is less than -25?\n\n\n\nShow/Hide Solution\n\n\nThe \\(z\\)-score is: \n\n\\(\\displaystyle{z = \\frac{-25 - (-23)}{7} = -0.2857}\\)\n\n Using R:\n\n\nx &lt;- -25\nmu &lt;- -23\nsigma &lt;- 7\n\nz &lt;- (x-mu)/sigma\nz\n\n[1] -0.2857143\n\npnorm(z)\n\n[1] 0.3875485\n\n\n*We find that the area to the left of \\(z\\) = -0.2857 is \\(0.3876\\)."
  },
  {
    "objectID": "4-Foundations_Statistical_Inference/07-Prob_Calculations_for_Means.html#probabilities-involving-a-mean",
    "href": "4-Foundations_Statistical_Inference/07-Prob_Calculations_for_Means.html#probabilities-involving-a-mean",
    "title": "Probability Calculations for Means (Reading)",
    "section": "Probabilities Involving a Mean",
    "text": "Probabilities Involving a Mean\nIf the sample mean is normally distributed, then we can consider the sample mean, \\(\\bar x\\), as one observation from a normal distribution with mean \\(\\mu\\) and standard deviation \\(\\displaystyle{\\frac{\\sigma}{\\sqrt{n}}}\\). With this in mind, we can compute the \\(z\\)-score for any value of this random variable:\n\\[z = \\frac{\\text{value} - \\text{mean}}{\\text{standard deviation}}=\\frac{\\bar x-\\mu}{\\sigma / \\sqrt{n} }\\]\nNotice that we just replaced the “value” with the normal random variable, the “mean” with its mean and “standard deviation” with its standard deviation.\nThe \\(z\\)-score follows a standard normal distribution. That is, it has a mean of 0 and a standard deviation of 1. Now we can simply input the \\(z\\)-score into pnorm() to get the probability that a randomly selected mean will be above or below a given value of \\(\\bar x\\).\nWorked Example: Finding the Area under a Normal Curve (Based on a Sample Mean)\nAfter finding the \\(z\\)-score, we can use pnorm() to find the area under the curve (i.e. the probability.) Suppose that a sample of size \\(n = 4\\) has been drawn from a normal population with mean \\(\\mu = 7\\) and standard deviation \\(\\sigma = 3\\). Since the parent population is normal, we know the sampling distribution of the sample mean \\(\\bar x\\) will be normal with mean \\(\\mu = 7\\) and standard deviation \\(\\displaystyle{ \\frac{\\sigma}{\\sqrt{n}} = \\frac{3}{\\sqrt{4}} = 1.5}\\). We can use this information to find the probability that \\(\\bar x\\) will be greater than 10.\nThe \\(z\\)-score is\n\\[z=\\frac{\\bar x-\\mu}{\\sigma / \\sqrt{n} } = \\frac{10 - 7}{3 / \\sqrt{4} }=2.0\\]\nThe probability that \\(z\\) will be greater than \\(2.00\\) is the area under the standard normal distribution to the right of \\(2.00\\). We find this area using the normal probability using R:\n\n\nxbar &lt;- 10\nmu &lt;- 7\nsigma &lt;- 3\nn &lt;- 4\nsigma_xbar &lt;- sigma/sqrt(n)\n\nz &lt;- (xbar-mu)/sigma_xbar\nz\n\n[1] 2\n\n#Area to the left:\npnorm(z)\n\n[1] 0.9772499\n\n#Area to the right:\n1-pnorm(z)\n\n[1] 0.02275013\n\n\nThe area to the right of \\(z = 2.00\\) is \\(0.02275\\). This is the probability that the random sample of \\(n = 4\\) items will have a mean that is greater than \\(10\\).\nIn this example, the sample mean was automatically normally distributed because the parent population was normally distributed. This will always be true, no matter what size sample is drawn.\nWorked Example: Finding the Area under a Normal Curve (Based on a Sample Mean)\nWhat do we do if the parent population is not normally distributed? If the sample size is large, then the Central Limit Theorem guarantees that the sample mean will be approximately normally distributed. Based on this, we can still do normal probability calculations for the mean of a random sample.\nThe distribution of the weekly costs incurred by Global Solutions Unlimited is right skewed. The population mean of the costs is $26,400 and the standard deviation is $23,200. A random sample of \\(n = 40\\) weeks is to be selected, what is the probability that the mean weekly costs will be less than $20,000?\nSince the number of observations is large, the Central Limit Theorem assures that the sample mean will follow a normal distribution. The \\(z\\)-score for \\(\\bar x =\\) $20,000 is:\n\\[z = \\frac{\\bar x-\\mu}{\\sigma / \\sqrt{n} } = \\frac{20,000-26,400}{23,200 / \\sqrt{40}} = -1.745 \\]\nUsing R, we can find the area to the left of this \\(z\\)-score:\n\nxbar &lt;- 20000\nmu &lt;- 26400\nsigma &lt;- 23200\nn &lt;- 40\nsigma_xbar &lt;- sigma/sqrt(n)\n\nz &lt;- (xbar-mu) / sigma_xbar\nz\n\n[1] -1.744705\n\n#Area to the left:\npnorm(z)\n\n[1] 0.04051812\n\n#Area to the right:\n1-pnorm(z)\n\n[1] 0.9594819\n\n\n\nNOTE: Recall the the Normal Probabilty Applet rounds the \\(z\\)-scores first, then calculates the probabilities so they differ slightly from R.\nThe area to the left of \\(z= -1.745\\) is \\(0.04052\\). This is the probability that the mean costs for the \\(n = 40\\) weeks will be less than $20,000.\n\n\nExample: Environmental Clean Up\n\n\nWe will now consider a complete example that shows how these probabilities are used in practice.\nThe United States Government decided to open some land near a uranium enrichment facility to public use. After a few years of the public hiking, biking, and sometimes even hunting on this land, workers from the facility noticed that there were several unnatural-looking mounds in the earth near the area. Because this land was once used by the facility and nobody knew the origin of these piles, the government closed public access to the land until they could assess if the mounds were safe.\n\n\nStep 1: Design the study.\nMeasurements were taken from the mounds to assess one of the contaminants, lead. The tests involved are very expensive. Each sample costs about $600 to process. The Environmental Protection Agency (EPA) has set a “No Action Level” (NAL) for lead. If the mean concentration in the soil of the contaminant is less than the NAL, then the area can be declared safe for public use. If the concentration of the contaminant reaches or exceeds the NAL, the site must be cleaned additionally before it is declared safe. The NAL for lead is 50 milligrams of lead per kilogram of soil (mg/kg).\nThe hypothesis for this test are:\n\\(H_0:~~\\mu = 50 \\frac{\\text{mg}}{\\text{kg}}\\)\n\\(H_a:~~\\mu &lt; 50 \\frac{\\text{mg}}{\\text{kg}}\\)\nIn environmental testing, we always assume the site is dirty. That is, our null hypothesis is that the mean level of contamination is at the NAL. We gather data to determine if there is sufficient evidence to support rejecting the null hypothesis.\n\n\nStep 2: Collect Data\nScientists collected \\(n = 61\\) measurements of the lead concentration in the soil, measured in mg/kg. The data are given in the file Uranium Plant Data-Lead.\n\n\nStep 3: Describe the Data\n\nAnswer the following question:\n\n\n\nCreate a histogram of the lead contamination data.\n\n\n\nShow/Hide Solution\n\n\nlibrary(rio)\nlibrary(mosaic)\n\nuranium &lt;- import('https://github.com/byuistats/BYUI_M221_Book/raw/master/Data/UraniumPlantData-Lead.xlsx')\n\nhistogram(uranium$`Lead (mg/kg)`, xlab=\"Lead (mg/kg)\")\n\n\n\n\n\n\n\n\n\n\nWhat is the shape of the distribution of the data?\n\n\n\nShow/Hide Solution\n\n\nThe lead concentration data are right skewed.\n\n\n\nAre any of the measured values above the NAL (50 mg/kg)? What might this suggest?\n\n\n\nShow/Hide Solution\n\n\nYes, there are several observations that are above the NAL. This may suggest that the lead concentrations are very high in some isolated soil samples. It may also be an indication of variability in the test results.\n\n\n\nVisually, does it look like the mean lead concentration is less than 50 mg/kg?\n\n\n\nShow/Hide Solution\n\n\nAnswers may vary.\n\n\n\nWhat is the value of the sample mean?\n\n\n\nShow/Hide Solution\n\n\nknitr::kable(favstats(uranium$`Lead (mg/kg)`))\n\n\n\n\n\nmin\nQ1\nmedian\nQ3\nmax\nmean\nsd\nn\nmissing\n\n\n\n\n\n9.8\n14.1\n19.8\n33.1\n115\n29.13279\n23.26021\n61\n0\n\n\n\n\n\n\nThe sample mean is 29.13 mg/kg.\n\n\n\n\n\nStep 4: Make Inferences\nWe assume the null hypothesis is true and we gather evidence against this requirement. We will find the probability that the mean lead concentration is less than \\(\\bar x\\) = 29.13 mg/kg, assuming that the true mean lead concentration is \\(\\mu\\) = 50 mg/kg. This probability is called the \\(P\\)-value.\nSince the sample size is large (\\(n = 61\\)), we can conclude that the sample mean is normally distributed. So, we can use R to find the probability that the sample mean will be less than 29.13. Historical analyses show that the population standard deviation is \\(\\sigma\\) = 24 mg/kg.\nFirst, we compute the \\(z\\)-score.\n\\[z=\\frac{\\bar x-\\mu}{\\sigma / \\sqrt{n} } = \\frac{29.13 - 50}{24 / \\sqrt{61} }=-6.79\\]\n\nx &lt;- 29.13\nmu &lt;- 50\nsigma &lt;- 24\nn &lt;- 61\nsigma_xbar &lt;- sigma/sqrt(n)\n\nz &lt;- (x-mu)/sigma_xbar\nz\n\n[1] -6.791663\n\npnorm(z)\n\n[1] 5.542414e-12\n\n\nUsing the pnorm() function, we find the area to the left of \\(z = -6.79\\). This is the \\(P\\)-value. It is not clear from the image of the applet, but the area to the left was shaded before the \\(z\\)-score was entered.\n\nThe \\(P\\)-value for this test is so small we have to resort to scientific notation. \\(5.54 \\times 10^{-12} = 0.000~000~000~005~6\\). This is a very small probability. Assuming the null hypothesis is true-that is, the site is unacceptably contaminated-it is very unlikely that we would find such a low mean contamination level among the \\(n = 61\\) randomly selected soil samples.\nSince the \\(P\\)-value is low, we reject the null hypothesis.\n\n\nStep 5: Take Action\nThere is sufficient evidence to suggest that the mean lead level is less than 50 mg/kg. We conclude that the lead concentration in the soil is low enough that it is not a danger to the public. Based on the results of this and other similar test results, the government has reopened public access to this area."
  },
  {
    "objectID": "4-Foundations_Statistical_Inference/07-Prob_Calculations_for_Means.html#review-of-key-concepts",
    "href": "4-Foundations_Statistical_Inference/07-Prob_Calculations_for_Means.html#review-of-key-concepts",
    "title": "Probability Calculations for Means (Reading)",
    "section": "Review of Key Concepts",
    "text": "Review of Key Concepts\n\nTo compute probabilities involving the sample mean \\(\\bar{x}\\) we must first determine if \\(\\bar{x}\\) is normally distributed. If it is not, we cannot compute probabilities. There are two cases when \\(\\bar{x}\\) is guaranteed to be normally distributed. They are:\n\n\nThe parent population is Normally distributed.\nThe Central Limit Theorem guarantees the distribution of \\(\\bar{x}\\) is Normal if the sample size \\(n\\) is large enough. For this course, we require \\(n \\geq 30\\).\n\n\nThe collection of all possible sample means \\(\\bar{x}\\) is also a population. The mean of the distribution of sample means is the population mean \\(\\mu\\). The standard deviation of the distribution of sample means is \\(\\frac{\\sigma}{\\sqrt{n}}\\), where \\(\\sigma\\) is the parent population standard deviation and \\(n\\) is the sample size.\nOnce we have determined that the sample mean is Normally distributed, we can compute probabilities with \\(\\bar{x}\\). We first compute \\(z = \\frac{\\bar{x} - \\mu}{\\sigma/\\sqrt{n}}\\) and then use R to compute probabilities about \\(z\\)."
  },
  {
    "objectID": "4-Foundations_Statistical_Inference/07-Prob_Calculations_for_Means.html#summary",
    "href": "4-Foundations_Statistical_Inference/07-Prob_Calculations_for_Means.html#summary",
    "title": "Probability Calculations for Means (Reading)",
    "section": "Summary",
    "text": "Summary\n\nRemember…\n\n\n\nA z-score for a sample mean is calculated as: \\(\\displaystyle{z = \\frac{\\text{value}-\\text{mean}}{\\text{standard deviation}} = \\frac{\\bar x-\\mu}{\\sigma/\\sqrt{n}}}\\)\nWhen the distribution of sample means is normally distributed, we can use a z-score R to calculate the probability that a sample mean is above, below or between some given value (or values)."
  },
  {
    "objectID": "4-Foundations_Statistical_Inference/05-The_Central_Limit_Theorem.html",
    "href": "4-Foundations_Statistical_Inference/05-The_Central_Limit_Theorem.html",
    "title": "The Distribution of Sample Means (Reading)",
    "section": "",
    "text": "By the end of this lesson, you should be able to:\n\nDescribe the concept of a sampling distribution of the sample mean\nState the Central Limit Theorem\nDetermine the mean of the sampling distribution of the sample mean for a given parent population\nDetermine the standard deviation of the sampling distribution of the sample mean for a given parent population\nDetermine the shape of the sampling distribution of the sample mean for a given parent population\nState the Law of Large Numbers"
  },
  {
    "objectID": "4-Foundations_Statistical_Inference/05-The_Central_Limit_Theorem.html#lesson-outcomes",
    "href": "4-Foundations_Statistical_Inference/05-The_Central_Limit_Theorem.html#lesson-outcomes",
    "title": "The Distribution of Sample Means (Reading)",
    "section": "",
    "text": "By the end of this lesson, you should be able to:\n\nDescribe the concept of a sampling distribution of the sample mean\nState the Central Limit Theorem\nDetermine the mean of the sampling distribution of the sample mean for a given parent population\nDetermine the standard deviation of the sampling distribution of the sample mean for a given parent population\nDetermine the shape of the sampling distribution of the sample mean for a given parent population\nState the Law of Large Numbers"
  },
  {
    "objectID": "4-Foundations_Statistical_Inference/05-The_Central_Limit_Theorem.html#introduction",
    "href": "4-Foundations_Statistical_Inference/05-The_Central_Limit_Theorem.html#introduction",
    "title": "The Distribution of Sample Means (Reading)",
    "section": "Introduction",
    "text": "Introduction\nThis lesson introduces three important concepts of statistical theory:\n\nThe Sampling Distribution of the Sample Mean\nThe Central Limit Theorem\nThe Law of Large Numbers\n\nIn practice, a single sample of data is obtained from a population of interest in order to make inference about the entire population. However, even though only one sample is obtained, there are many, many random samples that are possible to obtain from a population. Understanding what could happen from a theoretical perspective is important in knowing how to use the single sample of data appropriately when making inference about a population. Let’s look at an example."
  },
  {
    "objectID": "4-Foundations_Statistical_Inference/05-The_Central_Limit_Theorem.html#case-study-ddts-negative-impact-on-pregnant-women",
    "href": "4-Foundations_Statistical_Inference/05-The_Central_Limit_Theorem.html#case-study-ddts-negative-impact-on-pregnant-women",
    "title": "The Distribution of Sample Means (Reading)",
    "section": "Case Study: DDT’s Negative Impact on Pregnant Women",
    "text": "Case Study: DDT’s Negative Impact on Pregnant Women\n\n\nStep 1: Design the Study\nMosquitoes and other biting insects carry malaria. In an attempt to curb the spread of malaria and save lives, the pesticide DDT was used for many years to control the insect population, even indoors. Unfortunately, this pesticide does not break down quickly in nature and is very harmful to humans.\nA metabolite of DDT (called DDE) is also very dangerous. A metabolite is the byproduct that occurs when our body breaks down a substance. When DDT is broken down in humans, DDE is one of the metabolites that remains from the original DDT.\nScientists have shown that DDT and its metabolites cause reproductive problems in humans and other animals. When a pregnant woman has a contamination level as low as 10 mg/kg of DDE in her body, she is much more likely to give birth to an underweight baby or to deliver prematurely (Wells & Leonard, 2006).\nA study of 45 pregnant women who have been exposed to DDT / DDE was conducted using a convenience sample (Bornman, MS. 2005). Global Solutions Unlimited (GSU) will analyze the data from this study. These data will be used to assess the prevalence of DDT in drinking water.\n\nStep 2: Collect Data\nAfter selecting specific women to be sampled, researchers measured the level of DDE contamination for each of these randomly selected women.\n\nStep 3: Describe the Data\nThe researchers computed the mean and standard deviation of the observed levels of exposure for the 45 women. They found the mean observed contamination level was 24.75 mg/kg, and the highest level of contamination was 419.91 mg/kg! (Bornman, MS. 2005)\n\nStep 4: Make Inferences\nThe researchers only obtained one sample mean, \\(\\bar x\\). They do not get to see any other data. They only had enough time, funding, and other resources to survey the 45 women in their sample. An important question to ask at this point is whether or not the results of this sample show evidence that the mean DDE contamination in the full population of South American women was greater than 10 mg/kg.\nIf the researchers had selected a different random sample of pregnant women, the mean contamination of the women in the sample would undoubtedly be different. However, this did not happen. The researchers do not get to see the contamination levels for any other women. They only collected this one sample.\nThis is how research is often conducted…repeated samples are not typically drawn, so we do not get to know what might have happened had we gathered data from a different sample. The good news is that statistical theory provides us with the tools we need to be able to use our single sample in a meaningful way to make inference about the full population.\n\nStep 5: Take Action\nIn this study, there was a lot of evidence that the mean DDE contamination in the pregnant women was greater than 10 mg/kg. The probability of the DDE contamination in the women’s bodies exceeding 10 mg/kg (if there were no contamination) is approximately 0.02. This probability is low enough that there is reason for concern. Statistically speaking, there is sufficient evidence to suggest that the levels of DDE contamination are too high among South African women."
  },
  {
    "objectID": "4-Foundations_Statistical_Inference/05-The_Central_Limit_Theorem.html#introduction-to-sampling-distributions",
    "href": "4-Foundations_Statistical_Inference/05-The_Central_Limit_Theorem.html#introduction-to-sampling-distributions",
    "title": "The Distribution of Sample Means (Reading)",
    "section": "Introduction to Sampling Distributions",
    "text": "Introduction to Sampling Distributions\nImagine that each student in the classroom replicated the study described above. Each student sampled 45 women and measured the level of contamination. Each student calculates a sample mean for their own samples. Now imagine making a histogram with everyone’s sample means. The distribution of sample means is an example of a sampling distribution.\n\nNOTE: Any sample statistic we calculate has a sampling distribution because of the inherent randomness from sample to sample, even standard deviations! In this chapter, we focus on the distribution of sample means, though later on in the semester we will consider the distribution of sample proportions as well.\n\nRecall that the sample mean is denoted by the symbol \\(\\bar{x}\\). The mean is computed by adding up all of the values in the sample and dividing by the number of things in the sample.\nRecall also that the true mean of the full population is denoted by the symbol \\(\\mu\\) and that the standard deviation of the population is denoted by \\(\\sigma\\).\nNow, with those reminders in place, you will need to work through the following tutorial to come to understand the idea of a “sampling distribution of a sample mean.”\n\nA Tutorial on Sampling Distributions\n\nStep 1: Open this applet and click on “Begin”\n\nStep 2: Ensure your internet browser window is wide enough to show all of the applet controls.\n\n\n\n\nInternet browser window is too small. (Bad)\n\n\nInternet browser window large enough to see all controls. (Good)\n\n\n\n\n\n\n\n\n\n\n\n\nStep 3: Press the “Animated” button on the right of the applet. It will drop down a single random sample of N=5 data points (i.e., black boxes) from the parent population. (Because the sample is random, your sample will possibly land in a different place than what is shown here.)\n\n\n\nStep 4: Notice that a little blue box dropped down from your “Sample Data” of N=5 black boxes (data points). This little blue box represents the mean, \\(\\bar{x}\\), of your single sample of N=5 boxes.\n\n\nNow is where things get interesting. In real life, all we would use is the single sample of N=5 values and the sample mean, \\(\\bar{x}\\), to make inference (make our guess) about the mean, \\(\\mu\\), of the full population. (The full population in this applet is shown in the top black graph.) However, the purpose of this applet is to allow us to explore how our answer would change if we used a different sample of N=5 data points. A different sample will result in a different sample mean \\(\\bar{x}\\). So the question is, what values are possible for the sample mean? Are some values of the sample mean more likely than others?\n\nStep 5: Click the Animated button several more times. Each time, watch how a new random sample of N=5 data points is generated for the “Sample Data” graph, and then the mean of that sample is plotted in the bottom “Distribution of Means, N=5” graph. The following image shows the results after clicking the “Animated” button 10 times.\n\n\n\nStep 6: Now, let’s get the number of sample means (blue boxes) up to a million! To save time, there are buttons below the “Animated” button labeled “5”, “10,000”, and “100,000”. Clicking the “100,000” button 10 times will quickly get us up to one million sample means. Keep in mind that by doing this, you are essentially clicking the “Animated” button 1,000,000 times! (To actually click the “Animated” button one million times it would take you roughly 26 days of constantly clicking the “Animated” button over and over again!)\n\nNow study the arrows in the image below. As you do, you should notice that to the left of the “Distribution of Means, N=5” graph, the mean of the distribution of sample means (which is equal to 16) and standard deviation of the distribution of sample means (which is equal to 2.24) are given. Compare these values to the mean and standard deviation shown for the Parent Population graph.\n\nHopefully you noticed that the “mean of the distribution of means” is equal to the “mean of the population.” The mean of the means is the mean! Let’s say that again. The mean of all possible sample means is equal to the mean of the population. In short, the mean of the means is the mean. As for the standard deviation though, notice that the “standard deviation of the sample means” is smaller than the “standard deviation of the population.” It turns out that these two results hold true for any parent population and any sample size. There are two mathematical formulas that summarize these truths.\n\nThe Mean of the Sampling Distribution of the Sample Mean\n\\[\n\\underbrace{\\mu_{\\bar{x}}}_\\text{The mean of the sample means} = \\underbrace{\\mu}_\\text{The mean of the population}\n\\]\nThe Standard Deviation of the Sampling Distribution of the Sample Mean\n\\[\n\\underbrace{\\sigma_{\\bar{x}}}_\\text{The st. dev. of the sample means} = \\frac{\\overbrace{\\sigma}^\\text{The st. dev. of the population}}{\\underbrace{\\sqrt{n}}_\\text{sample size of each sample}}\n\\]\n\nThe Shape of the Sampling Distribution of the Sample Mean\nNotice that the shape of the sampling distribution of the sample mean is normal in the experiment we just performed in the above applet. This is an important result because we will be able to use the Normal Probability Applet to make probability calculations about sample means whenever the distribution of sample means is normal. Unfortunately, there are some situations where the distribution of sample means is not normal. This ruins our ability to make inference about the parent population. Let’s look at one such situation.\n\nStep 7: Press the [Clear Lower 3] button. This will erase your data and reset the applet. Then, use the drop-down menu below the [Clear Lower 3] button to change the distribution from a normal distribution to a skewed distribution. The applet should now show a right-skewed distribution for the “Parent Population” data graph.\n\n\n\nStep 8: Change the sample size to a very small sample size by selecting “N=2”. Then press the “Animated” button a few times to get a feel for what a sample of N=2 data points looks like from this right-skewed population.\n\n\n\nStep 9: Then, get a million such samples by clicking the “100,000” button ten times. You should get something similar to what is shown below. (Notice that the mean of the “Distribution of Means” is still equal to the mean of the “Parent population.” Also note that the standard deviation of the “Distribution of Means” is equal to \\(6.22/\\sqrt(2) = 4.40\\), which is \\(\\sigma/\\sqrt(n)\\), as shown by our formula stated previously for the “Standard deviation of the Sample Means.”)\n\n\nHowever, when it comes to the shape of the distribution of means, you should notice that the blue graph showing the “Distribution of Means, N=2” is not normally distributed. Like the distribution of the “Parent population” it is also right-skewed. This is problematic when it comes to trying to perform statistical analysis on a small sample (like N=2) from a population of skewed data because we can’t use the normal probability applet for things that are skewed. But there is a beautiful promise in statistical theory called the Central Limit Theorem that solves this problem. All we need is a larger sample size and the skewed distribution of means will magically “go away.” We will state this result formally in a moment. For now, see it happen yourself by doing the following.\n\nStep 10: Change the sample size from “N=2” (a small sample) to “N=25” (a much larger sample). Then, click the “Animated” button a few times to get a feel for what is happening. Finally, click the “100,000” button ten times to get a million sample means from a million different samples of “N=25” data points in each sample. Look at how beautifully normal the distribution of sample means (the blue graph) is becoming! The “Parent population” data is still skewed. Even the “Sample Data” is still skewed. But the “Distribution of Means” is becoming normal! This is the Central Limit Theorem in action.\n\n\n\n\n\nWhat is the Sampling Distribution of Sample Mean?\nThe sampling distribution of the sample mean is the set of all possible values of \\(\\bar x\\) that could occur. You have seen several examples of sampling distributions as you have plotted many means in the simulations and observed the approximately normal distribution that occurs. In the real world, you only observe your sample mean. You do not get to view the distribution. However, the fact that you sample randomly means that you could easily have drawn a different sample and had a different sample mean, \\(\\bar x\\). There are many possible sample means!"
  },
  {
    "objectID": "4-Foundations_Statistical_Inference/05-The_Central_Limit_Theorem.html#the-central-limit-theorem",
    "href": "4-Foundations_Statistical_Inference/05-The_Central_Limit_Theorem.html#the-central-limit-theorem",
    "title": "The Distribution of Sample Means (Reading)",
    "section": "The Central Limit Theorem",
    "text": "The Central Limit Theorem\n\nThe Central Limit Theorem states that the sampling distribution of the sample mean will be approximately normal if the sample size \\(n\\) of a sample is sufficiently large.\n\nIn the study of DDE levels in the South African women, we never saw the distribution of sample means. We only observed one sample mean, \\(\\bar x\\). Even though we do not get to see the distribution of all possible sample means, since the sample size (\\(n = 45\\)) was large, we can be assured that the sample mean \\(\\bar x\\) can be considered as one drawn from a normally distributed population of possible sample means. This allows us to make powerful statistical inference about the population.\nYou have observed in the simulations that if the sample size is large, the random variable \\(\\bar x\\) will be approximately normally distributed. In other words, the sampling distribution of the sample mean will be approximately normal if the sample size is sufficiently large. This important result is called the Central Limit Theorem. This is arguably the most important concept in all of Statistics.\nThe Central Limit Theorem works for every population where the standard deviation is defined (is not infinite). In other words, it will work for any distribution you find in the real world.\n\n\nWhen is \\(\\bar x\\) Normal?\nHow many observations are required so that the Central Limit Theorem will assure that the distribution of sample means will be approximately normal? The answer is, “it depends.”\n\nIf the parent population is normal, then the sampling distribution of \\(\\bar x\\) will always be normally distributed, no matter how many observations are selected.\nIf the parent population is not normal, then the sampling distribution \\(\\bar x\\) will be approximately normally distributed, if the sample size is large enough.\n\nIf the parent population is almost normal (e.g. mound-shaped and nearly symmetrical), then a sample of size n = 5 will probably be sufficient to assure that \\(\\bar x\\) will be approximately normally distributed.\nIf the parent population is heavily skewed, then it will require a larger sample size to be assured that \\(\\bar x\\) will be normally distributed. For most moderately skewed distributions, a sample size of around 30 is traditionally thought to be sufficiently large to assure that \\(\\bar x\\) will be approximately normally distributed. This is not a definitive number but is a rule of thumb.\nFor tremendously skewed distributions (e.g., the distribution of lottery payouts), a much larger sample will be required before the distribution of sample means is approximately normal. This may require billions of observations. Simulation can be used to determine if a particular sample size is sufficient. For this course, if the sample size is at least 30, we will conclude that the sampling distribution of \\(\\bar x\\) will be approximately normal.\n\n\n\n\nAnswer the following question:\n\n\n\nThere are two ways that \\(\\bar x\\) can be (approximately) normally distributed. What are they?\n\n\n\nShow/Hide Solution\n\n\n\\(\\bar x\\) will be normally distributed if the data were drawn from a normal population.\n\\(\\bar x\\) will be approximately normally distributed if the sample size is sufficiently large."
  },
  {
    "objectID": "4-Foundations_Statistical_Inference/05-The_Central_Limit_Theorem.html#mean-and-standard-deviation-of-the-distribution-of-sample-means",
    "href": "4-Foundations_Statistical_Inference/05-The_Central_Limit_Theorem.html#mean-and-standard-deviation-of-the-distribution-of-sample-means",
    "title": "The Distribution of Sample Means (Reading)",
    "section": "Mean and Standard Deviation of the Distribution of Sample Means",
    "text": "Mean and Standard Deviation of the Distribution of Sample Means\nThe following facts are always true. They do not depend on the Central Limit Theorem. They do not depend on the sample size. These facts hold for the sample mean \\(\\bar x\\) of any simple random sample of size \\(n\\) drawn from a population with mean \\(\\mu\\) and standard deviation \\(\\sigma\\).\n\nThe mean of the sample means is \\(\\mu\\)\nThe standard deviation of the sample means is \\(\\sigma / \\sqrt{n}\\)\n\nRemember, these facts are always true. They do not depend on normality or the sample size. So, even though these facts are often used in conjunction with the Central Limit Theorem, they do not depend on it.\nThink about the simulations you have observed. The mean of the sample means was always aligned with \\(\\mu\\). Also, if you took samples larger than \\(n = 1\\), the standard deviation of sample means was always smaller than \\(\\sigma\\).\n\nAnswer the following questions:\n\n\n\nThe amount of time passengers spend waiting for a bus on a particular urban route follows a distribution that has a mean of 8.7 minutes with a standard deviation of 2.2 minutes. Transportation officials observed the waiting times for a random sample of \\(n=121\\) individual passengers and recorded the sample mean, \\(\\bar x\\). We can think of this sample mean as one value observed out of all the possible sample means that could have been observed. What is the mean of the distribution of all possible sample means?\n\n\n\nShow/Hide Solution\n\n\\[\n\\mu = 8.7~\\text{minutes}\n\\]\n\n\nUse the information in the previous problem to answer this question: What is the standard deviation of the distribution of all possible sample means?\n\n\n\nShow/Hide Solution\n\n\\[\n\\frac{\\sigma}{\\sqrt{n}} = \\frac{2.2}{\\sqrt{121}} = 0.2~\\text{minutes}\n\\]"
  },
  {
    "objectID": "4-Foundations_Statistical_Inference/05-The_Central_Limit_Theorem.html#the-law-of-large-numbers",
    "href": "4-Foundations_Statistical_Inference/05-The_Central_Limit_Theorem.html#the-law-of-large-numbers",
    "title": "The Distribution of Sample Means (Reading)",
    "section": "The Law of Large Numbers",
    "text": "The Law of Large Numbers\n\nThe Law of Large Numbers states that the sample mean, \\(\\bar{x}\\), will become closer to \\(\\mu\\) as the sample size \\(n\\) becomes larger.\n\n\nAnswer the following questions:\n\n\n\nWe just learned that the standard deviation of sample means is \\(\\displaystyle{ \\sigma \\over \\sqrt{n} }\\). What happens to the standard deviation of sample means when the sample size is increased?\n\n\n\nShow/Hide Solution\n\n\nIf the sample size, \\(n\\), is increased, then the standard deviation of the sample mean will decrease. The fraction will get smaller.\n\n\n\nIf the standard deviation of the sample mean gets smaller, what happens to the values of \\(\\bar x\\)?\n\n\n\nShow/Hide Solution\n\n\nThe values that will be observed will be very close to each other and therefore close to \\(\\mu\\), if the sample size is large.\n\n\n\nThe result you have discovered in the previous two questions is called the Law of Large Numbers. The Law of Large Numbers states that if the sample size is large, then the sample mean will typically be close to the population mean, \\(\\mu\\). This happens because the standard deviation \\(\\sigma / \\sqrt{n}\\) will get smaller as the sample size \\(n\\) increases.\nNotice that this is very different from the Central Limit Theorem. The Central Limit Theorem is a statement about the DISTRIBUTION of sample means, that if the sample size is large that \\(\\bar x\\) will be approximately normal. The Law of Large numbers states that the sample mean \\(\\bar x\\) will be close to \\(\\mu\\).\nTake a moment to study the difference between the Central Limit Theorem and the Law of Large Numbers. They are very different, but it is easy to mix them up when you are first learning about them."
  },
  {
    "objectID": "4-Foundations_Statistical_Inference/05-The_Central_Limit_Theorem.html#review-of-key-concepts",
    "href": "4-Foundations_Statistical_Inference/05-The_Central_Limit_Theorem.html#review-of-key-concepts",
    "title": "The Distribution of Sample Means (Reading)",
    "section": "Review of Key Concepts",
    "text": "Review of Key Concepts\nIn order to “do statistics” we need to compute probabilities. This requires we know the distribution of the population of interest. The most important distribution in this course is the normal distribution. In this lesson, we are interested in another very important distribution: the distribution of all the sample means, \\(\\bar{x}\\). In order to do statistics with \\(\\bar{x}\\) we need the sampling distribution of \\(\\bar{x}\\) to be normal. There are two situations when the distribution of \\(\\bar{x}\\) is guaranteed to be normal (or at least very close to normal). They are:\n\nIf the parent population is normal, the distribution of the sample means \\(\\bar{x}\\) will be normal, for every sample size \\(n\\).\nEven if the parent population is not normal, the Central Limit Theorem guarantees that the distribution of the sample mean \\(\\bar{x}\\) will be approximately normal if the sample size \\(n\\) is large enough. For this course, if \\(n \\geq 30\\), we will say the distribution of the sample means will be approximately normal (even if the parent population is not normal).\n\nThe mean and standard deviation of \\(\\bar{x}\\) are:\n\nThe mean \\(\\mu_{\\bar{x}}\\) of the sample means is the population mean \\(\\mu\\).\nThe standard deviation \\(\\sigma_{\\bar{x}}\\) of the sample means is the population standard deviation \\(\\sigma\\) divided by the square root of \\(n\\), \\(\\sigma / \\sqrt{n}\\)"
  },
  {
    "objectID": "4-Foundations_Statistical_Inference/05-The_Central_Limit_Theorem.html#summary",
    "href": "4-Foundations_Statistical_Inference/05-The_Central_Limit_Theorem.html#summary",
    "title": "The Distribution of Sample Means (Reading)",
    "section": "Summary",
    "text": "Summary\n\nRemember…\n\n\n\nThe distribution of sample means is a distribution of all possible sample means (\\(\\bar x\\)) for a particular sample size.\nThe Central Limit Theorem states that the sampling distribution of the sample mean will be approximately normal if the sample size \\(n\\) of a sample is sufficiently large. In this class, \\(n\\ge 30\\) is considered to be sufficiently large.\nThe mean of the distribution of sample means is the mean \\(\\mu\\) of the population: \\(\\mu_{\\bar{x}} = \\mu\\).\nThe standard deviation of the distribution of sample means is the standard deviation \\(\\sigma\\) of the population divided by the square root of \\(n\\): \\(\\sigma_{\\bar{x}} = \\sigma/\\sqrt{n}\\).\nThe distribution of sample means is normal in either of two situations: (1) when the data is normally distributed or (2) when, thanks to the Central Limit Theorem (CLT), our sample size (\\(n\\)) is large.\nThe Law of Large Numbers states that as the sample size (\\(n\\)) gets larger, the sample mean (\\(\\bar x\\)) will get closer to the population mean (\\(\\mu\\)). This can be seen in the equation for \\(\\sigma_{\\bar{x}} = \\sigma/\\sqrt{n}\\). Notice as \\(n\\) increases, then \\(\\sigma_\\bar{x}\\) will get smaller."
  },
  {
    "objectID": "4-Foundations_Statistical_Inference/03-Normal_Distribution_Workbook.html",
    "href": "4-Foundations_Statistical_Inference/03-Normal_Distribution_Workbook.html",
    "title": "The Normal Distribution (Class)",
    "section": "",
    "text": "In your reading, you learned about the normal distribution which is a probability model that can calculate probabilities for certain types of events that follow a normal distribution."
  },
  {
    "objectID": "4-Foundations_Statistical_Inference/03-Normal_Distribution_Workbook.html#the-math",
    "href": "4-Foundations_Statistical_Inference/03-Normal_Distribution_Workbook.html#the-math",
    "title": "The Normal Distribution (Class)",
    "section": "The Math",
    "text": "The Math\nWe can calculate a Z-score using the formula\n\\[Z = \\frac{x - \\mu}{\\sigma} \\]\nwhere \\(\\mu\\) is the mean of the normal distribution and \\(\\sigma\\) is the standard deviation.\nNOTE: Z-scores follow what is called a standard normal distribution which means it is centered at \\(\\mu = 0\\) with a standard deviation \\(\\sigma=1\\).\nRecall that to get an area under the curve, we need the calculus. Fortunately, we have computers to do the heavy lifting for us."
  },
  {
    "objectID": "4-Foundations_Statistical_Inference/03-Normal_Distribution_Workbook.html#pnorm",
    "href": "4-Foundations_Statistical_Inference/03-Normal_Distribution_Workbook.html#pnorm",
    "title": "The Normal Distribution (Class)",
    "section": "pnorm()",
    "text": "pnorm()\nWe will use the function pnorm() to calculate the areas under the curve for specified values. The p in pnorm() stands for probability and norm obviously stands for the normal distribution.\nThe pnorm(x, mu, sigma) function takes the value, x, we wish to evaluate, the mean, \\(\\mu\\), and standard deviation, \\(\\sigma\\). We can use pnorm() in the original units of the data and put in \\(\\mu\\) and \\(\\sigma\\). Sticking with our example of Stanford admissions, we can calculate the probability of getting a value greater than 33.\nBy default, pnorm() gives the area to the LEFT of the given value. If we want the area to the right, we can use the lower.tail = FALSE or equivalently 1-pnorm(33,21,4).\n\n1-pnorm(33,21,4)\n\n[1] 0.001349898\n\n# Equivalently:\npnorm(33, 21,4, lower.tail = FALSE)\n\n[1] 0.001349898\n\n\nEquivalently, we can put the z-score into pnorm()\nIt’s easy to create a calculator in R that will calculate Z for us and probabilities automatically.\nNOTE: Z is what we call the Standard Normal Distribution. It has a mean of 0 and a SD of 1. If X is normally distributed, subtracting the mean and dividing by the standard deviation gives the standard normal distribution with a mean of 0 and an SD = 1.\n\n# To calculate a z-score, you can use R like a calculator:\n\nx &lt;- 33\nmu &lt;- 21\nstdev &lt;- 4\nz &lt;- (x-mu) / stdev\n\nz\n\n[1] 3\n\n# By default, pnorm() gives us the area to the LEFT of our observation\n\npnorm(x, mean = mu, sd = stdev)\n\n[1] 0.9986501\n\n1 - pnorm(x, mean = mu, sd = stdev)\n\n[1] 0.001349898\n\n# Equivalently:  \n\npnorm(z) ## Left Tail\n\n[1] 0.9986501\n\n1-pnorm(z) ## Right Tail\n\n[1] 0.001349898"
  },
  {
    "objectID": "4-Foundations_Statistical_Inference/03-Normal_Distribution_Workbook.html#iq-scores",
    "href": "4-Foundations_Statistical_Inference/03-Normal_Distribution_Workbook.html#iq-scores",
    "title": "The Normal Distribution (Class)",
    "section": "IQ Scores",
    "text": "IQ Scores\nIf IQ is normally distributed with a mean of 100 and a Standard Deviation of 11, what’s the probability of a randomly selected person having an IQ GREATER than 127?\nWhat about the probability of a randomly selected person having an IQ LESS than 85?\n\nx &lt;- \nmu &lt;- \nstdev &lt;- \n\nz &lt;- (x-mu) / stdev\n  \npnorm(z)  # LESS THAN\n\n[1] 0.9986501\n\n1-pnorm(z) # GREATER THAN\n\n[1] 0.001349898"
  },
  {
    "objectID": "4-Foundations_Statistical_Inference/01-Intro_to_Probability.html",
    "href": "4-Foundations_Statistical_Inference/01-Intro_to_Probability.html",
    "title": "Intro to Probability (Reading)",
    "section": "",
    "text": "Probability is a way of numerically quantifying how likely an event is to happen or not happen. The following historical account demonstrates this idea and shows how fractions (like 1/2 or 3/4) or percentages (like 50% or 75%) can be used to represent probabilities.\n\n\n\nOn August 3, 1492, Columbus set sail from Spain for his intended destination: the Indies (Caso, Adolph 1990). He was on the Santa Maria, which had a crew of approximately 41 men (“Cristobal colon” 1991; “Christopher Columbus”). Several other men were aboard the Nina and the Pinta (“Cristobal colon” 1991). On October 12, he landed on an island in the Bahamas he called San Salvador.\nThe return trip was not without challenges. The Santa Maria ran aground on Christmas Day, 1492, and was abandoned on the island we now call Hispaniola (home to Haiti and the Dominican Republic). Following this incident, Columbus sailed for Spain. Severe storms made the journey difficult. A particularly bad storm on February 14, 1493 made the crew fear for their lives. By morning, the storm was even worse!\nRecognizing his dependence upon God, Columbus ordered that a pilgrimage should be made to a particular shrine upon their safe arrival in Spain. He decided that they would use random chance to determine who would make the pilgrimage. They took one chick pea for each man on board. A knife was used to mark one of the chick peas with a cross. The chick peas were placed in a hat and shaken up. Each man was to draw a chick pea, and the one who had the cross would make the pilgrimage.\n“The first who put in his hand was [Columbus,] and he drew out the bean with a cross, so the lot fell on him; and he was bound to go on the pilgrimage and fulfil the vow” (Caso, Adolph 1990).\n\nAnswer the following questions:\n\n\n\nRemember, there were 41 men aboard his ship. What is the probability that Columbus would draw out the marked chick pea? Express your answer as a fraction, and then convert it to a decimal.\n\n\n\nShow/Hide Solution\n\nThere is only one marked chick pea in the hat, out of 41 chick peas total. Out of is expressed arithmetically by division. The probability is \\(\\frac{1}{41} = 0.0244\\). (Note: this is about 2%.)\n\n\nBased on your answer to the previous question, how likely is it that Columbus would draw out the marked chick pea?\n\n\n\nShow/Hide Solution\n\n\nThere is only about a 2% chance that Columbus will draw out the marked Chick Pea. This is not very likely.\n\n\n\n\nA Second Drawing\nColumbus’ promise to make the pilgrimage did not stop the storm. It was determined that there should be a pilgrimage to another site they held sacred. Again, chick peas representing each member of the crew were placed in a hat and shaken up. The lot fell on a sailor…named Pedro de Villa (Caso, Adolph 1990).\n\nAnswer the following questions:\n\n\n\nWhat is the probability that Columbus would not draw out the marked chick pea? Express your answer as a fraction, and then covert it to a decimal?\n\n\n\nShow/Hide Solution\n\n\nThere are 40 other men on board plus Columbus. So, the probability that Columbus would not draw out the marked chick pea is: \\(\\frac{40}{41} = 0.9756\\). (Note: this is almost 98%.)\n\n\n\nBased on your answer to the previous question, how likely is it that Columbus would not draw out the marked chick pea?\n\n\n\nShow/Hide Solution\n\n\nIt is very likely that Columbus would not draw out the marked chick pea. This result is not surprising.\n\n\n\nIn this second drawing, either Columbus would draw out the marked chick pea, or he would not. Add the probability that Columbus would draw out the marked chick pea and the probability that he would not draw out the marked chick pea. What is the value of this sum?\n\n\n\nShow/Hide Solution\n\n\nThe sum of the probabilities is 1:\n\n\\[\n\\frac{1}{41} + \\frac{40}{41} = \\frac{41}{41} = 1\n\\]\n\n\n\nAdditional Drawings\nAfter the drawing in which Pedro de Villa was chosen to make a pilgrimage, two additional drawings were held. In both cases, Columbus drew out the marked chick pea (Caso, Adolph 1990). In all, Christopher Columbus drew the marked chick pea in three of the four drawings. It can be shown that the probability that this would occur due to chance is very small: 0.0000566.\nBonus material. Read only if you are interested.\nThis calculation is more involved than the calculations you will be required to make in this course this semester. But if you are still interested, read on.\nIn each individual drawing, there was a 1/41 chance of Columbus getting the marked chick pea. Similarly, there was a 40/41 chance of not getting it. Since there were four drawings total, and the goal is to measure the probability of “three of those drawings” resulting in Columbus getting the marked chick pea, it becomes important to think about all of the orders in which Columbus could have gotten 3 out of 4.\n\n\n\n\n\n\n\n\n\n\nPossible Outcome\nFirst Drawing\nSecond Drawing\nThird Drawing\nFourth Drawing\n\n\n\n\nWhat actually happened…\nGot it.\nDidn’t get it.\nGot it.\nGot it.\n\n\nBut he could have…\nGot it.\nGot it.\nDidn’t get it.\nGot it.\n\n\nOr he could have…\nGot it.\nGot it.\nGot it.\nDidn’t get it.\n\n\nOr he could have…\nDidn’t get it.\nGot it.\nGot it.\nGot it.\n\n\n\nIn each of the above cases, notice that Columbus would have gotten the marked chick pea a total of 3 out of 4 times. So, this tells us there are four diffent ways to get the chick pea 3 out of 4 times.\nThe probability of what actually happened to Columbus in the order in which it happened would be computed by multiplying the individual probabilities of each drawing together.\n\\[\n  \\frac{1}{41} \\cdot \\frac{40}{41} \\cdot \\frac{1}{41} \\cdot \\frac{1}{41} \\approx 0.00001415548\n\\]\nBut then, we must also add to this the other “possible” scenarios that would also lead to getting the chick pea 3 out of 4 times, but as shown below, because multiplication is commutative (the order doesn’t matter) these “different” situations result in the same probability as the first.\n\\[\n  \\frac{1}{41} \\cdot \\frac{1}{41} \\cdot \\frac{40}{41} \\cdot \\frac{1}{41} \\approx 0.00001415548\n\\]\n\\[\n  \\frac{1}{41} \\cdot \\frac{1}{41} \\cdot \\frac{1}{41} \\cdot \\frac{40}{41} \\approx 0.00001415548\n\\]\n\\[\n  \\frac{40}{41} \\cdot \\frac{1}{41} \\cdot \\frac{1}{41} \\cdot \\frac{1}{41} \\approx 0.00001415548\n\\]\nThus, all that is needed is to multiply the first probability of roughly 0.00001415548 by 4 to get \\(0.00001415548 \\cdot 4 = 0.00005662192\\).\nEnd of Bonus Material.\nTo put some perspective on this, a high school athlete in the United States is over 26 times more likely to play professional sports than Columbus was to draw the three marked peas! (Fields, Mike 2008) Consider how you might explain the occurrence of this extremely unlikely event. (While no response is required of you right now, this kind of question will be very important later, so take a little time to ponder it.) In fact, it is worth restating the question, “How might you explain the occurrence of this extremely unlikely event?”\nNow, take a moment to practice what you have read by answering the following questions.\n\nAnswer the following questions:\n\n\n\nIf a fair, six-sided die* is rolled, what is the probability of rolling a 6?\n\n\n\nShow/Hide Solution\n\n\nThe probability of rolling a 6 on a die is \\(\\displaystyle{ \\frac{1}{6}} = 0.1667\\). This is because a six-sided die has 6 sides total and only 1 side that has a “6” on it. Thus, 1/6 represents the chance of getting a “six” divided by the “total number of possibilities on the die” in the denominator (6).\n\n\n\nIf a fair, six-sided die is rolled, what is the probability of not rolling a 6?\n\n\n\nShow/Hide Solution\n\n\nThe probability of not rolling a 6 on a die is \\(\\displaystyle{\\frac{5}{6}} = 0.8333.\\) This is because there are five sides on the die that are “not a 6” (the sides representing 1, 2, 3, 4, and 5) and six total sides, so 5 out of 6 sides will yield something other than a “six.”\n\n\n\nWhen a die is rolled, what is the sum of the probability of rolling a 6 and the probability of not rolling a six?\n\n\n\nShow/Hide Solution\n\n\nThe probability of rolling a six is \\(\\displaystyle{\\frac{1}{6}}\\) and the probability of not rolling a six is \\(\\displaystyle{\\frac{5}{6}}\\). These things cannot happen at the same time, so the probability of either rolling a six or not rolling a six is \\[ \\frac{1}{6} + \\frac{5}{6} = \\frac{6}{6} = 1 \\]\n\n*The only possible outcomes in this case are that you either roll a six or that you do not roll a six. The probability that one of these will happen is 1. If we list all the possible outcomes, the probability that at least one of them will occur is 1.\n\n\nIn general, if we know the probability that a particular outcome will occur, how could we calculate the probability that it will not occur?\n\n\n\nShow/Hide Solution\n\n\nIf we know the probability that an event will occur, we can subtract this probability from 1 to find the probability that the event will not occur. This is because the sum of the probabilities that the event will occur and that the event will not occur must be 1.\n\n\n\\(*\\)Note: The word “die” is the singular form of the word “dice.” When we refer to a die, we are talking about a fair, six-sided die.\n\n\n\n\n\nYou may already have a good understanding of the basics of probability. It is worth noting that there is a special notation used to denote probabilities. The probability that an event, \\(x\\), will occur is written \\(P(x)\\) and pronounced as “probability-of-event-x.” As an example, the probability that you will roll a 6 on a fair six-sided die can be written as\n\\[\nP\\text{(Roll a \"six\" on a fair six-sided die)}= \\frac{1}{6} = \\frac{\\text{number of sides that show a \"six\"}}{\\text{total number of sides on the die}}\n\\]\n\n\n\nProbabilities follow patterns, called probability distributions, or just distributions, for short. There are three rules that a probability distribution must follow. Answer the following questions to explore what these three rules might be.\n\nAnswer the following questions:\n\n\n\nAs an answer to a homework problem, a student reported, The probability of finding life on Mars is -0.35. What is wrong with this statement?\n\n\n\nShow/Hide Solution\n\n\nProbabilities cannot be negative because they represent the “number of ways something specific can happen” (like how many planets that are “just like” Mars and do have life on them) divided by “the total number of possibilities” (in this case, the total number of planets that exist that are “just like” Mars). The fewest planets there could be that are “just like” Mars and have life on them is zero. So the lowest a probability can go is zero.\n\n\n\nA student in an introductory statistics class wrote the following statement on an exam: The probability that the event will occur is 1.25 (i.e. 125%). What is wrong with this statement?\n\n\n\nShow/Hide Solution\n\n\nProbabilities cannot be larger than 1 (or 100%). This is because probabilities represent frequencies of occurrence, and the most something can happen is “all the time” or 100% of the time.\n\n\n\nA student estimated that the probability that he would finish his homework is 0.45 (i.e., 45%), and the probability that he would not finish his homework is 0.35 (i.e., 35%). What is wrong with this student’s statement?\n\n\n\nShow/Hide Solution\n\nThis can be viewed as one of two problems:\n\nThe probabilities for all the events do not add up to 1 (or 100%.)\nThe probability that he does not finish his homework is actually 1 minus the probability that he will finish his homework or 0.55 (i.e., 55%).\n\n\n\n\nIn this course we are interested in experiments where the outcomes of the experiment are uncertain, yet they follow a pattern or probabilitiy distribution. As you read in the above questions and answers, these probability distributions follow three rules.\n\n  The three rules of probability are:\n\nRule 1: The probability of an event \\(X\\) is a number between 0 and 1.\n\n\\[0 \\leq P(X) \\leq 1\\]\n\nRule 2: If you list all the outcomes of an experiment (such as rolling a die) the probability that one of these outcomes will occur is 1. In other words, the sum of the probabilities of all the possible outcomes of any experiment is 1.\n\n\\[\\sum P(X) = 1\\]\n\nRule 3: (Complement Rule) The probability that an event \\(X\\) will not occur is 1 minus the probability that it will occur.\n\n\\[P(\\text{not}~X) = 1 - P(X)\\]\n  You may have noticed that the Complement Rule is just a combination of the first two rules.\n\n\n\nAnswer the following questions:\n\n\n\nWhich of the probability rules was violated by the statement in Question 10?\n\n\n\nShow/Hide Solution\n\n\nRule 1\n\n\n\nWhich of the probability rules was violated by the statement in Question 11?\n\n\n\nShow/Hide Solution\n\n\nRule 1\n\n\n\nWhich of the probability rules was violated by the statement in Question 12?\n\n\n\nShow/Hide Solution\n\n\nRule 2 or Rule 3\n\n\n\n\nInformally, a distribution can be thought of as being “all the possible outcomes of an experiment and how often they occur.”\n\n\n\nA BYU-Idaho student was overhead saying, “I went shopping and bought some random items.” Did the person actually take a random sample of the items at the store? Did they write all the items down and randomly select the items for purchase? Of course not!\nWhat did the student mean? That the items they bought seemed unrelated. When we consciously or subconsciously choose a sample, it is not random.\nSo, what does it mean to be random? When something is random, it is not just haphazard, with no pattern. A random process follows a very distinct pattern over time—the distribution of its outcomes. For example, if you roll a die thousands of times, about one-sixth of the time you will roll a four. This is a very clear pattern, or part of a pattern. The entire pattern (or, the entire distribution) is that each number on the die is rolled about one-sixth of the time.\nBut there’s something different about the patterns followed by random processes than other kinds of patterns. Other kinds of patterns can be very predictable, such as a color pattern of the red, yellow, blue, red, yellow, blue, and so on. If you’re following this pattern and happen to see yellow, you know the next color will be blue. By contrast, you never know what you will get on the next roll of a six-sided die. You do know that in the long run you will roll fours about one-sixth of the time.\nWhen something is random, we can be sure that it follows a long-term pattern. This long-term pattern is called its probability distribution. However, what makes “randomness” interesting is that despite knowing the long-term pattern, or how often something will occur over time, we still never know what the outcome of the next experiment will be."
  },
  {
    "objectID": "4-Foundations_Statistical_Inference/01-Intro_to_Probability.html#probability",
    "href": "4-Foundations_Statistical_Inference/01-Intro_to_Probability.html#probability",
    "title": "Intro to Probability (Reading)",
    "section": "",
    "text": "Probability is a way of numerically quantifying how likely an event is to happen or not happen. The following historical account demonstrates this idea and shows how fractions (like 1/2 or 3/4) or percentages (like 50% or 75%) can be used to represent probabilities.\n\n\n\nOn August 3, 1492, Columbus set sail from Spain for his intended destination: the Indies (Caso, Adolph 1990). He was on the Santa Maria, which had a crew of approximately 41 men (“Cristobal colon” 1991; “Christopher Columbus”). Several other men were aboard the Nina and the Pinta (“Cristobal colon” 1991). On October 12, he landed on an island in the Bahamas he called San Salvador.\nThe return trip was not without challenges. The Santa Maria ran aground on Christmas Day, 1492, and was abandoned on the island we now call Hispaniola (home to Haiti and the Dominican Republic). Following this incident, Columbus sailed for Spain. Severe storms made the journey difficult. A particularly bad storm on February 14, 1493 made the crew fear for their lives. By morning, the storm was even worse!\nRecognizing his dependence upon God, Columbus ordered that a pilgrimage should be made to a particular shrine upon their safe arrival in Spain. He decided that they would use random chance to determine who would make the pilgrimage. They took one chick pea for each man on board. A knife was used to mark one of the chick peas with a cross. The chick peas were placed in a hat and shaken up. Each man was to draw a chick pea, and the one who had the cross would make the pilgrimage.\n“The first who put in his hand was [Columbus,] and he drew out the bean with a cross, so the lot fell on him; and he was bound to go on the pilgrimage and fulfil the vow” (Caso, Adolph 1990).\n\nAnswer the following questions:\n\n\n\nRemember, there were 41 men aboard his ship. What is the probability that Columbus would draw out the marked chick pea? Express your answer as a fraction, and then convert it to a decimal.\n\n\n\nShow/Hide Solution\n\nThere is only one marked chick pea in the hat, out of 41 chick peas total. Out of is expressed arithmetically by division. The probability is \\(\\frac{1}{41} = 0.0244\\). (Note: this is about 2%.)\n\n\nBased on your answer to the previous question, how likely is it that Columbus would draw out the marked chick pea?\n\n\n\nShow/Hide Solution\n\n\nThere is only about a 2% chance that Columbus will draw out the marked Chick Pea. This is not very likely.\n\n\n\n\nA Second Drawing\nColumbus’ promise to make the pilgrimage did not stop the storm. It was determined that there should be a pilgrimage to another site they held sacred. Again, chick peas representing each member of the crew were placed in a hat and shaken up. The lot fell on a sailor…named Pedro de Villa (Caso, Adolph 1990).\n\nAnswer the following questions:\n\n\n\nWhat is the probability that Columbus would not draw out the marked chick pea? Express your answer as a fraction, and then covert it to a decimal?\n\n\n\nShow/Hide Solution\n\n\nThere are 40 other men on board plus Columbus. So, the probability that Columbus would not draw out the marked chick pea is: \\(\\frac{40}{41} = 0.9756\\). (Note: this is almost 98%.)\n\n\n\nBased on your answer to the previous question, how likely is it that Columbus would not draw out the marked chick pea?\n\n\n\nShow/Hide Solution\n\n\nIt is very likely that Columbus would not draw out the marked chick pea. This result is not surprising.\n\n\n\nIn this second drawing, either Columbus would draw out the marked chick pea, or he would not. Add the probability that Columbus would draw out the marked chick pea and the probability that he would not draw out the marked chick pea. What is the value of this sum?\n\n\n\nShow/Hide Solution\n\n\nThe sum of the probabilities is 1:\n\n\\[\n\\frac{1}{41} + \\frac{40}{41} = \\frac{41}{41} = 1\n\\]\n\n\n\nAdditional Drawings\nAfter the drawing in which Pedro de Villa was chosen to make a pilgrimage, two additional drawings were held. In both cases, Columbus drew out the marked chick pea (Caso, Adolph 1990). In all, Christopher Columbus drew the marked chick pea in three of the four drawings. It can be shown that the probability that this would occur due to chance is very small: 0.0000566.\nBonus material. Read only if you are interested.\nThis calculation is more involved than the calculations you will be required to make in this course this semester. But if you are still interested, read on.\nIn each individual drawing, there was a 1/41 chance of Columbus getting the marked chick pea. Similarly, there was a 40/41 chance of not getting it. Since there were four drawings total, and the goal is to measure the probability of “three of those drawings” resulting in Columbus getting the marked chick pea, it becomes important to think about all of the orders in which Columbus could have gotten 3 out of 4.\n\n\n\n\n\n\n\n\n\n\nPossible Outcome\nFirst Drawing\nSecond Drawing\nThird Drawing\nFourth Drawing\n\n\n\n\nWhat actually happened…\nGot it.\nDidn’t get it.\nGot it.\nGot it.\n\n\nBut he could have…\nGot it.\nGot it.\nDidn’t get it.\nGot it.\n\n\nOr he could have…\nGot it.\nGot it.\nGot it.\nDidn’t get it.\n\n\nOr he could have…\nDidn’t get it.\nGot it.\nGot it.\nGot it.\n\n\n\nIn each of the above cases, notice that Columbus would have gotten the marked chick pea a total of 3 out of 4 times. So, this tells us there are four diffent ways to get the chick pea 3 out of 4 times.\nThe probability of what actually happened to Columbus in the order in which it happened would be computed by multiplying the individual probabilities of each drawing together.\n\\[\n  \\frac{1}{41} \\cdot \\frac{40}{41} \\cdot \\frac{1}{41} \\cdot \\frac{1}{41} \\approx 0.00001415548\n\\]\nBut then, we must also add to this the other “possible” scenarios that would also lead to getting the chick pea 3 out of 4 times, but as shown below, because multiplication is commutative (the order doesn’t matter) these “different” situations result in the same probability as the first.\n\\[\n  \\frac{1}{41} \\cdot \\frac{1}{41} \\cdot \\frac{40}{41} \\cdot \\frac{1}{41} \\approx 0.00001415548\n\\]\n\\[\n  \\frac{1}{41} \\cdot \\frac{1}{41} \\cdot \\frac{1}{41} \\cdot \\frac{40}{41} \\approx 0.00001415548\n\\]\n\\[\n  \\frac{40}{41} \\cdot \\frac{1}{41} \\cdot \\frac{1}{41} \\cdot \\frac{1}{41} \\approx 0.00001415548\n\\]\nThus, all that is needed is to multiply the first probability of roughly 0.00001415548 by 4 to get \\(0.00001415548 \\cdot 4 = 0.00005662192\\).\nEnd of Bonus Material.\nTo put some perspective on this, a high school athlete in the United States is over 26 times more likely to play professional sports than Columbus was to draw the three marked peas! (Fields, Mike 2008) Consider how you might explain the occurrence of this extremely unlikely event. (While no response is required of you right now, this kind of question will be very important later, so take a little time to ponder it.) In fact, it is worth restating the question, “How might you explain the occurrence of this extremely unlikely event?”\nNow, take a moment to practice what you have read by answering the following questions.\n\nAnswer the following questions:\n\n\n\nIf a fair, six-sided die* is rolled, what is the probability of rolling a 6?\n\n\n\nShow/Hide Solution\n\n\nThe probability of rolling a 6 on a die is \\(\\displaystyle{ \\frac{1}{6}} = 0.1667\\). This is because a six-sided die has 6 sides total and only 1 side that has a “6” on it. Thus, 1/6 represents the chance of getting a “six” divided by the “total number of possibilities on the die” in the denominator (6).\n\n\n\nIf a fair, six-sided die is rolled, what is the probability of not rolling a 6?\n\n\n\nShow/Hide Solution\n\n\nThe probability of not rolling a 6 on a die is \\(\\displaystyle{\\frac{5}{6}} = 0.8333.\\) This is because there are five sides on the die that are “not a 6” (the sides representing 1, 2, 3, 4, and 5) and six total sides, so 5 out of 6 sides will yield something other than a “six.”\n\n\n\nWhen a die is rolled, what is the sum of the probability of rolling a 6 and the probability of not rolling a six?\n\n\n\nShow/Hide Solution\n\n\nThe probability of rolling a six is \\(\\displaystyle{\\frac{1}{6}}\\) and the probability of not rolling a six is \\(\\displaystyle{\\frac{5}{6}}\\). These things cannot happen at the same time, so the probability of either rolling a six or not rolling a six is \\[ \\frac{1}{6} + \\frac{5}{6} = \\frac{6}{6} = 1 \\]\n\n*The only possible outcomes in this case are that you either roll a six or that you do not roll a six. The probability that one of these will happen is 1. If we list all the possible outcomes, the probability that at least one of them will occur is 1.\n\n\nIn general, if we know the probability that a particular outcome will occur, how could we calculate the probability that it will not occur?\n\n\n\nShow/Hide Solution\n\n\nIf we know the probability that an event will occur, we can subtract this probability from 1 to find the probability that the event will not occur. This is because the sum of the probabilities that the event will occur and that the event will not occur must be 1.\n\n\n\\(*\\)Note: The word “die” is the singular form of the word “dice.” When we refer to a die, we are talking about a fair, six-sided die.\n\n\n\n\n\nYou may already have a good understanding of the basics of probability. It is worth noting that there is a special notation used to denote probabilities. The probability that an event, \\(x\\), will occur is written \\(P(x)\\) and pronounced as “probability-of-event-x.” As an example, the probability that you will roll a 6 on a fair six-sided die can be written as\n\\[\nP\\text{(Roll a \"six\" on a fair six-sided die)}= \\frac{1}{6} = \\frac{\\text{number of sides that show a \"six\"}}{\\text{total number of sides on the die}}\n\\]\n\n\n\nProbabilities follow patterns, called probability distributions, or just distributions, for short. There are three rules that a probability distribution must follow. Answer the following questions to explore what these three rules might be.\n\nAnswer the following questions:\n\n\n\nAs an answer to a homework problem, a student reported, The probability of finding life on Mars is -0.35. What is wrong with this statement?\n\n\n\nShow/Hide Solution\n\n\nProbabilities cannot be negative because they represent the “number of ways something specific can happen” (like how many planets that are “just like” Mars and do have life on them) divided by “the total number of possibilities” (in this case, the total number of planets that exist that are “just like” Mars). The fewest planets there could be that are “just like” Mars and have life on them is zero. So the lowest a probability can go is zero.\n\n\n\nA student in an introductory statistics class wrote the following statement on an exam: The probability that the event will occur is 1.25 (i.e. 125%). What is wrong with this statement?\n\n\n\nShow/Hide Solution\n\n\nProbabilities cannot be larger than 1 (or 100%). This is because probabilities represent frequencies of occurrence, and the most something can happen is “all the time” or 100% of the time.\n\n\n\nA student estimated that the probability that he would finish his homework is 0.45 (i.e., 45%), and the probability that he would not finish his homework is 0.35 (i.e., 35%). What is wrong with this student’s statement?\n\n\n\nShow/Hide Solution\n\nThis can be viewed as one of two problems:\n\nThe probabilities for all the events do not add up to 1 (or 100%.)\nThe probability that he does not finish his homework is actually 1 minus the probability that he will finish his homework or 0.55 (i.e., 55%).\n\n\n\n\nIn this course we are interested in experiments where the outcomes of the experiment are uncertain, yet they follow a pattern or probabilitiy distribution. As you read in the above questions and answers, these probability distributions follow three rules.\n\n  The three rules of probability are:\n\nRule 1: The probability of an event \\(X\\) is a number between 0 and 1.\n\n\\[0 \\leq P(X) \\leq 1\\]\n\nRule 2: If you list all the outcomes of an experiment (such as rolling a die) the probability that one of these outcomes will occur is 1. In other words, the sum of the probabilities of all the possible outcomes of any experiment is 1.\n\n\\[\\sum P(X) = 1\\]\n\nRule 3: (Complement Rule) The probability that an event \\(X\\) will not occur is 1 minus the probability that it will occur.\n\n\\[P(\\text{not}~X) = 1 - P(X)\\]\n  You may have noticed that the Complement Rule is just a combination of the first two rules.\n\n\n\nAnswer the following questions:\n\n\n\nWhich of the probability rules was violated by the statement in Question 10?\n\n\n\nShow/Hide Solution\n\n\nRule 1\n\n\n\nWhich of the probability rules was violated by the statement in Question 11?\n\n\n\nShow/Hide Solution\n\n\nRule 1\n\n\n\nWhich of the probability rules was violated by the statement in Question 12?\n\n\n\nShow/Hide Solution\n\n\nRule 2 or Rule 3\n\n\n\n\nInformally, a distribution can be thought of as being “all the possible outcomes of an experiment and how often they occur.”\n\n\n\nA BYU-Idaho student was overhead saying, “I went shopping and bought some random items.” Did the person actually take a random sample of the items at the store? Did they write all the items down and randomly select the items for purchase? Of course not!\nWhat did the student mean? That the items they bought seemed unrelated. When we consciously or subconsciously choose a sample, it is not random.\nSo, what does it mean to be random? When something is random, it is not just haphazard, with no pattern. A random process follows a very distinct pattern over time—the distribution of its outcomes. For example, if you roll a die thousands of times, about one-sixth of the time you will roll a four. This is a very clear pattern, or part of a pattern. The entire pattern (or, the entire distribution) is that each number on the die is rolled about one-sixth of the time.\nBut there’s something different about the patterns followed by random processes than other kinds of patterns. Other kinds of patterns can be very predictable, such as a color pattern of the red, yellow, blue, red, yellow, blue, and so on. If you’re following this pattern and happen to see yellow, you know the next color will be blue. By contrast, you never know what you will get on the next roll of a six-sided die. You do know that in the long run you will roll fours about one-sixth of the time.\nWhen something is random, we can be sure that it follows a long-term pattern. This long-term pattern is called its probability distribution. However, what makes “randomness” interesting is that despite knowing the long-term pattern, or how often something will occur over time, we still never know what the outcome of the next experiment will be."
  },
  {
    "objectID": "4-Foundations_Statistical_Inference/01-Intro_to_Probability.html#conclusion",
    "href": "4-Foundations_Statistical_Inference/01-Intro_to_Probability.html#conclusion",
    "title": "Intro to Probability (Reading)",
    "section": "Conclusion",
    "text": "Conclusion\nAs with all the classes you take at BYU-Idaho, it is up to you to decide what you want to get out of this class. If you choose to approach the things you study in class with an open mind, if you prepare diligently and work hard to complete all the learning activities, and if you humbly seek the Lord’s help to understand the intellectual and spiritual truths discussed in this course and in other courses, you will have an outstanding educational experience that will be a blessing to you throughout your life. May you enjoy the journey this semester into statistics!"
  },
  {
    "objectID": "4-Foundations_Statistical_Inference/01-Intro_to_Probability.html#summary",
    "href": "4-Foundations_Statistical_Inference/01-Intro_to_Probability.html#summary",
    "title": "Intro to Probability (Reading)",
    "section": "Summary",
    "text": "Summary\n\nRemember…\n\n\n\nIn this class you will use the online textbook that has been written for you by your statistics teachers. All of the assignments and quizzes, available in I-Learn, will be based on the readings, so study it well. Most weeks will cover two lessons.\nYou have successfully located the online textbook. Ensure you have also located the course in I-Learn and can access the quizzes and assignments that are there.\nEnsure you have located the contact information for your instructor in the I-Learn course. Recording the contact information of peers from class would also be a wise idea.\nThis course uses R for all statistical analysis. Check that you have access to the software on your computer. If not, see I-Learn for details on how to obtain it through the University for free.\nBy doing the work, staying on schedule, and living the Honor Code you will succeed in this class.\nThe three rules of probability are:\n\nA probability is a number between 0 and 1. \\[0 \\leq P(X) \\leq 1\\]\nIf you list all the outcomes of a probability experiment (such as rolling a die) the probability that one of these outcomes will occur is 1. In other words, the sum of the probabilities in any probability is 1. \\[\\sum P(X) = 1\\]\nThe probability that an outcome will not occur is 1 minus the probability that it will occur. \\[P(\\text{not}~X) = 1 - P(X)\\]"
  },
  {
    "objectID": "3-Data_Wrangling_Visualization/Bonus_More_Tidy_Practice.html",
    "href": "3-Data_Wrangling_Visualization/Bonus_More_Tidy_Practice.html",
    "title": "Level up your Tidy-ness",
    "section": "",
    "text": "Putting it All Together\nHere are two more data wrangling questions to test your skills. Try as best you can to work each step on your own before checking solutions.\nThe questions relate to the High School survey used in other examples.\n\n# Load libraries and data\n\nlibrary(rio)\nlibrary(mosaic)\nlibrary(tidyverse)\nlibrary(car)\n\nsurvey &lt;- import('https://github.com/byuistats/Math221D_Cannon/raw/master/Data/HighSchoolSeniors_subset.csv') %&gt;% tibble()\n\n\n\nTelepaths, Gender and Sleep\nSuppose we want to see who gets more sleep on non-school nights, males or females whose chosen superpower would be telepathy. Also, create a column that is the ratio of sleep hours on non-school nights to 8. This calculates the percent of recommended sleep on non-school nights.\n\nCreate a dataset that includes columns Gender, Superpower, Sleep_Hours_Non-Schoolnight, and the ratio of non-schoolnight sleep hours divided by 8, for Males and Females who choose Telepathy as their superpower.\n\n\n# Your Code:\n\n\n\nSolution\n\n\nunique(survey$Superpower)\n\n[1] \"Telepathy\"      \"Invisibility\"   \"Fly\"            \"Freeze time\"   \n[5] \"Super strength\"\n\ntelepaths &lt;- survey %&gt;%\n  select(Gender, Superpower, Sleep_Hours_Non_Schoolnight) %&gt;%\n  filter(Superpower==\"Telepathy\") %&gt;%\n  mutate(\n    percent_of_recommended = Sleep_Hours_Non_Schoolnight / 8\n  )\n\ntelepaths\n\n# A tibble: 81 × 4\n   Gender Superpower Sleep_Hours_Non_Schoolnight percent_of_recommended\n   &lt;chr&gt;  &lt;chr&gt;                            &lt;dbl&gt;                  &lt;dbl&gt;\n 1 Male   Telepathy                            7                  0.875\n 2 Female Telepathy                            9                  1.12 \n 3 Male   Telepathy                            9                  1.12 \n 4 Female Telepathy                            8                  1    \n 5 Female Telepathy                            9                  1.12 \n 6 Male   Telepathy                           11                  1.38 \n 7 Female Telepathy                           11                  1.38 \n 8 Female Telepathy                            9                  1.12 \n 9 Female Telepathy                            9                  1.12 \n10 Female Telepathy                           10                  1.25 \n# ℹ 71 more rows\n\n\n\n\nCreate a summary table comparing males and females whose preferred super power is telepathy that includes:\n\na. Mean, standard deviation, and sample size of Sleep Hours on non-school nights \nb. Mean, standard deviation, and sample size of the percent of recommended sleep\nHINT: Use the unique() function to see what the options are for a given categorical variable.\n\n# Your Code\n\n\n\nSolution\n\n\ntelepaths %&gt;%\n  group_by(Gender) %&gt;%\n  summarise(\n    mn_hrs = mean(Sleep_Hours_Non_Schoolnight),\n    mn_percent_recommended = mean(percent_of_recommended),\n    count = n()\n  )\n\n# A tibble: 2 × 4\n  Gender mn_hrs mn_percent_recommended count\n  &lt;chr&gt;   &lt;dbl&gt;                  &lt;dbl&gt; &lt;int&gt;\n1 Female   8.73                   1.09    60\n2 Male     8.19                   1.02    21\n\n\n\n\n\nVegetarians and Height\n\nHow many vegetarians say meat is their favorite food?\n\nHINT: This can be done with a single filter statement\n\n# Your Code:\n\n\n\nSolution\n\n\nsurvey %&gt;%\n  filter(Favorite_Food == \"Meat\",\n         Vegetarian == \"Yes\")\n\n# A tibble: 1 × 60\n  Country Region DataYear ClassGrade Gender Ageyears Handed       Height_cm\n  &lt;chr&gt;   &lt;chr&gt;     &lt;int&gt;      &lt;int&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;            &lt;dbl&gt;\n1 USA     NC         2022         11 Male         16 Right-Handed       178\n# ℹ 52 more variables: Footlength_cm &lt;dbl&gt;, Armspan_cm &lt;dbl&gt;,\n#   Languages_spoken &lt;dbl&gt;, Travel_to_School &lt;chr&gt;,\n#   Travel_time_to_School &lt;int&gt;, Reaction_time &lt;dbl&gt;,\n#   Score_in_memory_game &lt;dbl&gt;, Favourite_physical_activity &lt;chr&gt;,\n#   Imprtance_reducing_pllutin &lt;int&gt;, Imprtance_recycling_rubbish &lt;int&gt;,\n#   Imprtance_cnserving_water &lt;int&gt;, Imprtance_saving_energy &lt;int&gt;,\n#   Imprtance_wning_cmputer &lt;int&gt;, Imprtance_Internet_access &lt;int&gt;, …\n\n\n\n\nCompare mean, and standard deviation of heights between those who are vegetarian and those who aren’t. Include the number of respondents in your analysis.\n\nBe sure to filter out any major outliers in heights first.\n\n# Your Code:\n\n\n\nSolution\n\n\nsurvey %&gt;%\n  select(Height_cm, Vegetarian) %&gt;%\n  filter(Height_cm &lt; 214,\n         Height_cm &gt; 100) %&gt;%\n  group_by(Vegetarian) %&gt;%\n  summarise(\n    med_ht = median(Height_cm),\n    mean_ht = mean(Height_cm),\n    sd_ht = sd(Height_cm),\n    count = n()\n  )\n\n# A tibble: 2 × 5\n  Vegetarian med_ht mean_ht sd_ht count\n  &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;\n1 No           170.    171.  10.8   285\n2 Yes          163     162.  17.1    15\n\n\nAfter removing outliers, it looks like vegetarians are shorter, on average.\n\n# Bonus Boxplot:\nveg &lt;- survey %&gt;%\n  select(Height_cm, Vegetarian) %&gt;%\n  filter(Height_cm &lt; 214,\n         Height_cm &gt; 100)\n\n\nboxplot(veg$Height_cm ~ veg$Vegetarian, col = c(5,6), main = \"Heights (cm) of Vegetarians and Non-Vegetarians\", xlab=\"vegetarian\", ylab = \"Height (cm)\")\n\n\n\n\n\n\n\n\n\n\nCreate a dataset that:\n\nIncludes a column that is percent of recommended sleep (Sleep_Hours_Schoolnight divided by 8 using a mutate statement)\nIncludes only columns for Favourite_physical_activity, Reaction_time, percent_recommended_sleep (part a)\nIncludes only students whose favorite physical activity is Walking/Hiking, Basketball, Swimming, Soccer\nFilters Reaction Times to be less than 1 second\n\n\n\n# Your Code:\n\n\n\nSolution\n\n\nphys_act &lt;- survey %&gt;%\n  mutate(\n    pct_recommended_sleep = Sleep_Hours_Schoolnight / 8\n  ) %&gt;%\n  filter(Favourite_physical_activity %in% c('Walking/Hiking', \"Basketball\", \"Swimming\", \"Soccer\"),\n         Reaction_time &lt; 1) %&gt;%\n  select(Favourite_physical_activity, Reaction_time, pct_recommended_sleep)\n\n\nUse the clean dataset to:\n\nCreate a side-by-side boxplot for the percent of recommended sleep comparing favourite physical activity\n\n\n# Your Code:\n\n\n\nSolution\n\n\nboxplot(phys_act$pct_recommended_sleep ~ phys_act$Favourite_physical_activity, xlab = \"Favorite Physical Activity\", ylab = \"% Recommended Sleep on School Nights\", main = \"School Night Sleep by Favorite Physical Activity\", col = c(2,3,4,5))\n\n\n\n\n\n\n\n\n\n\nCreate a side-by-side boxplot for the reaction times comparing favourite physical activity\n\n\n# Your Code:\n\n\n\nSolution\n\n\nboxplot(phys_act$Reaction_time ~ phys_act$Favourite_physical_activity, xlab = \"Favorite Physical Activity\", ylab = \"Reaction Time\", main = \"Reaction Time Results by Favorite Physical Activity\", col = c(2,3,4,5))\n\n\n\n\n\n\n\n\n\nWhich physical activity group has the quickest reaction time?"
  },
  {
    "objectID": "3-Data_Wrangling_Visualization/07-Unit1_Unit2_AA.html",
    "href": "3-Data_Wrangling_Visualization/07-Unit1_Unit2_AA.html",
    "title": "Data Wrangling - Application Activity",
    "section": "",
    "text": "The Big 5 personality test is the most widely accepted tool for modelling personality in academic psychology. It is based on decades of statistical analysis of personality descriptions across languages and cultures. The big 5 traits are:\n\nOpenness\nConscientiousness\nExtroversion\nAgreeableness\nNeuroticism\n\nBrother Cannon collected personality data on students for the past several semesters, including a few metrics that may be associated with personality traits.\nNOTE: Scores for personality traits are given in percentiles relative to the general population.\nIn this activity, you will practice the process for approaching a dataset outlined in class:\n\nLoad the data and libraries\nExplore the data and generate hypotheses\nPrepare the data for analysis\nPerform the appropriate analysis\n\nData preparation will include using the filter() function. For now, analysis means creating good visualizations that tell a story using ggplot() and base R."
  },
  {
    "objectID": "3-Data_Wrangling_Visualization/07-Unit1_Unit2_AA.html#extroversion",
    "href": "3-Data_Wrangling_Visualization/07-Unit1_Unit2_AA.html#extroversion",
    "title": "Data Wrangling - Application Activity",
    "section": "Extroversion",
    "text": "Extroversion\nQUESTION: Create a new dataset called extro that includes only columns for birth month and extroversion scores. Make sure it only has values that are real.\nHINT: Extroversion is measured in percentiles, and you should already know what the months of the year are.\n\nextro &lt;- \n\nError in parse(text = input): &lt;text&gt;:4:0: unexpected end of input\n2: extro &lt;- \n3: \n  ^\n\n\nQUESTION: USE GGPLOT to create a side-by-side boxplot of Extroversion scores for all birth months.\n\nggplot()\n\n\n\n\n\n\n\n\nQUESTION: Based on the boxplot, which month appears to be the least extroverted? Explain your reasoning.\nANSWER:\nQUESTION: Based on the boxplot, which month appears to be the most extroverted? Explain your reasoning.\nANSWER:"
  },
  {
    "objectID": "3-Data_Wrangling_Visualization/07-Unit1_Unit2_AA.html#neuroticism",
    "href": "3-Data_Wrangling_Visualization/07-Unit1_Unit2_AA.html#neuroticism",
    "title": "Data Wrangling - Application Activity",
    "section": "Neuroticism",
    "text": "Neuroticism\nQUESTION: Create a dataset called neuro that includes only the Section and Neuroticism columns:\n\nneuro &lt;- \n\nError in parse(text = input): &lt;text&gt;:4:0: unexpected end of input\n2: neuro &lt;- \n3:   \n  ^\n\n\nQUESTION: USE GGPLOT to create a side-by-side boxplot comparing Neuroticism for all the different sections:\nQUESTION: Based on the boxplot, which section appears to be the lowest in trait neuroticism? Explain your reasoning.\nANSWER:"
  },
  {
    "objectID": "3-Data_Wrangling_Visualization/05-Exploring_New_Data_with_Tidyverse.html",
    "href": "3-Data_Wrangling_Visualization/05-Exploring_New_Data_with_Tidyverse.html",
    "title": "Into The Tidyverse",
    "section": "",
    "text": "Statistics is only as interesting as the research questions we create. However, we cannot hope to answer those questions appropriately without going through the trouble of wrangling data.\nThe more time we spend digging into the data and noticing irregularities, the more likely we are to make better research questions and more appropriate conclusions. It doesn’t matter how sophisticated our analysis becomes if it’s based on bad data.\nIn this activity, you will explore a survey about happiness and related factors. By reviewing the data at a high level and then drilling into specific variables, you will gain a better idea about what this survey contains, generate interesting research questions and, with clean data, make better-informed answers to those questions."
  },
  {
    "objectID": "3-Data_Wrangling_Visualization/05-Exploring_New_Data_with_Tidyverse.html#lying",
    "href": "3-Data_Wrangling_Visualization/05-Exploring_New_Data_with_Tidyverse.html#lying",
    "title": "Into The Tidyverse",
    "section": "Lying",
    "text": "Lying\nRepeat the above analysis comparing how peoples’ happiness scores depend on their attitudes about lying.\n\nCreate a new dataset called lying that excludes the #N/A values in Lying and the Happiness scores outliers, and includes only “Happiness_score” and “Lying”:\n\n\nlying_data &lt;- happiness %&gt;%\n  filter(Happiness_score &lt;= __, \n         _______________ &gt;= 0, \n         Lying __ \"#N/A\"\n  ) %&gt;%\n  ______(Happiness_score, Lying)\n\n#View(lying_data)\n\nError in parse(text = input): &lt;text&gt;:3:30: unexpected input\n2: lying_data &lt;- happiness %&gt;%\n3:   filter(Happiness_score &lt;= __\n                                ^\n\n\n\nCreate a side-by-side boxplot and summary statistics (using favstats()) table for each attitude about Lying:\n\n\nboxplot(____________________ ~ lying_data$Lying)\n\nfavstats(lying_data$Happiness_score ~ _______________)\n\nError in parse(text = input): &lt;text&gt;:2:10: unexpected input\n1: \n2: boxplot(__\n            ^"
  },
  {
    "objectID": "3-Data_Wrangling_Visualization/03-Select.html",
    "href": "3-Data_Wrangling_Visualization/03-Select.html",
    "title": "select()",
    "section": "",
    "text": "Selecting Columns\nConsider the High School survey data with 60 columns and 312 respondents.\n\n# Load libraries and data\n\nlibrary(rio)\nlibrary(mosaic)\nlibrary(tidyverse)\nlibrary(car)\n\nsurvey &lt;- import('https://github.com/byuistats/Math221D_Cannon/raw/master/Data/HighSchoolSeniors_subset.csv') %&gt;% tibble()\n\nIt is likely that we are not interested in analyzing every column in this dataset. Many may even be useless. We can use the tidyverse function, select() to create a subset of the columns that we are primarily interested in.\nRecall that we can “pipe” the raw data into tidy functions using %&gt;%. Suppose we want to see if there are differences in reaction times (Reaction_time) for left-handed and right-handed students. We could create a more manageable dataset with only the columns of interest:\n\nsurvey %&gt;%\n  select(Handed, Reaction_time)\n\n# A tibble: 312 × 2\n   Handed       Reaction_time\n   &lt;chr&gt;                &lt;dbl&gt;\n 1 Left-Handed          0.349\n 2 Right-Handed         0.358\n 3 Right-Handed         0.447\n 4 Right-Handed         0.438\n 5 Left-Handed          0.542\n 6 Right-Handed         0.428\n 7 Ambidextrous         0.258\n 8 Right-Handed         0.427\n 9 Right-Handed         0.412\n10 Right-Handed         0.346\n# ℹ 302 more rows\n\n\n\nCombining Tidy Functions\n\n\n\nClick to see how to filter out Ambitextrous participants and reaction time outliers (reaction times less than 1 seconds):\n\n\n\nClick to see\n\n\nclean &lt;- survey %&gt;%\n  filter(Handed != \"Ambidextrous\",\n         Reaction_time &lt; 1) %&gt;%\n  select(Handed, Reaction_time)\n\nboxplot(clean$Reaction_time ~ clean$Handed, col = c(2,3), ylab = \"Reaction Times\", xlab=\"\", main = \"Distribution of Reaction times for \\n Left and Right-hand Dominance\")\n\n\n\n\n\n\n\n\nWhich hand dominance appears to have quicker reaction times?"
  },
  {
    "objectID": "3-Data_Wrangling_Visualization/01-Wrangling_Basics.html",
    "href": "3-Data_Wrangling_Visualization/01-Wrangling_Basics.html",
    "title": "Intro to Data Wrangling",
    "section": "",
    "text": "In statistics classes, you are typically provided simple, clean datasets to load and analyze with ease. This is a terrible disservice to anyone who will deal with data outside of the classroom.\nAnyone who works with data will have to do some data wrangling. Data wrangling is an appropriate description of cleaning, sorting, filtering, summarizing, transforming, and a whole host of other activities to make data usable for a specific purpose.\nIn this document, we introduce a moderately messy dataset and demonstrate basic programming commands to help us get data ready for analysis or visualization."
  },
  {
    "objectID": "3-Data_Wrangling_Visualization/01-Wrangling_Basics.html#the-pipe-in-action",
    "href": "3-Data_Wrangling_Visualization/01-Wrangling_Basics.html#the-pipe-in-action",
    "title": "Intro to Data Wrangling",
    "section": "The Pipe in Action",
    "text": "The Pipe in Action\nSuppose we have a dataset, marital_status_data, with 2 columns: age and marital_status. We expect the marital_status to be one of 4 options: “Single”, “Married”, “Divorced”, “Widowed”, but some joker input, “It’s complicated”. Because we are mainly interested in making inference about the primary statuses, we may want to filter out the rows with “It’s complicated” as the status.\nWe would begin with the raw data, “pipe” it into the filter() function and tell R what I want it to do. I can either tell R what I want to keep or what I want to exclude (more here):\nmarital_Status_data %&gt;% filter(marital_status != \"It's complicated\")\nNOTE: The != is read “not equal to”, and is a common logical operator used in computer programming. So the above code returns a subset of the original data omitting the rows.\nI can use pipes sequentially to do complicated data wrangling in very few lines of code."
  },
  {
    "objectID": "2-Descriptive_Statistics/6-Bivariate_Categorical_Data_Summaries.html",
    "href": "2-Descriptive_Statistics/6-Bivariate_Categorical_Data_Summaries.html",
    "title": "Bivariate Categorical Data Summaries",
    "section": "",
    "text": "There are situations where you would like to study the relationship between 2 categorical variables.\nIn this section, we introduce numerical and graphical summaries of comparing categorical variables and discuss some quirks about dealing with categorical data.\n\n\n\nSummarize bivariate data in contingency tables\nCalculate row and column percents\nCreate side-by-side bar charts grouping\n\n\n\n\nIn this section, we will be using survey responses about Star Wars. The survey was carried out by FiveThirtyEight about the first 6 Star Wars films. The survey contains demographic information as well as movie rankings and character favorability rankings.\n\n\n\n\nlibrary(tidyverse)\nlibrary(mosaic)\nlibrary(rio)\nlibrary(car)\n\nsw &lt;- read_csv('https://raw.githubusercontent.com/byuistats/Math221D_Cannon/master/Data/StarWarsData_clean.csv')"
  },
  {
    "objectID": "2-Descriptive_Statistics/6-Bivariate_Categorical_Data_Summaries.html#lesson-objectives",
    "href": "2-Descriptive_Statistics/6-Bivariate_Categorical_Data_Summaries.html#lesson-objectives",
    "title": "Bivariate Categorical Data Summaries",
    "section": "",
    "text": "Summarize bivariate data in contingency tables\nCalculate row and column percents\nCreate side-by-side bar charts grouping"
  },
  {
    "objectID": "2-Descriptive_Statistics/6-Bivariate_Categorical_Data_Summaries.html#a-more-civilized-age",
    "href": "2-Descriptive_Statistics/6-Bivariate_Categorical_Data_Summaries.html#a-more-civilized-age",
    "title": "Bivariate Categorical Data Summaries",
    "section": "",
    "text": "In this section, we will be using survey responses about Star Wars. The survey was carried out by FiveThirtyEight about the first 6 Star Wars films. The survey contains demographic information as well as movie rankings and character favorability rankings."
  },
  {
    "objectID": "2-Descriptive_Statistics/6-Bivariate_Categorical_Data_Summaries.html#load-the-data-and-libraries",
    "href": "2-Descriptive_Statistics/6-Bivariate_Categorical_Data_Summaries.html#load-the-data-and-libraries",
    "title": "Bivariate Categorical Data Summaries",
    "section": "",
    "text": "library(tidyverse)\nlibrary(mosaic)\nlibrary(rio)\nlibrary(car)\n\nsw &lt;- read_csv('https://raw.githubusercontent.com/byuistats/Math221D_Cannon/master/Data/StarWarsData_clean.csv')"
  },
  {
    "objectID": "2-Descriptive_Statistics/6-Bivariate_Categorical_Data_Summaries.html#calculating-proportions",
    "href": "2-Descriptive_Statistics/6-Bivariate_Categorical_Data_Summaries.html#calculating-proportions",
    "title": "Bivariate Categorical Data Summaries",
    "section": "Calculating Proportions",
    "text": "Calculating Proportions\nWhen looking at a single categorical variable, we input a table into the prop.table() function to get proportions instead of counts. It’s slightly more complicated with 2 variables because there are several proportions that can be calculated. The denominator depends on what we’re interested in studying as illustrated above (percent of females? or percent of Fans?).\nNOTE: The denominator corresponds to the “of”. When talking about proportion “of Males”, total males should be the denominator. The proportion “of Fans” means that the denominator should be the total number of fans. Proportion “of respondents” means the denominator should be the table total.\nWe can use the prop.table() function to get:\n\nTable Percents: Sum to 1 across the entire table\n\nRow Percents: Sum to 1 across rows\n\nColumn Percents: Sum to 1 across columns\n\n\nTable Proportions\nThe default for prop.table() is to give the overall percentages (counts / table total). So the proportions add to 1 across the whole table.\n\nprop.table(table(sw$Gender, sw$`Are You a Fan of SW?`))\n\n        \n                No       Yes\n  Female 0.1931540 0.2909535\n  Male   0.1454768 0.3704156\n\n\nFor example, about 37% of respondents are male fans of Star Wars. About 19% of respondents are females who are not fans.\nThis is not typically the most interesting way to look at the data. We are more often interested in row or column proportions.\n\n\nRow Proportions\nWe can specify row proportions by including another input into the prop.table() function. We specify which margin to use as the denominator.\nRecall that the table() function will put the first input as the row and the second input as the column. To get row proportions, we tell R to divide by the row totals:\n\nprop.table(table(sw$Gender, sw$`Are You a Fan of SW?`), margin = 1)\n\n        \n                No       Yes\n  Female 0.3989899 0.6010101\n  Male   0.2819905 0.7180095\n\n\nQUESTION: What percent of males are fans of Star Wars?\nANSWER:\nQUESTION: What percent of females are fans of Star Wars?\nANSWER:\n\n\nColumn Proportions\nTo get column proportions, specify margin = 2 (the second input in the table() function)\n\nprop.table(table(sw$Gender, sw$`Are You a Fan of SW?`), margin = 2)\n\n        \n                No       Yes\n  Female 0.5703971 0.4399261\n  Male   0.4296029 0.5600739\n\n\nQUESTION: What percent of Star Wars fans are females?\nANSWER:\nQUESTION: What percent of Star Wars fans are males ?\nANSWER:"
  },
  {
    "objectID": "2-Descriptive_Statistics/6-Bivariate_Categorical_Data_Summaries.html#visual-summaries",
    "href": "2-Descriptive_Statistics/6-Bivariate_Categorical_Data_Summaries.html#visual-summaries",
    "title": "Bivariate Categorical Data Summaries",
    "section": "Visual Summaries",
    "text": "Visual Summaries\nThe best way to visualize 2 categorical variables is with a side-by-side bar chart. Soon we will learn a better way to make visualizations in R. For now, it’s a bit clunky to get the right type of graph that makes sense. But here’s the process:\n\nCreate and name a contingency table. Your column variable will be how the bars are grouped, and the row variable will determine the colors of the grouped bars\nInput the table name into the barplot() including the additional inputs: beside=TRUE which puts the bars next to each other, and legend=rownmes(table_name) which will add a legend showing the which bars correspond to which\nThe default colors are atrocious. You can specify the colors by adding the additional input col=c(2,3,4) or col=c(\"lightblue\", \"lightgreen\", \"darkred\",...) including as many colors as there are levels in the row variable.\n\nVisually, bar charts are the optimal way to express categorical data. Pie charts, while very common, are problematic because of weaknesses in basic human perception.\n\ntbl1 &lt;- table(sw$Gender, sw$`Are You a Fan of SW?`)\n\nbarplot(tbl1, beside=TRUE, legend=rownames(tbl1))\n\n\n\n\n\n\n\n# Adding Color\nbarplot(tbl1, beside=TRUE, legend=rownames(tbl1), col=c(2, 4))"
  },
  {
    "objectID": "2-Descriptive_Statistics/6-Bivariate_Categorical_Data_Summaries.html#proportion-table",
    "href": "2-Descriptive_Statistics/6-Bivariate_Categorical_Data_Summaries.html#proportion-table",
    "title": "Bivariate Categorical Data Summaries",
    "section": "Proportion Table",
    "text": "Proportion Table\nWe would like to compare the relationship between Gender and Household Income.\nCreate and name a table that shows the percent of Genders in each of the income levels:\n\ntbl3 &lt;- table()\n\nError in table(): nothing to tabulate\n\n\nQuestion: What percent of female respondents are in the 150,000+ category?\nAnswer:\nQuestion: What percent of male respondents are in the 150,000+ category? Answer:\nCreate a bar plot that compares the income distribution for each gender in the study:\n\nbarplot(tbl3, beside = TRUE, legend=rownames(tbl3))\n\nError: object 'tbl3' not found\n\n\nSwap the row and column inputs and create the bar chart with the opposite grouping:\n\ntbl4 &lt;- table()\n\nError in table(): nothing to tabulate\n\nbarplot(tbl4, beside = TRUE, legend=rownames(tbl4))\n\nError: object 'tbl4' not found\n\n\nWhich chart is more interesting?"
  },
  {
    "objectID": "2-Descriptive_Statistics/4-Bivariate_Quantitative_Data_Summaries.html",
    "href": "2-Descriptive_Statistics/4-Bivariate_Quantitative_Data_Summaries.html",
    "title": "Summarizing Bivariate Data",
    "section": "",
    "text": "Bivariate data refers to situations where you have a quantitative response variable and a quantitative explanatory variable.\nAs with other data summaries, we can describe the relationship between 2 quantitative variables numerically and visually.\nBy the end of this lesson, you should be able to:\n\nCreate scatterplots for 2 quantitative variables R\n\nDescribe what the correlation coefficient, \\(r\\), quantifies\n\nCalculate \\(r\\) using the cor() function\n\nTwo datasets will be used to illustrate these concepts. The first contains self-reported confidence in mathematics and test scores. The second contains eruption duration and time between eruptions of Old Faithful geyser in Yellowstone National Park.\n\n# Load the libraries and data\nlibrary(tidyverse)\nlibrary(mosaic)\nlibrary(rio)\nlibrary(car)\n\ngeyser &lt;- import('https://byuistats.github.io/BYUI_M221_Book/Data/OldFaithful.xlsx')\nnames(geyser)\n\n[1] \"Duration\" \"Wait\"     \"Source\"  \n\nmath &lt;- import('https://byuistats.github.io/BYUI_M221_Book/Data/MathSelfEfficacy.xlsx')\nnames(math)\n\n[1] \"Gender\"               \"Score\"                \"ConfidenceRatingMean\"\n[4] \"Comments\""
  },
  {
    "objectID": "2-Descriptive_Statistics/4-Bivariate_Quantitative_Data_Summaries.html#scatter-plot",
    "href": "2-Descriptive_Statistics/4-Bivariate_Quantitative_Data_Summaries.html#scatter-plot",
    "title": "Summarizing Bivariate Data",
    "section": "Scatter plot",
    "text": "Scatter plot\nReview the scatter plot showing the relationship between students’ self reported confidence rating and test score.\n\nplot(math$Score ~ math$ConfidenceRatingMean)\n\n\n\n\n\n\n\n\nQuestion: Before calculating the Correlation Coefficient, r, describe in words the direction and strength of the relationship. (strength, direction)\nAnswer:\nQuestion: Does it look linear?\nAnswer:\nQuestion: What’s your best guess at, \\(r\\) based on the scatterplot?\nAnswer:\nCalculate the Correlation Coefficient, \\(r\\):\n\ncor(math$Score ~ math$ConfidenceRatingMean)\n\n[1] 0.7278648\n\n\nQuestion: How far off was your guess?\nAnswer:"
  },
  {
    "objectID": "2-Descriptive_Statistics/4-Bivariate_Quantitative_Data_Summaries.html#scatter-plot-1",
    "href": "2-Descriptive_Statistics/4-Bivariate_Quantitative_Data_Summaries.html#scatter-plot-1",
    "title": "Summarizing Bivariate Data",
    "section": "Scatter plot",
    "text": "Scatter plot\nMake a scatter plot showing the relationship between wait time and the duration of the next eruption.\nDeciding which is the response variable in this data is a bit tricky. To remain consistent, assume that the wait time between eruptions explains how long the subsequent eruption will last.\nCreate a scatterplot of the relationship between wait time and duration. Remember to use the formula y ~ x\nQuestion: Before calculating the Correlation Coefficient, r, describe in words the direction and strength of the relationship.\nAnswer:\nQuestion: What’s your best guess at, \\(r\\) based on the scatter plot?\nAnswer:\nQuestion: Does it look linear?\nAnswer:\nCalculate the Correlation Coefficient, \\(r\\):\nQuestion: How far off was your guess?\nAnswer:"
  },
  {
    "objectID": "2-Descriptive_Statistics/2-Graphical_Quantitative_Data_Summaries.html",
    "href": "2-Descriptive_Statistics/2-Graphical_Quantitative_Data_Summaries.html",
    "title": "Graphical Quantitative Data Summaries",
    "section": "",
    "text": "In this section, you will learn the basic visualizations for quantitative variables. After completion, you should be able to:\n\nInterpret data presented in a histogram\n\nIdentify left-skewed, right-skewed and symmetric distributions from histograms\n\nInterpret a boxplot\n\nCreate histograms and boxplots in R\n\n\n\nWe will use R to calculate measures of center and spread using data collected about costs incurred by hospitals due to certain lawsuits. The lawsuits in question were about surgeries performed on the wrong patient, or on the right patient but the wrong part of the patient’s body (the wrong site).\nLoad the libraries and the data into R:\n\nlibrary(rio)\nlibrary(mosaic)\nlibrary(tidyverse)\nlibrary(car)\n\nwrong_patient &lt;- import(\"https://github.com/byuistats/Math221D_Course/raw/main/Data/WrongSiteWrongPatient.xlsx\")"
  },
  {
    "objectID": "2-Descriptive_Statistics/2-Graphical_Quantitative_Data_Summaries.html#reading-data-into-r",
    "href": "2-Descriptive_Statistics/2-Graphical_Quantitative_Data_Summaries.html#reading-data-into-r",
    "title": "Graphical Quantitative Data Summaries",
    "section": "",
    "text": "We will use R to calculate measures of center and spread using data collected about costs incurred by hospitals due to certain lawsuits. The lawsuits in question were about surgeries performed on the wrong patient, or on the right patient but the wrong part of the patient’s body (the wrong site).\nLoad the libraries and the data into R:\n\nlibrary(rio)\nlibrary(mosaic)\nlibrary(tidyverse)\nlibrary(car)\n\nwrong_patient &lt;- import(\"https://github.com/byuistats/Math221D_Course/raw/main/Data/WrongSiteWrongPatient.xlsx\")"
  },
  {
    "objectID": "2-Descriptive_Statistics/2-Graphical_Quantitative_Data_Summaries.html#histograms",
    "href": "2-Descriptive_Statistics/2-Graphical_Quantitative_Data_Summaries.html#histograms",
    "title": "Graphical Quantitative Data Summaries",
    "section": "Histograms",
    "text": "Histograms\nHistograms are commonly used to visualize a single, quantitative variable. It provides a way to visualize how data points are spread out.\nThe key components of a histogram are:\n\nBars: Histograms consist of adjacent bars, where each bar represents a range of values, known as a “bin.” The height of each bar indicates the frequency (or count) of data points that fall within that bin.\nBins: The x-axis is divided into equal intervals (bins). Each bin captures a specific range of values. For example, if you’re plotting test scores, a bin might cover scores at 5-point intervals.\nFrequency: The y-axis shows the frequency or number of observations that fall within each bin. This allows you to see how many data points lie in each bin.\n\n\nHow to Interpret a Histogram:\n\nShape of Distribution: The overall shape of the histogram can indicate the distribution of the data.\nCentral Tendency: You can often observe where most of the data points cluster, giving an idea of the central tendency (mean, median).\nSpread: You can see how spread out the data is, which provides insights into variability.\nOutliers: Histograms can help identify any potential outliers by showing bars with significantly lower frequencies compared to others.\n\n\n\nDescribing the “Shape” of a Distribution\nWe will describe the shape of the distribution of a data set using the following basic categories:\n\nSymmetric (bell-shaped)\n\nRight skewed\n\nLeft skewed\n\nA distribution is symmetric if both the left and right side of the distribution appear to be roughly a mirror image of each other. A special symmetric distribution is a bell-shaped distribution. When data follow a bell-shaped distribution, the histogram looks like a bell. Bell-shaped distributions play an important role in Statistics and will play a role in most of the future lessons.\nA distribution is right-skewed if a histogram of the distribution shows a long right tail. This can occur if there are some very large outliers on the right-hand side of the distribution. A distribution is left-skewed if a histogram shows that it has a long tail to the left.\n\n\n\nRight-skewed\n\n\nSymmetric & Bell-shaped\n\n\nLeft-skewed\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMean: $10.45\nMedian: $9.04\nMean is to the right of the median.\n\n\nMean: 71.1 inches\nMedian: 71 inches\nMean and median are roughly equal.\n\n\nMean: 3.42\nMedian: 3.45\nMean is to the left of the median.\n\n\n\n\nIf a distribution has only one peak, it is said to be unimodal. The three distributions illustrated above are all unimodal distributions. Some people might argue that there are several peaks in the GPA data, so it should not be considered unimodal. Even though there are jagged bumps in the histogram, it is important to visualize the overall shape in the data. When interpreting a histogram, it can be helpful to blur your eyes and imagine the overall shape after smoothing out the bumps. If the overall trend indicates that there is more than one bump, then we do not consider the distribution to be unimodal. We will usually only work with unimodal data sets in this course.\nSome distributions have no distinct peak, others have more than one peak. When there is no distinct peak, and the histogram shows a relatively flat shape, we might say the data follow a uniform distribution. If there are two distinct peaks, a distribution is called bimodal. If there are more than two peaks, we refer to the distribution as multimodal."
  },
  {
    "objectID": "2-Descriptive_Statistics/2-Graphical_Quantitative_Data_Summaries.html#histograms-in-r",
    "href": "2-Descriptive_Statistics/2-Graphical_Quantitative_Data_Summaries.html#histograms-in-r",
    "title": "Graphical Quantitative Data Summaries",
    "section": "Histograms in R",
    "text": "Histograms in R\nTo make a histogram in R, you can use the histogram() function. This takes as an input a single quantitative variable:\n\nhistogram(wrong_patient$Wrong_Patient)\n\n\n\n\n\n\n\n\nQUESTION: What is the basic shape of the wrong patient procedure lawsuits?\nANSWER:"
  },
  {
    "objectID": "2-Descriptive_Statistics/2-Graphical_Quantitative_Data_Summaries.html#summary",
    "href": "2-Descriptive_Statistics/2-Graphical_Quantitative_Data_Summaries.html#summary",
    "title": "Graphical Quantitative Data Summaries",
    "section": "Summary",
    "text": "Summary\n\nRemember…\n\n\n\nA percentile is calculated in R using quantile(data, 0.#) where the 0.# is the percentile written as a decimal number. So the 20th percentile would be written as 0.2.\nA percentile is a number such that a specified percentage of the data are at or below this number. For example, if say 80% of college students were shorter than (or equal to) 70 inches tall in height, then the 80th percentile of heights of college students would be 70 inches.\nStandard deviation is calculated in R for a sample of data using sd(data).\nThe standard deviation is a number that describes how spread out the data typically are from the mean of that data. A larger standard deviation means the data are more spread out from their mean than data with a smaller standard deviation. The standard deviation is never negative. A standard deviation of zero implies all values in the data set are exactly the same.\nTo compute any of the five-number summary values in R, use the R function favstats(data) which also includes the mean and standard deviation.\nThe five-number summary consists of (1) the minimum value in the data, (2) the first quartile (25th percentile) of the data, (3) the median of the data (50th percentile), (4) the third quartile (75th percentile) of the data, and (5) the maximum value occurring in the data.\nTo create a boxplot in R, use the boxplot(data) or for multiple columns boxplot(data1, data2, names=c(\"Name of Column 1\", \"Name of Column 2)).\nBoxplots are a visualization of the five-number summary of a data set."
  },
  {
    "objectID": "1-Getting_Started/Test.html",
    "href": "1-Getting_Started/Test.html",
    "title": "Testing…Testing…1…2…3",
    "section": "",
    "text": "Introduction\nThis type of file is called a “markdown” file. Markdown is like Microsoft Word but much more powerful. This file is made specifically for creating fancy reports and has a file type .qmd meaning “Quarto Markdown”.\nYou will become very familiar with these files throughout the semester. For now, it’s only necessary to download this file, save it in a sensible folder on your computer or OneDrive, and “run” it.\nClicking on the “Render” button above will create an .html document that should open up in your default browser. This .html document will be created and saved in the same location as this TestingTesting.qmd document.\nNOTE: If this document is in your Downloads folder, that is also where the html file will appear.\nAs you see, we can make section headers and type regular text. But the power of .qmd files is that we can code inside these documents and present our output directly within the document.\nClick “Render” to test to see if your software is set up.\nWhen coding, we have to tell the computer when we’re writing text and when we expect it to compile code. Below is an example of a “code chunk” that creates a made up graph.\nYou do not have to understand this right now. We’re only testing that R and RStudio are set up correctly.\n\n\nInstalling Libraries\nEven though we have R and RStudio installed, we can load additional toolboxes that have more tools than R provides by itself.\n\nx &lt;- seq(0,10, length = 100)\ny &lt;- 2+exp(x)\n\nplot(x,y, type = \"l\", lwd=2, col=\"darkblue\", main = \"Exponential Function\")"
  },
  {
    "objectID": "1-Getting_Started/5-Stat_Process.html",
    "href": "1-Getting_Started/5-Stat_Process.html",
    "title": "The Statistical Process",
    "section": "",
    "text": "By the end of this lesson, you should be able to:\n\nDescribe the five steps of the Statistical Process\nDistinguish between an observational study and an experiment\nDifferentiate between a population and a sample\nDescribe each of the following sampling schemes:\n\nSimple random sampling\nStratified sampling\nSystematic sampling\nCluster sampling\nConvenience sampling\n\nExplain the importance of using random sampling\nDistinguish between a quantitative and a categorical variable"
  },
  {
    "objectID": "1-Getting_Started/5-Stat_Process.html#lesson-outcomes",
    "href": "1-Getting_Started/5-Stat_Process.html#lesson-outcomes",
    "title": "The Statistical Process",
    "section": "",
    "text": "By the end of this lesson, you should be able to:\n\nDescribe the five steps of the Statistical Process\nDistinguish between an observational study and an experiment\nDifferentiate between a population and a sample\nDescribe each of the following sampling schemes:\n\nSimple random sampling\nStratified sampling\nSystematic sampling\nCluster sampling\nConvenience sampling\n\nExplain the importance of using random sampling\nDistinguish between a quantitative and a categorical variable"
  },
  {
    "objectID": "1-Getting_Started/5-Stat_Process.html#introduction",
    "href": "1-Getting_Started/5-Stat_Process.html#introduction",
    "title": "The Statistical Process",
    "section": "Introduction",
    "text": "Introduction\nStatistics are used in every aspect of society. Every statistical analysis follows a pattern we will call the Statistical Process. This process will be introduced in this lesson and will be used throughout the course.\n\n\nThe Statistical Process and Daniel’s Experiment\n\n\n\nStained-glass depiction of Daniel’s deliverance from the lions’ den. Found in the old Dominican priory church at Hawkesyard in Staffordshire, England. (Photo credit: Fr Lawrence Lew, O.P. Used by permission.)\n\n\n\nThe Old Testament prophet Daniel planned one of the earliest recorded scientific research studies. We will use his example to illustrate the following five steps of The Statistical Process.\nThe following icons can help you remember these steps. Notice that each icon has a letter and an image to help you remember the five steps of the Statistical Process.\n\n\n\n\n\n\n\n\n\n\n\nThe Statistical Process\n\n\n\n\n\n\nDesign the Study\n\n\n\nCollect the Data\n\n\n\nDescribe the Data\n\n\n\nMake Inference\n\n\n\nTake Action\n\n\n\n\n\n\n\nStep 1: Design the Study\n\nAn important step in scientific inquiry or problem solving can be to state a research question such as:\n\nWill internet advertising increase a company’s revenue?\nDoes expressing gratitude increase a person’s satisfaction with life in general?\nDoes a newly developed vaccine prevent the spread of disease?\n\nResearchers also investigate the background of the situation. What have other people discovered about this situation? How can we find the answer to the research question? What do we need to do? What is the population (or total collection of all individuals) under consideration? What kind of data need to be collected?\nBefore collecting data, researchers make a hypothesis, or an educated guess about the outcome of their research. A hypothesis is a statement such as the following:\n\nUsing internet advertising will increase the company’s sales revenue.\nPeople who express gratitude will be more satisfied with life than those who do not.\nA newly-developed vaccine is effective at preventing tuberculosis.\n\n\n\nDaniel’s Experiment\nAfter taking Israel captive, Babylon’s King Nebuchadnezzar asked his chief officer to bring Israelite children who were well favoured, and skillful in all wisdom, and cunning in knowledge, and understanding science to stand in the king’s palaces (Daniel 1:4). To aid their preparation, Nebuchadnezzar planned to feed them his meat and wine for three years (Daniel 1:5).\nDaniel did not want to defile himself by partaking of the king’s meat and wine. He asked permission to eat pulse[^1] and drink water instead. His supervisor, Melzar, was afraid to displease the king. He thought that after eating pulse and water, the selected Israelites would look worse than their peers, and he would be punished (Daniel 1:8-10).\nWith an understanding of the background of the situation, Daniel proposed an experiment. He said, Prove thy servants, I beseech thee, ten days; and let them give us pulse to eat, and water to drink. Then let our countenances be looked upon before thee, and the countenance of the children that eat of the portion of the king’s meat: and as thou seest, deal with thy servants (Daniel 1:12-13.). In short, Daniel’s implied research question can be stated as: Will those who eat pulse and drink water appear healthier than those who eat the king’s meat and drink his wine? Melzar agreed to the experiment.\n\nAnswer the following question:\n\n\n\nWhat is Daniel’s hypothesis?\n\n\n\nSolution\n\nDaniel’s hypothesis is that the Israelite children who eat pulse and drink water will appear healthier in just ten days, compared to those who eat the king’s meat and drink his wine.\n\n\n\n\n\n\nStep 2: Collect Data\n\nWhen designing a study, much attention is given to the process by which data are observed. When examining data, it is also important to understand the data collection procedures. A sample is a subset (a portion) of a population. How is this sample obtained? How are the observations made?\nDaniel’s study design required that data be collected at the end of 10 days. Melzar would compare the appearances of two groups of people: (1) Israelites who ate pulse and drank water versus (2) Israelites who ate the king’s meat and drank his wine.\n\n\n\nStep 3: Describe the Data\n\nWhen we describe data, we use any tools appropriate to the situation. This can include creating graphs or calculating summary statistics to help understand or visualize the data.\nFor Daniel’s experiment, the data are described in Daniel 1:15: And at the end of ten days [the] countenances [of those who ate pulse] appeared fairer and fatter in flesh than all the children which did eat the portion of the king’s meat.\n\n\n\nStep 4: Make Inferences\n\nInference is the process of using the information contained in a sample from a population to make a general statement (i.e. to infer something) about the entire population. Later in the course we will learn techniques that make this type of analysis possible.\nMelzar made an inference. Based on the results of the sample, he determined that (in general) those who eat pulse and drink water will be healthier than those who eat the king’s meat and drink his wine Daniel 1:15-16.\n\n\n\nStep 5: Take Action\n\nThe goal of a statistical analysis is to determine which action to take in a particular situation. Actions can include many things: launching an internet ad campaign (or not), expressing gratitude (or not), getting vaccinated (or not), etc.\nMelzar took action as described in Daniel 1:16: Thus Melzar took away the portion of their meat, and the wine that they should drink; and gave [all the Israelite children] pulse.\nWas the experiment a success?\n“Now at the end of the days that the king had said he should bring them in… the king communed with them; and among them all was found none like Daniel, Hananiah, Mishael, and Azariah And in all matters of wisdom and understanding, that the king enquired of them, he found them ten times better than all the magicians and astrologers that were in all his realm” Daniel 1:18-20.\n\n\n\nSummary of the Statistical Process\n\nDaniel’s experience can also help you learn the Statistical Process. Look at the first letter of each of the steps in the Statistical Process. You can use the phrase “Daniel Can Discern More Truth” to help you to help you remember the five steps in the Statistical Process.\n\nThe Statistical Process\n\n\n\n\n \nPneumonic\nActual Process Step\n\n\n\n\nStep 1:\nDaniel\nDesign the study\n\n\nStep 2:\nCan\nCollect data\n\n\nStep 3:\nDiscern\nDescribe the data\n\n\nStep 4:\nMore\nMake inferences\n\n\nStep 5:\nTruth\nTake action\n\n\n\n\nThe Statistical Process will be used throughout the course. Take time to memorize the five steps.\n\n\nThe study designed by the Old Testament prophet Daniel provides an ancient example of a designed experiment. Daniel’s experiment included two groups of people: those who had the experimental treatment eating pulse and drinking water (called the treatment group) and those who ate the standard food the king’s meat (called the control group.) The treatment group receives the experimental procedure. The control group is used for comparison.\n\nAnswer the following question:\n\n\n\nWhy was it important that Daniel’s experiment included a control group?\n\n\n\nSolution\n\nIf there was no control group, then there would be no way to compare the effect of the diets (the treatments). Having a control group allows a researcher to see the effect of not taking any action. For Daniel, the control group (who ate the king’s meat and drank his wine) provided a basis for comparing the effect of the new treatment (i.e. eating pulse and drinking water.)"
  },
  {
    "objectID": "1-Getting_Started/5-Stat_Process.html#design-of-studies",
    "href": "1-Getting_Started/5-Stat_Process.html#design-of-studies",
    "title": "The Statistical Process",
    "section": "Design of Studies",
    "text": "Design of Studies\nMost research projects can be classified into one of two basic categories: observational studies or designed experiments. In an experiment, researchers control (to some extent) the conditions under which measurements are made. In an observational study, researchers simply observe what happens, without controlling the conditions under which measurements are made. Both types of study follow the five steps of the Statistical Process.\n\n\nDesigned Experiments\nIn a designed experiment, researchers manipulate the conditions that the participants experience. They often do this by randomly assigning subjects to one of two groups, a “treatment” group (sometimes called the experimental group) and a “control” group (though this could be second treatment group instead of a control group). The experiment is typically conducted by applying some kind of treatment to the subjects in the treatment group and observing the effect of the treatment. Those in the control group do not receive the treatment and are also observed. In this way researchers can determine the effects of the treatment by comparing the treatment group results to the control group results. The following example illustrates the use of these two groups.\nJonas Salk’s First Polio Vaccine Trial\nBeginning around 1916 and through the 1950s, a mysterious plague attacked infants and children. Symptoms included excruciating muscle pain and a stiff neck. This illness, which became known as poliomyelitis or simply “polio,” left children disfigured, paralyzed, and sometimes even dead.\nWhile working as a researcher at the University of Pittsburgh School of Medicine, Dr. Jonas E. Salk developed a vaccine that might help prevent the spread of this disease. He conducted what has become one of the most famous designed experiments in history.\nThis short video below provides a compelling summary of the famous Jonas Salk vaccine experiment. As you watch, notice each of the 5 steps of a statistical study in this study.  \nAs explained in the video, in the first Salk trial almost 1.1 million children participated in the study. Even though the sample size was large, flaws in the study design rendered the results useless.\nUndaunted, Dr. Salk fixed the problems with the design and enrolled hundreds of thousands of additional children for the second phase of his study. In all, over 1.8 million infants and children participated in this experiment, making it the largest drug trial to date.\n\nStep 1: Design the study.\nThe participants in a study are commonly called subjects. Sometimes subjects are called experimental units or simply units. In the Salk trials, the children who participated were the subjects.\nSubjects (the children) were randomly assigned to one of two groups. The first group was given the experimental vaccine, the treatment. The treatment is the new or experimental condition that is imposed on the subjects. The subjects who receive the treatment make up the treatment group.\nThe second group was given a control or placebo. In this study, the control was an injection that looked just like the vaccine, but contained a harmless saline solution. The control group or placebo group is made up of the subjects assigned to receive the control.\nThis study was double blind. Neither the children’s parents nor their doctors knew whether a particular child received the treatment or the control. Both parties were blinded to this information.\nBecause the children were assigned to the groups randomly, the two groups should be similar. If the vaccine is not effective, the number of future cases of polio should be about the same in each group. However, if Salk’s vaccine helped to prevent the spread of polio, then fewer cases should occur in the vaccinated group.\n\nAnswer the following questions:\n\n\n\nSome children can be identified as having a higher risk of developing polio. Would it have been better if they were assigned to the treatment group so they could get the vaccine?\n\n\n\nSolution\n\n\nNo. The two groups need to be as similar as possible. Specifically, the people in the treatment group need to have the same potential (on average) of contracting polio as the people in the control group. If we put the people who are at a higher risk of developing polio in the treatment group, we run the risk of having more people in the treatment group getting polio simply because they are more likely to get it, whether they are vaccinated or not. Likewise, we might have fewer people in the control group getting polio just because they are less likely to get it, whether they are vaccinated or not.\nThese two effects would create a bias against the vaccine, by making the vaccine look like it doesn’t work, or doesn’t work as well as it does. It might also make it appear that people who aren’t vaccinated stay healthy and the vaccine is not needed. There is even a chance that people will conclude that the vaccine actually gives people polio.\nRandomly assigning subjects to the two groups tends to yield groups with similar characteristics—in this example, similar potential for contracting polio. Randomly assigning subjects to groups therefore defends us against problems like those mentioned in the previous paragraph.\n\n\n\nWhy is it important for the subject and those who assess the health of the subject to be unaware of whether or not that child received the vaccine?\n\n\n\nSolution\n\n\nSubjects: Suppose a subject in the study thinks they’re being treated. It has been documented that subjects with such knowledge tend to show improvement whether they are receiving the treatment or not. To see why, consider how you might feel and act if you were told you had been vaccinated. You might have a more hopeful outlook, leading to healthier living habits such as better hygiene and nutrition. Such changes would tend to reduce your chance of contracting polio whether you’ve received the vaccine or not. This might make the vaccine look like it works better than it does. It also might make the vaccine look like it works, even if it doesn’t.\nNow suppose subjects in the control group know they are not being treated. This can also change the way they feel and act, in ways that can make them more likely to contract polio than they would be if they weren’t in the study. This could make it look like the incidence of polio among unvaccinated persons is higher than it is, again making the vaccine look like it works better than it does.\nTo reduce bias caused by such errors, subjects should not know to which group they are assigned.\nResearchers: Suppose a researcher assessing the health of a subject is told that the subject is in the control group. It has been documented that in such a case, the researcher is more likely to record that the subject has symptoms even if the subject is not actually in the control group. This makes it look like unvaccinated persons are more likely to get polio than they really are, which makes it look like the vaccine works better than it does.\nThere are other effects of knowing to which group the subject belongs, such as doctors treating or advising the patient differently than they would without such knowledge. Such differences can make it harder to tell whether the vaccine works, and how well.\nTo reduce bias caused by such effects, those assessing the health of the subjects should not be told to which group the subject belongs.\n\n\n\n\nStep 2: Collect data.\nThe researchers followed up with each child to determine if they contracted polio. They recorded the number of children in each group that developed polio during the study period. Not all of Salk’s experiments were double-blind. Here is a summary of the results from the regions where a double-blind study was conducted (Francis et al., 1955; Brownlee, 1955):\n\n\nChildren Who Developed Polio\n\n\n\n\n\n\n\nYes\n\n\n\n\nNo\n\n\n\n\nTotal\n\n\n\n\n\n\n\n\nTreatment Group\n\n\n\n\n57\n\n\n\n\n200,688\n\n\n\n\n200,745\n\n\n\n\n\n\nPlacebo Group\n\n\n\n\n142\n\n\n\n\n201,087\n\n\n\n\n201,229\n\n\n\n\n\n\nStep 3: Describe the data.\nOne way to summarize the data is to compute the proportion of children in each group that developed polio. The proportion of children in the treatment group that developed polio during the study period is:\n\\[ \\frac{57}{200745} = 0.000~283~9 \\]\n\nAnswer the following questions:\n\n\n\nCalculate the proportion of children in the placebo group that developed polio during the study period.\n\n\n\nSolution\n\n\n\\[ \\displaystyle{\\frac{142}{201229} = 0.000~705~7} \\]\n\n\n\nCompare the two proportions. What do you observe?\n\n\n\nSolution\n\n\nThe proportion of children in the placebo group that develop polio during the study period was more than double the proportion of children in the treatment group that developed polio during the study period. That suggests that the treatment is effective in reducing the proportion of children that will develop polio.\n\n\n\n\nStep 4: Make inferences\nCareful statistical analysis of the records suggested that this difference was so great that it was attributable to the vaccine and not to chance. Assuming that the vaccine had no effect, the probability that the difference in the proportions between the two groups would be at least as extreme as the difference Dr. Salk observed was very low: 0.00000000093. Because this probability is so small, it is highly unlikely that these results are due to chance.\n\n\nStep 5: Take action\nOnce it was clear that the vaccine was effective, children who were unvaccinated or had received the placebo were given Salk’s vaccine. Since 1954, there has been a marked decrease in the number of polio cases worldwide (Offit, 2005). Public health researchers are striving to eradicate this disease entirely.\n\n\n\nObservational Studies\nIn an observational study researchers observe the responses of the individuals, without controlling the conditions experienced by the individuals. Therefore, they do not assign the participants to treatment or control groups.\nObservational studies commonly occur in business settings. One example is a financial audit. The purpose of a financial audit is to assess the accuracy of a company’s financial business practices. ImmunAvance Ltd., a non-government health care organization, hired the Accounting Office at Global Optimization Unlimited to perform an independent audit of their financial practices. ImmunAvance provides inoculation and other preventative health care services in rural African communities.\n\n\n\n\n\nSampling Methods\nStep 2: Collect data\nThere are several procedures that can be used to select a random sample from a population, including: simple random sampling (SRS), stratified sampling, systematic sampling, cluster sampling, , and convenience sampling (or, haphazard sampling). These are examples of sampling methods.\n\nRandom Sampling Methods\nA simple random sample (SRS) is the best method for obtaining a sample from a population. This method allows each possible sample of a certain size an equal chance at being selected as the chosen sample. A difficulty of this method is that a list of all of the items in the population must be accessible before the sample is taken. Often, we obtain a SRS by allowing a computer to randomly select a certain number of items from the full list of the population. It is akin to the idea of putting all of the names into a hat, shaking them up, and randomly drawing out a few.\n\nFor example, suppose there are 18,000 students in the population of a certain university. School officials can use a computer to randomly choose values between 1 and 18,000 to identify which students are to be selected to complete a survey. In Excel, the command to obtain a random number between 1 and 18,000 is sample(1:18000, 1). A simple random sample can be obtained any time there is a complete list of the items to be sampled and they are all accessible. All the statistical procedures in this course assume that simple random sampling has been used. But in practice, the SRS is often difficult (or impossible) to implement.\n\nA stratified sample is when the items to be sampled are organized in groups of homogeneous (similar) items called strata, then a simple random sample is drawn from each of these strata. Stratified sampling works well when the items are similar within each stratum and tend to differ from one stratum to another. We often use stratified sampling in order to obtain a sample in such a way that we can make comparisons between each of the groups (or strata).\n\nFor example, in obtaining a sample of students from a university, school officials could define the strata as: (1) freshman, (2) sophomores, (3) juniors, and (4) seniors. A simple random sample could then be obtained from each of these strata. This would ensure that each class rank of students was represented in the sample. It would also allow the school officials to see how freshman, sophomore, junior, and senior level students compared in their answers to a survey.\n\nA systematic sample is where every \\(k^{\\text{th}}\\) item in the population is selected to be part of the sample, beginning at a random starting point. Systematic sampling works well when the items are in a random, but sequential ordering. If the items are not arranged randomly, a systematic sample can miss important parts of the population.\n\nFor example, consider a fast food company where every 10th customer is given the opportunity to compete a satisfaction survey in exchange for a small discount coupon towards their next purchase. An airport security line also often implements a procedure where every 100th (or so) person is selected for a more “in depth” security examination. Similarly, factories that use assembly lines will pull say every 500th item from the assembly line to perform a quality control check on the item.\n\nA cluster sample (sometimes called a block sample) consists of taking all items in one or more randomly selected clusters, or blocks. When the variation from one block to another is relatively low, compared to the variation within the block, cluster sampling is a reasonable way to get a sample.\n\nFor example, ecologists could draw grids on a map of a forest to create small sampling regions, or sampling clusters. Then, by randomly selecting one or two of these clusters from the map, the ecologists could go to the areas marked on the map and document information on the health of every tree they find in those clusters. This is a practical way to get a sample in this case because the ecologists only have to go to a few areas of the forest, but are still able to obtain a random sample of all of the trees in the forest. It is also worth noting that the ecologists would not be interesting in comparing the health of the trees from the selected clusters to each other like they would in a stratified sample. Instead, they are just looking for a feasible way to obtain a single random sample of all of the trees in the forest, but want to keep their traveling time to a minimum while collecting their sample. In contrast, to obtain a simple random sample of trees from the same forest, the ecologists would first have to go out and number every tree in the entire forest. Then they would need to use a computer to randomly pick which trees to collect data on. Finally, they would then have to go back to the forest and collect data on the selected trees from across the entire forest. Such an approach just isn’t feasible in practice, so we are willing to settle instead for the cluster sample.\n\nA convenience sample involves selecting items that are relatively easy to obtain and does not use random selection to choose the sample. This method of sampling can be assumed to always bring bias into the sample.\n\nAs an example of a convenience sample, an auditor could haphazardly select items from a filing cabinet. This is frequently done when a quick and simple sample is needed, but may not yield a sample that represents the population well. When possible, convenience samples should be avoided.\n\n\n\n\n\nTypes of Data\nWhenever we collect data, we record information about the things we are studying. There are two basic types of data that can be recorded: quantitative measurements and categorical labels. We will call these types of data simply “quantitative” or “categorical” variables. We use the word “variable” to denote the idea that the quantitative measurements or categorical labels can vary from person to person, or item to item, in our study.\nQuantitative variables provide measurement information on each individual (or item) in our study. They represent things that are numeric in nature; things that are measured. They often include units of measurement along with the quantitative value of the measurement. For example, the heights of children measured in inches (or centimeters), or their weight measured in pounds (or kilograms). For a quantitative variable, it makes sense to apply arithmetic operations to the data (such as adding values together, computing the average of the values, or comparing two values). If one child weighs 30 pounds (13.61 kg) and a second child weights 60 pounds (27.22) then the second child is twice as heavy as the first.\nCategorical variables allow us to place each individual (or item) into to a specific category. Categorical variables are labels, and it does not make sense to do arithmetic with them. For example the gender of a newborn child, the ethnicity of an individual, a person’s job title, the brand of phone they own, or the area code of a telephone number, etc are all categorical variables. Notice that although a telephone number consists of numbers, it is not a quantitative measurement. It does not make sense to double someone’s phone number, to average phone numbers together, or to say one phone number is half the size of another. But the area code of the phone number gives information about the region where the phone number was first initiated, which is categorical information.\nNOTE: We often refer to the list of distinct category names in a categorical variable as “levels”.\nReturning to the sample accounts receivable record, we find this data to have information on both types of variables.\n\nAnswer the following question:\n\n\n\nFor each of the following variables taken from this accounts receivable record, indicate whether the variable is quantitative or categorical.\n\n\nTerms\n\n\n\nSolution\n\n\nThe variable “Terms” is categorical. It classifies the invoice by the terms of payment for that invoice.\n\n\n\nAccount number\n\n\n\nSolution\n\n\nThe variable “Account number” is categorical. Even though the account number is given a number, it is actually functioning as a label. It is not something that is counted or measured. It does not make sense to do arithmetic operations (like adding 1 or multiplying by 2) to the account number.\n\n\n\nInvoice amount\n\n\n\nSolution\n\n\nThe variable “Invoice amount” is quantitative. It makes sense to do arithmetic operations to this value. For example, the amount of Invoice 5745 (which is $990.00) is somewhat more than twice as much as that of Invoice 2378 (which is $478.00).\n\n\n \n\n\nStep 3: Describe the data\nAfter auditors collect a sample and compile the data, they review the evidence. Auditors may use graphs or compute numbers (such as the average) to summarize the evidence they found."
  },
  {
    "objectID": "1-Getting_Started/5-Stat_Process.html#making-inferences-hypothesis-testing",
    "href": "1-Getting_Started/5-Stat_Process.html#making-inferences-hypothesis-testing",
    "title": "The Statistical Process",
    "section": "Making Inferences: Hypothesis Testing",
    "text": "Making Inferences: Hypothesis Testing\nStep 4: Make inferences\nAuditors use the information drawn from the sample to form an opinion about the population. Whenever sample data is used to infer a characteristic of a population, it is called making an inference. Inferential statistics represents a collection of methods that can be used to make inference about a population. Based on the documents reviewed, the auditors assess if the company is conducting its business in a proper manner.\nWhen conducting an audit, the implicit assumption is that transactions have been posted properly. As auditors sample the company’s records, they are looking to see if everything is consistent with the original assumption that all transactions have been posted properly. It would only be in the case of discovering suspicious activity or evidence of fraudulent reporting that the auditors would change their belief about the company and accuse the company ImmunAvance of falsely reporting on their financial statements.\n\n“Piled Higher and Deeper” by Jorge Cham  \n\nThere is a formal procedure for determining when enough evidence has been found to make accusations of fraud. Later this semester, after we establish some foundational principles of statistics, we will study these statistical methods in depth. Of course, these methods can be used for much more than just determining if a company has reported their financial statements fraudulently. So we will look at many different ways these statistical procedures can be applied to research and industry.\nFor ImmunAvance’s audit, based on the samples of financial statements that had been selected, while there were a few errors in the documents, there was not evidence dramatic enough to claim that the company had been fraudulent. So the company passed their audit.\n\nStep 5: Take Action\nThe auditors prepare a report in which they give their opinion on the status of the company’s current operations.\nSince there was not enough evidence to suggest that ImmunAvance’s financial statements were fraudulent, the auditor’s conclusion is that no adjustment is necessary. The few observed discrepancies were apparently just the result of random chance errors, not the deliberate falsefying of information."
  },
  {
    "objectID": "1-Getting_Started/5-Stat_Process.html#summary",
    "href": "1-Getting_Started/5-Stat_Process.html#summary",
    "title": "The Statistical Process",
    "section": "Summary",
    "text": "Summary\n\nRemember…\n\n\n\nThe Statistical Process has five steps: Design the study, Collect the data, Describe the data, Make inference, Take action. These can be remembered by the pneumonic “Daniel Can Discern More Truth.”\nIn a designed experiment, researchers control the conditions of the study, typically with a treatment group and a control group, and then observe how the treatments impact the subjects. In a purely observational study, researchers don’t control the conditions but only observe what happens.\nThe population is the entire group of all possible subjects that could be included in the study. The sample is the subset of the population that is actually selected to participate in the study. Statistics use information from the sample to make claims about what is true about the entire population.\nThere are many sampling methods used to obtain a sample from a population. The best methods use some sort of randomness (like pulling names out of a hat, rolling dice, flipping coins, or using a computer generated list of random numbers) to avoid bias.\n\n\nA simple random sample (SRS) is a random sample taken from the full list of the population. This is the least biased (best) sampling method, but can only be implemented when a full list of the population is accessible.\nA stratified sample divides the population into similar groups and then takes an SRS from each group. The main reason to use this sampling method is when a study wants to compare and contrast certain groups within the population, say to compare freshman, sophomores, juniors, and seniors at a university.\nA systematic sample samples every kth item in the population, beginning at a random starting point. This is best applied when subjects are lined up in some way, like at a fast food restaurant, an airport security line, or an assembly line in a factory.\nA cluster sample consists of taking all items in one or more randomly selected clusters, or blocks. For example, ecologists could draw grids on a map of a forest to create small sampling regions and then sample all trees they find in a few randomly selected regions. Note that this differs from a stratified sample in that only a few sub-groups (clusters) are selected and that all subjects within the selected clusters are included in the study.\nA convenience sample involves selecting items that are relatively easy to obtain and does not use random selection to choose the sample. This method of sampling can be assumed to always bring bias into the sample.\n\n\nThe best way to avoid bias when trying to make conclusions about a population from a single sample of that population is to use a random sampling method to obtain the sample.\nQuantitative variables represent things that are numeric in nature, such as the value of a car or the number of students in a classroom. Categorical variables represent non-numerical data that can only be considered as labels, such as colors or brands of shoes."
  },
  {
    "objectID": "1-Getting_Started/5-Stat_Process.html#references",
    "href": "1-Getting_Started/5-Stat_Process.html#references",
    "title": "The Statistical Process",
    "section": "References",
    "text": "References\nBible Dictionary, “Pulse” at http://churchofjesuschrist.org/scriptures/bd/pulse.\nBrownlee, K. A. (1955). Statistics of the 1954 polio vaccine trials. Journal of the American Statistical Association, 50(272), pp. 1005-1013.\nFrancis, T., et. al. (1955). An evaluation of the 1954 poliomyletis vaccine trials. American Journal of Public Health and the Nation’s Health, 45(5)\nOffit, P. A. (2005). Why are pharmaceutical companies gradually abandoning vaccines? Health Affairs, 24(3), 622-630. doi:10.1377/hlthaff.24.3.622"
  },
  {
    "objectID": "1-Getting_Started/3-Introducing_R.html",
    "href": "1-Getting_Started/3-Introducing_R.html",
    "title": "Introducing R - Class Notes",
    "section": "",
    "text": "The file you are now reading is created using the Markdown language. Think of Markdown like Microsoft Word on steroids. The difference is, instead of clicking through drop-down menus to change font, line spacing, etc. you need to type in the instructions inside the document.\nThis file has the extension .qmd which stands for Quarto Markdown. All assignments and activities will be given as .qmd files.\nThe upside is that you can easily write nice looking reports and directly incorporate code output including summary tables and graphs.\n\n\nAt the very top of the document, you will see some blue and green text between two sets of ---. This gives instructions to the computer about what kind of document to make. It is called the YAML. What YAML stands for is up for debate. But the instructions are necessary for building the document and making it look nice.\nThe YAML above is very basic. We may expand on it a little as the semester progresses. But for now, this document can be made into an HTML file by “Rendering” it.\n\n\n\nAbove, you should see a “Render” button. Click on it and see what happens.\nWe can make section headers, type text, include code chunks and stitch it all together into a decent-looking report with only a few lines of text."
  },
  {
    "objectID": "1-Getting_Started/3-Introducing_R.html#the-yaml",
    "href": "1-Getting_Started/3-Introducing_R.html#the-yaml",
    "title": "Introducing R - Class Notes",
    "section": "",
    "text": "At the very top of the document, you will see some blue and green text between two sets of ---. This gives instructions to the computer about what kind of document to make. It is called the YAML. What YAML stands for is up for debate. But the instructions are necessary for building the document and making it look nice.\nThe YAML above is very basic. We may expand on it a little as the semester progresses. But for now, this document can be made into an HTML file by “Rendering” it."
  },
  {
    "objectID": "1-Getting_Started/3-Introducing_R.html#rendering-the-document",
    "href": "1-Getting_Started/3-Introducing_R.html#rendering-the-document",
    "title": "Introducing R - Class Notes",
    "section": "",
    "text": "Above, you should see a “Render” button. Click on it and see what happens.\nWe can make section headers, type text, include code chunks and stitch it all together into a decent-looking report with only a few lines of text."
  },
  {
    "objectID": "1-Getting_Started/3-Introducing_R.html#this-is-a-section-subheader",
    "href": "1-Getting_Started/3-Introducing_R.html#this-is-a-section-subheader",
    "title": "Introducing R - Class Notes",
    "section": "This is a section subheader",
    "text": "This is a section subheader\n\nsub-subheader\nYou get the point."
  },
  {
    "objectID": "1-Getting_Started/3-Introducing_R.html#the-combine-function-c",
    "href": "1-Getting_Started/3-Introducing_R.html#the-combine-function-c",
    "title": "Introducing R - Class Notes",
    "section": "The Combine Function, c()",
    "text": "The Combine Function, c()\nSometimes I want to type in data manually. The simplest way to accomplish this is to use the combine function, c(). Think of c() as a way to combine a list of things that are alike. For example, I can create a new dataset using the combine function:\n\nc(21, 22, 18, 19, 19, 20, 22)\n\n[1] 21 22 18 19 19 20 22\n\n\nI can create a list of names as well, but I have to put characters in quotes:\n\nc(\"Billy\", \"Joel\", \"Bobby\", \"Dylan\", \"Carly\", \"Rae\", \"Jeppson\")\n\n[1] \"Billy\"   \"Joel\"    \"Bobby\"   \"Dylan\"   \"Carly\"   \"Rae\"     \"Jeppson\"\n\n\nRunning the above code isn’t very helpful because I won’t be able to do anything with those lists. Computers are so literal. All the code did was print out a list that I made. If I want to be able to use those lists, I need to give them a name."
  },
  {
    "objectID": "1-Getting_Started/3-Introducing_R.html#the-assignment-operator",
    "href": "1-Getting_Started/3-Introducing_R.html#the-assignment-operator",
    "title": "Introducing R - Class Notes",
    "section": "The Assignment Operator",
    "text": "The Assignment Operator\nIn R, we like to name things. You can name pretty much anything including datasets, graphs, output, etc. When we want to assign a name to something we use &lt;-, which is made up of a “less than” sign and a dash. We put the name on the left side.\nLet’s demonstrate by naming our lists for ages and names above.\n\nages &lt;- c(21, 22, 18, 19, 19, 20, 22)\n\nstudents &lt;- c(\"Billy\", \"Joel\", \"Bobby\", \"Dylan\", \"Carly\", \"Rae\", \"Jeppson\")\n\nNow if I want to refer to those data, I can just refer to the name:\n\nages\n\n[1] 21 22 18 19 19 20 22\n\nstudents\n\n[1] \"Billy\"   \"Joel\"    \"Bobby\"   \"Dylan\"   \"Carly\"   \"Rae\"     \"Jeppson\"\n\n\nSHORT KEY: You can quickly create the assignment operator by hitting “Alt” plus “-” on a PC, or “Option” plus “-” on a Mac."
  },
  {
    "objectID": "1-Getting_Started/3-Introducing_R.html#importing-raw-data",
    "href": "1-Getting_Started/3-Introducing_R.html#importing-raw-data",
    "title": "Introducing R - Class Notes",
    "section": "Importing Raw Data",
    "text": "Importing Raw Data\nIn this course, we will read raw data into R and use it for analysis and visualizations. Say there is a dataset stored online that we would like to bring into R to anlayze. We can use the import() function from the rio library to load the data.\nRunning the following code instructs R to import that data…and nothing else.\n\nread.csv('https://raw.githubusercontent.com/byuistats/Math221D_Cannon/master/Data/All_class_combined_personality_data.csv')\n\nAgain, I can’t do anything with it because it hasn’t been stored anywhere. If I want to be able to do anything with the data, I must somehow store it in R. I do this by assigning a name to the data. I can make up any name I like as long as it begins with a letter and has no spaces. Let’s call it “steve”.\n\nsteve &lt;- read.csv('https://raw.githubusercontent.com/byuistats/Math221D_Cannon/master/Data/All_class_combined_personality_data.csv')\n\nNotice how nothing printed out. This is because computers, being literal, assigned a dataset to something I named “steve”. I haven’t told the computer to do anything with it yet. But now I have an R “object” that I can use."
  },
  {
    "objectID": "1-Getting_Started/3-Introducing_R.html#viewing-raw-data",
    "href": "1-Getting_Started/3-Introducing_R.html#viewing-raw-data",
    "title": "Introducing R - Class Notes",
    "section": "Viewing Raw Data",
    "text": "Viewing Raw Data\nThere are several ways to view raw data in R:\n\nRun “steve” by itself\nuse the View() function\n\nRun each line individually by putting the curso on the line you want to run and hitting:\n\nPC: ctrl+Enter\nMac: CMD+Enter\n\n\nsteve\n\nView(steve)\n\nBEST PRACTICE: It is best to name objects in R with descriptive names relating to what the object is. big5_data would be a better name than steve, for example, because this is data about the big 5 personality traits."
  },
  {
    "objectID": "1-Getting_Started/3-Introducing_R.html#selecting-a-column-from-a-dataset",
    "href": "1-Getting_Started/3-Introducing_R.html#selecting-a-column-from-a-dataset",
    "title": "Introducing R - Class Notes",
    "section": "Selecting a Column From a Dataset",
    "text": "Selecting a Column From a Dataset\nDatasets usually have many columns. When we want to select one column from a dataset to summarize, we can use the $ operator. For example, if we want to look at only the scores for Extroversion, we can run the following code:\n\nsteve$Extroversion \n\nThis prints out only the values in the Extroversion column.\nWATCHOUT: The column name has to be typed EXACTLY as it is in the dataset.\nR makes it a little easier because after you type the $ it will bring up a dropdown menu and you can select the column you’re interested in.\nSPACES: If there are spaces in a column name you have to wrap the whole column name using backticks `. On most keyboards, this is on the same key as the ~ next to the 1 key. It’s always easier to use the dropdown menu for complicated column names.\nNOTE: The drop down only works when you’re typing forward. If you delete text back to the dollar sign it won’t show up. To get it back, delete the dollar sign and retype it.\nWe will use the selection $ all very frequently to specify which columns we want to use for data summaries, visualizations, and analyses.\nFor example, I can easily get the mean Openness score by putting steve$Openness into the mean() function:\n\nmean(steve$Openness)\n\n[1] 71.81111"
  },
  {
    "objectID": "1-Getting_Started/1-Course_Intro.html",
    "href": "1-Getting_Started/1-Course_Intro.html",
    "title": "Course Introduction",
    "section": "",
    "text": "By the end of this lesson, you should be able to:\n\nExplain the course policies\nAccess course resources (course outline, lesson schedule, preparation activities, reading quizzes, homework assignments, assessments, etc.)\nCommunicate with the instructor and group members\nAccess statistical analysis software tools for class quizzes, assignments, and exams\nApply principles of the gospel of Jesus Christ in this class\nApply the three rules of probability for different probability scenarios"
  },
  {
    "objectID": "1-Getting_Started/1-Course_Intro.html#lesson-outcomes",
    "href": "1-Getting_Started/1-Course_Intro.html#lesson-outcomes",
    "title": "Course Introduction",
    "section": "",
    "text": "By the end of this lesson, you should be able to:\n\nExplain the course policies\nAccess course resources (course outline, lesson schedule, preparation activities, reading quizzes, homework assignments, assessments, etc.)\nCommunicate with the instructor and group members\nAccess statistical analysis software tools for class quizzes, assignments, and exams\nApply principles of the gospel of Jesus Christ in this class\nApply the three rules of probability for different probability scenarios"
  },
  {
    "objectID": "1-Getting_Started/1-Course_Intro.html#welcome-to-the-course",
    "href": "1-Getting_Started/1-Course_Intro.html#welcome-to-the-course",
    "title": "Course Introduction",
    "section": "Welcome to the Course!",
    "text": "Welcome to the Course!\nIn this course, you will explore important connections between the academic discipline of Statistics and the world around us. By pondering these ideas, your understanding of statistics will increase, as will your knowledge and testimony of the restored Gospel of Jesus Christ.\nThis course has been designed to help you slowly build up a knowledge base of ideas and skills. Not all of these ideas and skills will come easily. It takes a lot of work and practice before some things will even start to make sense, so you should not be surprised to find that it may take you a little time to comprehend these ideas. Just be patient. Once you’re far enough into the course, the ideas will start to come together, and you will see how much progress you have really made. You will understand what this course is all about, and you will be glad you persisted in your efforts to learn."
  },
  {
    "objectID": "1-Getting_Started/1-Course_Intro.html#course-description",
    "href": "1-Getting_Started/1-Course_Intro.html#course-description",
    "title": "Course Introduction",
    "section": "Course Description",
    "text": "Course Description\nThis course covers the following topics as they are applied to Statistics: graphical representations of data, measures of center and spread; elementary probability; sampling distributions; correlation and regression; statistical inference involving means, proportions, and contingency tables.\n\n\nCourse Learning Outcomes\nIn this course, we will:\n\nSummarize data numerically and graphically using R programming language\nMake decisions regarding situations with inherent randomness\nApply probability distributions to investigate questions\nEmploy confidence intervals in various situations\nImplement tests of diverse hypotheses\nCommunicate the results of statistical analyses to relevant audiences\n\n\n\n\nHow the Outcomes will Be Assessed\nWhile you may not be tested on everything you learn in this course, the instructor will be assessing your mastery of the Course Learning Outcomes. You may also be asked to create reports with quality visualizations and analyze new datasets. At times, the instructor may assess your performance of a skill, or the instructor may assess products you create using particular skills. In addition, the instructor may engage in personal communication with you to determine how well you understand the course content.\n\n\n\nKeys to Success\n\nFive Principles of the Learning Model\nYou will experience much deeper learning if you follow the Five Principles of the BYU-Idaho Learning Model\n\nExercise Faith: Exercise faith in the Lord Jesus Christ as a principle of action and power.\nLearn by the Holy Ghost: Understand that true teaching is done by and with the Holy Ghost.\nLay Hold on the Word of God: Lay hold of the word of God.\nAct for Themselves: Act for yourself and accept responsibility for learning and teaching.\nLove, Serve, and Teach One Another: Love, serve, and teach other students in your classes.\n\n\n\nThree Process Steps of the Learning Model\nYou will learn more in less time if you follow the Three Process Steps of the BYU-Idaho Learning Model\n\nPrepare: This involves (a) spiritual preparation, (b) individual preparation, and (c) group preparation.\nTeach One Another: You should (a) be on time, (b) pray together, and (c) actively engage with other students.\nPonder/Prove: You should (a) ponder what you have learned, (b) record your learning, and (c) pursue unanswered questions and discuss what you learn with others.\n\nIf you feel confused or have questions about anything in the lesson, take immediate action (Exercise Faith; Act for Themselves) and talk with your classmates, the teaching assistant, or the instructor (Love, Serve, and Teach One Another).\nTeach One Another\nAt BYU-Idaho, an “A” student will demonstrate “diligent application of Learning Model principles, including initiative in serving other students” (BYU-Idaho Catalog). In this class, you will have the opportunity to work with other students.\nDoctrine and Covenants 84:106 states, “And if any man among you be strong in the Spirit, let him take with him him that is weak, that he may be edified in all meekness, that he may become strong also.” In the spirit of this revelation, you will have the opportunity to help others in the class when you have developed an understanding of a principle. Likewise, you will be able to receive help from others (peers, tutors, TA, and your instructor) when you are still working to understand concepts.\nIn a spirit of love and service, please reach out to others. You are not graded on a curve. If someone else does well, it does not adversely affect you. Research has shown that students who help other students to understand the material gain a much deeper grasp on the concepts of the course. Please take opportunities to help your peers succeed."
  },
  {
    "objectID": "1-Getting_Started/1-Course_Intro.html#course-structure",
    "href": "1-Getting_Started/1-Course_Intro.html#course-structure",
    "title": "Course Introduction",
    "section": "Course Structure",
    "text": "Course Structure\nThis course consists of lessons presented in a topical order in which concepts and skills learned in the earlier lessons provide the requisite knowledge to succeed in later lessons. If the general order of the lessons doesn’t make sense at first, don’t worry. It will all come together in the end, and you’ll see the reasoning behind why the lessons have been presented in this particular order.\nYour main goal as a student will be to complete all of the learning activities within each lesson by their due dates every week. These activities follow a consistent weekly schedule, and it will be up to you to make sure that you keep on pace with all your assignments. These weekly activities may include the following:\n\nReading assigned texts or viewing presentations before class\nCompleting the practice problems\nParticipating in group discussions and assignments with other class members\nWriting papers and/or developing presentations\nParticipating in meetings with the instructor, teaching assistants, and other students\n\nA typical lesson will include a preparation reading, in-class and practice application activites. Because not all material is equally challenging, some lessons will span multiple days. But expect to cover 2-3 lessons a week.\nWhile a general flow of reading, classwork and practice will be the typical lesson flow, there may be adjustments to the schedule from time to time. Changes are typically due to adjusting the pace of the material to support student learning. Please be flexible as we adapt the pace to suit the needs of the class.\nYou should create a study schedule that will keep you on pace throughout the semester. This is a rigorous course with a lot of subject matter to cover, and it can be extremely difficult to recover if you fall too far behind in your work. So, please make every effort to study on a regular basis and get your work turned in on time."
  },
  {
    "objectID": "1-Getting_Started/1-Course_Intro.html#course-materials",
    "href": "1-Getting_Started/1-Course_Intro.html#course-materials",
    "title": "Course Introduction",
    "section": "Course Materials",
    "text": "Course Materials\nThis course has been designed with the student in mind. Every effort has been made to provide a high quality experience at the lowest possible cost.\nTextbook\nTo keep costs as low as possible for students and their families, no physical textbook is required for this class. The readings for this course are provided on this website and will continue to be available to you after the course is completed. Please report any problems with the textbook (links not working, loading slowly, inability to view images, etc.) to your instructor.\nYou may also download all coursework at the main page. This includes all class notes, practice problems and application activities.\nComputer Equipment\nYou will need a laptop with the ability to install software locally.\nNOTE: It is possible to complete this course with a Chromebook or other cloud-based device, but it adds significant complications and stress. For those who are new to programming and statistics, this can be the difference between success and failure in the class."
  },
  {
    "objectID": "1-Getting_Started/1-Course_Intro.html#course-resources",
    "href": "1-Getting_Started/1-Course_Intro.html#course-resources",
    "title": "Course Introduction",
    "section": "Course Resources",
    "text": "Course Resources\nPeer Support\nYour experience in this course will be enhanced as you work with other students to learn and grow together.\nHelp Desk\nThe BYU-Idaho Help Desk has been established to help students with technological problems related to approved course software. You can access the Help Desk at any time in three ways: - Walk-in: The Help Desk is located in room 322 of the McKay Library - Call in: 208-496-1411 (toll free) - Email: helpdesk@byui.edu Additional information is available at the Help Desk web page: http://www.byui.edu/helpdesk/\nWhen you have technical problems with I-Learn, you should first try contacting the Help Desk before you contact your instructor. They are connected with the IT support staff who can resolve problems with I-Learn. Please take a moment now to look at the Help Desk web page. That way, if a problem does arise later on in the course, you will know where to go for help.\nTutoring Center\nThe BYU-Idaho Study Skills/Tutoring Center is a powerful resource for students who would like a little extra help with a course. The Tutoring Center is located in the McKay Library in room 272. This is in the east wing of the second floor.\nThe Tutoring Center provides many services to help students succeed: - Individual tutors - Walk-in tutoring in the Math Study Center (McKay 266 & 270) - Virtual tutoring\nPlease take 5 minutes to explore the Study Skills/Tutoring Center web site.\nFaculty Support\nYour instructor is committed to your success. If you have any needs or concerns, please contact your instructor for help. If you feel yourself getting behind or struggling, talk to your teacher right away. If caught in time, a small problem can be addressed quickly before it grows."
  },
  {
    "objectID": "1-Getting_Started/2-Installing_R.html",
    "href": "1-Getting_Started/2-Installing_R.html",
    "title": "Installing R",
    "section": "",
    "text": "In this course, we will use RStudio to perform necessary visualizations and analyses and to create web-based reports.\nR and RStudio are not the same thing. R is like the engine in a car, and RStudio is the driver’s controls. We don’t need to know the guts of a combustion engine to drive, and we don’t need to know what’s going on in R, but we do need it on our computer.\nTo install R and RStudio on your computer, please follow these instructions:\n\nGo to the R-Studio page and follow the instructions:\na. Install R. The “Download and Install R” button will take you to the R website. You will have to click through a few links to get to the download.\n\nFor PC: “Download R for Windows” &gt; “Install R for the first time” &gt; Whatever the latest version is will be at the top.\n\nFor Mac: “Download R for Mac” &gt; There will be 2 options on the left with the latest version of R (R-4.4.3, for example). Which version you download depends on what kind of processor chip you have, but most will work with the version ending in “arm64.pkg”\n\nReturn to the R-studio download page and click the “Download RStudio Desktop” button. Run the downloaded program and accept the defaults.\n\nOnce the RStudio installer downloads, open the resulting file.\nClick “Continue” or “Okay” or “Accept” for all of the several various windows that will appear.\nOnce the installation finishes you can use your computer’s search bar to search for “RStudio” in your apps.\n\n\nNOTE: If you are using a Chromebook or other “web browsing only” computer that will not allow you to install software locally, then set up an account at RStudio Cloud instead of installing R and RStudio as shown above. Use your BYU-I email and user ID."
  },
  {
    "objectID": "1-Getting_Started/2-Installing_R.html#chromebooks-and-cloud-based-notebooks",
    "href": "1-Getting_Started/2-Installing_R.html#chromebooks-and-cloud-based-notebooks",
    "title": "Installing R",
    "section": "",
    "text": "NOTE: If you are using a Chromebook or other “web browsing only” computer that will not allow you to install software locally, then set up an account at RStudio Cloud instead of installing R and RStudio as shown above. Use your BYU-I email and user ID."
  },
  {
    "objectID": "1-Getting_Started/2-Installing_R.html#keep-calm",
    "href": "1-Getting_Started/2-Installing_R.html#keep-calm",
    "title": "Installing R",
    "section": "Keep Calm",
    "text": "Keep Calm\nIf you have tried going through these instructions and it isn’t working, come to class and we will help troubleshoot."
  },
  {
    "objectID": "1-Getting_Started/2-Installing_R.html#mac-processing-chip",
    "href": "1-Getting_Started/2-Installing_R.html#mac-processing-chip",
    "title": "Installing R",
    "section": "Mac Processing Chip",
    "text": "Mac Processing Chip\nFor Macs, Which version of R-Studio you download depends on which processing chip you have. If you followed the instructions above and R-Studio opens but gives you a big error, you need to download the other version of R linked above."
  },
  {
    "objectID": "1-Getting_Started/4-Installing_Packages.html",
    "href": "1-Getting_Started/4-Installing_Packages.html",
    "title": "Installing Libraries",
    "section": "",
    "text": "Introduction\nR has many built-in toolboxes. R also has access to a vast array of toolboxes that we must install if we want to use them. This is like going to the Home Depot to buy a specialized toolbox and then storing it in your garage. We only have to “buy” it once.\nWe will be using 4 main libraries throughout the semester. This code will walk you through how to install, load and use each of the libraries.\n\n\nInstalling the Libraries\nTo install a library, we use the install.packages(\"\") command, where we specify the library we want in the quotes inside the parentheses.\nThis is something we only have to do once for each library. The 4 main libraries we will use in the class are the tidyverse, mosaic, rio, and car libraries.\nRun the code below by clicking the green arrow at the top right of the code chunk.\nYou can also run individual lines inside of a code chunk by pressing:\n\nPC: ctrl + Enter\nMac: CMD + Enter\n\n\ninstall.packages(\"rio\")\ninstall.packages(\"mosaic\")\ninstall.packages(\"tidyverse\")\ninstall.packages(\"car\")\n\nAgain, the above code only needs to be executed once. It is like going to the Home Depot to purchas a tool box for specific purposes.\n\n\nLoading Libraries\nOnce the toolboxes are “purchased” we still need to get them out of the tool shet when we need to use them. This is something we have to do each time we open and use R Studio for a project.\nWe use the library() function in a code chunk and insert which library we want to use inside the parentheses:\n\nlibrary(rio)\nlibrary(mosaic)\nlibrary(tidyverse)\nlibrary(car)\n\nThe above code chunk is something that will become very familiar as we will load these libraries in every activity.\nEach of the above libraries has special tools for doing managing data, creating graphs, data summarization, etc. that we will learn as we go. While we won’t use all of them in each assignment, it is easier to get in the habit of loading them all for each activity.\n\nrio is useful for loading in data of different types using a single function, import()\nmosaic has many data summarization tools\ntidyverse has a vast range of tools that make it easy to wrangle and visualize data\ncar is a little more specific and is used in this class to do one specific type of chart later in the semester.\n\nNOTE: While you only have to install libraries once, you have to load them every time you want to use one. It’s like going to the garage to get the toolbox you need for the job.\n\n\nTesting, Testing\nAfter running the code chunks above, run the code below to verify that you can read in a dataset and make data summaries:\nUse the import() function to load the dataset by running the following code chunk:\n\nwrong_site &lt;- import(\"https://github.com/byuistats/Math221D_Course/raw/main/Data/WrongSiteWrongPatient.xlsx\") %&gt;% tibble()\n\nwrong_site\n\n# A tibble: 411 × 3\n   Wrong_Site Wrong_Patient Comments                                            \n        &lt;dbl&gt;         &lt;dbl&gt; &lt;chr&gt;                                               \n 1      97465        250000 On rare occasions, a medical procedure is performed…\n 2      36141        106900 or on the wrong patient.  These are called wrong-si…\n 3     102362         62307 &lt;NA&gt;                                                \n 4      69951        192800 The Washington Post highlighted a few cases in a Ju…\n 5      83242         20769 * An ophthalmologist operated on the wrong eye of a…\n 6      12824          2680 * Some men have undergone prostate cancer surgery a…\n 7     129900          4300 * One third of the cases resulted in death or serio…\n 8      51764         30819 &lt;NA&gt;                                                \n 9     145976         23214 The medical field is trying to reduce the number of…\n10     152844         26099 &lt;NA&gt;                                                \n# ℹ 401 more rows\n\n\nYou should see a printout of the first few rows of a dataset. The output will either show up below the code chunk or in the “Console” window. Either way works. I can show you how to adjust the settings in class depending on your preference."
  },
  {
    "objectID": "1-Getting_Started/6-Response_and_Explanatory_Variables.html",
    "href": "1-Getting_Started/6-Response_and_Explanatory_Variables.html",
    "title": "Response and Explanatory Variables",
    "section": "",
    "text": "In this section, we will:\n\nReview the definitions of Response/Dependent variables and Explanatory/Independent variables\n\nLearn some R tools to help us look at the data (View(), glimpse(), names())\n\nLook at several datasets and identify the response variables and explanatory variables\n\nIdentify data types (categorical or quantitative)\n\nIdentify each level of a categorical variable\n\n\n\nDon’t forget, before we can use the tools in the different toolboxes, we need to retrieve them from the shed. In R, that means, loading the libraries:\n\nlibrary(tidyverse)\nlibrary(mosaic)\nlibrary(car)\nlibrary(rio)"
  },
  {
    "objectID": "1-Getting_Started/6-Response_and_Explanatory_Variables.html#load-the-libraries",
    "href": "1-Getting_Started/6-Response_and_Explanatory_Variables.html#load-the-libraries",
    "title": "Response and Explanatory Variables",
    "section": "",
    "text": "Don’t forget, before we can use the tools in the different toolboxes, we need to retrieve them from the shed. In R, that means, loading the libraries:\n\nlibrary(tidyverse)\nlibrary(mosaic)\nlibrary(car)\nlibrary(rio)"
  },
  {
    "objectID": "1-Getting_Started/6-Response_and_Explanatory_Variables.html#data",
    "href": "1-Getting_Started/6-Response_and_Explanatory_Variables.html#data",
    "title": "Response and Explanatory Variables",
    "section": "Data",
    "text": "Data\nDatasets contain rows and columns. Rows in a dataset represent individual observations or records. Each row contains all the relevant data for a single case, instance, or subject being studied. For example, in a dataset about students, each row might represent a different student, with all their associated data (such as name, age, and grade) contained within that row.\nColumns in a dataset represent the different variables or attributes being measured or recorded. Each column contains one specific type of data across all the observations. For example, in the same student dataset, there could be a column for “Name,” a column for “Age,” and another for “Grade.”\nNOTE: Column names in a dataset are considered variables.\nTogether, rows and columns provide a structured format that makes it easy to organize, analyze, and visualize the data.\n\nData Types (Review)\nIn this class, we will use the broad categories of “quantitative” and “categorical” variables to distinguish between numerical values and data that represent group information respectively. As we will see below, it’s not always as easy as it seems to tell the difference.\n\n\nQuantitative Variables\nVariables that are quantitative consists of values that represent measurable quantities.\n\n\nCategroical Variables\nCategorical data consists of values that represent categories or groups. We refer to each of the categories as “levels” of the variable.\nTo see all of the levels of a categorical variable, we can use the unique() function. The code chunk below has a list of favorite pets. The unique() function takes as an input a categorical variable and returns a list of the distinct values.\n\nfavorite_pet &lt;- c(\"cat\", \"cat\", \"dog\", \"ferret\", \"cat\", \"cat\", \"dog\", \"lizard\", \"dog\")\n\nunique(favorite_pet)\n\n[1] \"cat\"    \"dog\"    \"ferret\" \"lizard\"\n\n\nThis lists all the “levels” of a categorical variable."
  },
  {
    "objectID": "1-Getting_Started/6-Response_and_Explanatory_Variables.html#responsedependent-variable",
    "href": "1-Getting_Started/6-Response_and_Explanatory_Variables.html#responsedependent-variable",
    "title": "Response and Explanatory Variables",
    "section": "Response/Dependent Variable",
    "text": "Response/Dependent Variable\nThe response variable, also known as the dependent variable, is the outcome or the variable that researchers are interested in explaining or predicting. Its value depends on the influence of other variables. For example, in a study examining the effect of study time on exam scores, the exam score is the response variable because it changes in response to different amounts of study time."
  },
  {
    "objectID": "1-Getting_Started/6-Response_and_Explanatory_Variables.html#explanatoryindependent-variable",
    "href": "1-Getting_Started/6-Response_and_Explanatory_Variables.html#explanatoryindependent-variable",
    "title": "Response and Explanatory Variables",
    "section": "Explanatory/Independent Variable",
    "text": "Explanatory/Independent Variable\nThe explanatory variable, also known as the independent variable, is the variable that is manipulated or controlled in a statistical experiment to observe its effect on the response variable. It is considered the cause or predictor in the relationship being studied. In the example above, the amount of study time is the explanatory variable, as it is the factor that explains exam scores."
  },
  {
    "objectID": "2-Descriptive_Statistics/1-Numerical_Quantitative_Data_Summaries.html",
    "href": "2-Descriptive_Statistics/1-Numerical_Quantitative_Data_Summaries.html",
    "title": "Numerical Quantitative Data Summaries",
    "section": "",
    "text": "In this section we discuss data summaries for quantitative data. After this lesson, you should be able to:\n\nUnderstand common measures of center (mean, median, mode)\n\nUnderstand common measures of spread (standard deviation, variance, percentiles), why they are important, and how to interpret them\n\nUse R to calculate measures of center and spread\n\nThroughout this section, we will use the “wrong patient” dataset which contains information about malpractice lawsuits of operations performed on the wrong patient or the wrong site on the patient.\n\n\nWe will use R to calculate measures of center and spread using data collected about costs incurred by hospitals due to certain lawsuits. The lawsuits in question were about surgeries performed on the wrong patient, or on the right patient but the wrong part of the patient’s body (the wrong site).\nLoad the libraries and the data into R:\n\nlibrary(rio)\nlibrary(mosaic)\nlibrary(tidyverse)\nlibrary(car)\n\nwrong_patient &lt;- import(\"https://github.com/byuistats/Math221D_Course/raw/main/Data/WrongSiteWrongPatient.xlsx\")"
  },
  {
    "objectID": "2-Descriptive_Statistics/1-Numerical_Quantitative_Data_Summaries.html#reading-data-into-r",
    "href": "2-Descriptive_Statistics/1-Numerical_Quantitative_Data_Summaries.html#reading-data-into-r",
    "title": "Numerical Quantitative Data Summaries",
    "section": "",
    "text": "We will use R to calculate measures of center and spread using data collected about costs incurred by hospitals due to certain lawsuits. The lawsuits in question were about surgeries performed on the wrong patient, or on the right patient but the wrong part of the patient’s body (the wrong site).\nLoad the libraries and the data into R:\n\nlibrary(rio)\nlibrary(mosaic)\nlibrary(tidyverse)\nlibrary(car)\n\nwrong_patient &lt;- import(\"https://github.com/byuistats/Math221D_Course/raw/main/Data/WrongSiteWrongPatient.xlsx\")"
  },
  {
    "objectID": "2-Descriptive_Statistics/1-Numerical_Quantitative_Data_Summaries.html#mean",
    "href": "2-Descriptive_Statistics/1-Numerical_Quantitative_Data_Summaries.html#mean",
    "title": "Numerical Quantitative Data Summaries",
    "section": "Mean",
    "text": "Mean\nThe sample mean or sample arithmetic mean is the most common tool to estimate the center of a distribution. It is referred to simply as the mean. It is computed by adding up the observed data and dividing by the number of observations in the data set.\nIn Statistics, important ideas are given a name. Very important ideas are given a symbol. The sample mean has both a name (mean) and a symbol (\\(\\bar x\\), called “x-bar”).\n\\[\n  \\bar{x} \\text{ is used to denote the sample mean}\n\\]\nYou may have heard people refer to the sample mean as the average. Technically, the word average refers to any number that is used to estimate the center of a distribution. The mean, median and mode are all examples of “averages.” To avoid confusion, it is best to use the words mean, median, and mode instead of the word average, so that it is clear which “average” your are referencing.\nCalculate the mean payout for operations done on the wrong patient:\n\nmean(wrong_patient$Wrong_Patient)\n\n[1] NA\n\n\nWhen there are missing values in the dataset, the mean() function will return NA. We can make R give us a mean by telling the function to remove the NA values:\n\nmean(wrong_patient$Wrong_Patient, na.rm=TRUE)\n\n[1] 46172.05\n\n\nNOTE: The mean is a good measure of center when there are few outliers and the data are fairly symmetric."
  },
  {
    "objectID": "2-Descriptive_Statistics/1-Numerical_Quantitative_Data_Summaries.html#median",
    "href": "2-Descriptive_Statistics/1-Numerical_Quantitative_Data_Summaries.html#median",
    "title": "Numerical Quantitative Data Summaries",
    "section": "Median",
    "text": "Median\nThe median is the middle value in a sorted data set. Half of the observations in the data set are below the median and half are above the median. To find the median, you:\n\nSort the values from smallest to largest\n\nDo one of the following:\n\nIf there are an odd number of values, the median is the middle value in the sorted list.\nIf there are an even number of values, the median is the mean of the two middle values in the sorted list.\n\n\n\nCalculate the Median payout for operations done on the wrong patient using the median() function:\n\nmedian(wrong_patient$Wrong_Patient, na.rm=TRUE)\n\n[1] 18882\n\n\nQUESTION: What does this mean?\nANSWER:\nQUESTION: Notice how much bigger the Mean is from the Median. Why is that the case?\nANSWER:\nQUESTION: Which do you think is most appropriate to use, the Mean or the Median? Why?\nANSWER:\nNOTE: The Median is a good measure of center when the data are skewed or there are large outliers."
  },
  {
    "objectID": "2-Descriptive_Statistics/1-Numerical_Quantitative_Data_Summaries.html#mode",
    "href": "2-Descriptive_Statistics/1-Numerical_Quantitative_Data_Summaries.html#mode",
    "title": "Numerical Quantitative Data Summaries",
    "section": "Mode",
    "text": "Mode\nThe most frequently occurring value is called the mode. This only works well when you have a few possible outcomes or are counting the frequency of categories. If you have truly quantitative data, such as dollar amounts of payouts, it is unlikely to have the exact same value paid out many times.\nIf no number occurs more than once in the data set, we say that there is no mode.\nBecause there is no meaningful mode in the wrong patient data, let’s look at another example to calculate the mode:\nCalculate a Mode\nWe can tabulate the frequency of specific values using the table() function:\n\n# Create a new dataset called data2:\ndata2 &lt;- c(3,4,9,5,2,3,5,4,2,3,1,5,3,1,2,6,2,4,6,2,2,2,9,1,2,7,8,100)\n\n# The `table()` function counts up all the times specific values show up.  This works for numbers or categories:\ntable(data2)\n\ndata2\n  1   2   3   4   5   6   7   8   9 100 \n  3   8   4   3   3   2   1   1   2   1 \n\n\nThe first row of the table() output is the value being counted. The second row is the frequency of occurrence.\nQUESTION: Which value is most frequently occurring?\nANSWER:\nQUESTION: How many times did that value occur?\nANSWER:"
  },
  {
    "objectID": "2-Descriptive_Statistics/1-Numerical_Quantitative_Data_Summaries.html#why-do-we-care",
    "href": "2-Descriptive_Statistics/1-Numerical_Quantitative_Data_Summaries.html#why-do-we-care",
    "title": "Numerical Quantitative Data Summaries",
    "section": "Why do we care?",
    "text": "Why do we care?\nImagine you are growing feed corn on 100 acres. There are 2 products you could purchase, both claim an average yield of 205 bushels/acre. Product A costs less than Product B. Which one would you plant?\nIf you were basing your decision exclusively on the average yield, you would obviously go with the cheaper option. But what if I told you that with Product A you could get anything from 85 bushels/acre to 250 bushels/acre. Product B ranges from 198 to 212 bushels/acre. Would that change your decision?\nProduct A looks much more risky now. Considering both the average AND the variability helps us make much more informed decisions.\nThe average is only half the story!"
  },
  {
    "objectID": "2-Descriptive_Statistics/1-Numerical_Quantitative_Data_Summaries.html#standard-deviation",
    "href": "2-Descriptive_Statistics/1-Numerical_Quantitative_Data_Summaries.html#standard-deviation",
    "title": "Numerical Quantitative Data Summaries",
    "section": "Standard Deviation",
    "text": "Standard Deviation\nThe standard deviation is a measure of the spread in the distribution. If the data tend to be close together, then the standard deviation is relatively small. If the data tend to be more spread out, then the standard deviation is relatively large.\nKEY POINT: Think of the standard deviation as the “average distance” of each data point away from the mean.\nWe can easily calculate the standard deviation in R using the sd() function:\n\nsd(wrong_patient$Wrong_Patient, na.rm=TRUE)\n\n[1] 105986.7\n\n\nThe standard deviation of the payouts is $105,986.70. This number contains information from all the lawsuits. If the payouts had been more diverse, the standard deviation would be larger. If the payouts were more uniform (i.e. closer together), then the standard deviation would have been smaller. If all the payouts somehow had the same amount, then the standard deviation would be zero.\n\nSummary\n\n  Standard Deviation\n  The standard deviation is one number that describes the spread in a set of data. If the data points are close together the standard deviation will be smaller than if they are spread out.\n  At this point, it may be difficult to understand the meaning and usefulness of the standard deviation. For now, it is enough for you to recognize the following points:\n\nThe standard deviation is a measure of how spread out the data are.\nIf the standard deviation is large, then the data are very spread out.\nIf the standard deviation is zero, then all the values are the identical–there is no spread in the data.\nThe standard deviation cannot be negative."
  },
  {
    "objectID": "2-Descriptive_Statistics/1-Numerical_Quantitative_Data_Summaries.html#variance",
    "href": "2-Descriptive_Statistics/1-Numerical_Quantitative_Data_Summaries.html#variance",
    "title": "Numerical Quantitative Data Summaries",
    "section": "Variance",
    "text": "Variance\nThe variance is the square of the standard deviation. The sample variance is denoted by the symbol \\(s^2\\). You found the sample standard deviation for payouts for operating on the wrong patient above is $105,986.70. So, the sample variance for this data set is \\(s^2 = 105,986.70^2 = 11,233,176,611\\).\nYou can also calculate the variance directly from data using the var() function.\n\nvar(wrong_patient$Wrong_Patient, na.rm=TRUE)\n\n[1] 11233176611\n\n\nThe standard deviation and the variance each have their own pros and cons.\nStandard Deviation is in the original units as the data which makes it easy to interpret.\nThe variance is more difficult to interpret but has many important mathematical attributes that make it more appropriate in certain situations not covered in this introductory course."
  },
  {
    "objectID": "2-Descriptive_Statistics/1-Numerical_Quantitative_Data_Summaries.html#percentiles-and-quartiles",
    "href": "2-Descriptive_Statistics/1-Numerical_Quantitative_Data_Summaries.html#percentiles-and-quartiles",
    "title": "Numerical Quantitative Data Summaries",
    "section": "Percentiles and Quartiles",
    "text": "Percentiles and Quartiles\nA percentile is a number such that a specified percentage of the data are at or below this number. For example, the 99th percentile is a number such that 99% of the data are at or below this value. As another example, half (50%) of the data lie at or below the 50th percentile. The word percent means \\(\\div 100\\). This can help you remember that the percentiles divide the data into 100 equal groups.\nQuartiles are special percentiles. The word quartile is from the Latin quartus, which means “fourth.” The quartiles divide the data into four equal groups. The quartiles correspond to specific percentiles. The first quartile, Q1, is the 25th percentile. The second quartile, Q2, is the same as the 50th percentile or the median. The third quartile, Q3, is equivalent to the 75th percentile.\nNOTE: Percentiles can be used to describe the center and spread of any distribution and are particularly useful when the distribution is skewed or has outliers.\nUse R’s quantile() function to calculate percentiles. This functions requires two inputs separated by a comma: the data and the desired percentile input as a decimal.\n\nTo calculate the 25th percentile for the costs of surgery done on the Wrong Site:\n\n\nquantile(wrong_patient$Wrong_Site, .25, na.rm=TRUE)\n\n  25% \n29496 \n\n\nQUESTION: What does this number mean?\nANSWER:\nThe first quartile (\\(Q_1\\)) or 25th percentile (calculated in R) of the wrong-site data is: $29,496. (This result is illustrated in the figure below.) This means that 25 percent of the time hospitals lost a wrong-site lawsuit, they had to pay $29,496 or less. The 25th percentile can be written symbolically as: P25 = $29,496. Other percentiles can be written the same way. The 99th percentile can be written as P99.\nQUESTION: What is the 13th percentile of the wrong patient data?\n\nquantile()\n\nError in quantile(): argument \"x\" is missing, with no default\n\n\nQUESTION: Interpret this value:\nANSWER:\nQUESTION: Find P90.\n\nquantile()\n\nError in quantile(): argument \"x\" is missing, with no default\n\n\nQUESTION: Half of the wrong-site lawsuits judgments were less than or equal to what value?\n\nquantile()\n\nError in quantile(): argument \"x\" is missing, with no default\n\n## OR....\n\n\nThe Five-Number Summary\nAnother way to summarize data is with the five-number summary. The five-number summary is comprised of the minimum, the first quartile, the second quartile (or median), the third quartile, and the maximum.\nThere is a very easy way to get the Five-Number Summary along with the mean and standard deviation. The favstats() function in the mosaic library gives us all of our favorite statistics.\nAs before, we will have to install the mosaic library once, then load it when we want to use it.\nTo find the values for a five-number summary in R, do the following\n\nInput the data into the favstats() function:\n\n\nfavstats(wrong_patient$Wrong_Patient)\n\n min   Q1 median      Q3     max     mean       sd   n missing\n   0 3900  18882 50145.5 1250000 46172.05 105986.7 176     235"
  },
  {
    "objectID": "2-Descriptive_Statistics/1-Numerical_Quantitative_Data_Summaries.html#your-turn",
    "href": "2-Descriptive_Statistics/1-Numerical_Quantitative_Data_Summaries.html#your-turn",
    "title": "Numerical Quantitative Data Summaries",
    "section": "Your Turn",
    "text": "Your Turn\nCreate a summary statistics table for the cost of operating on the wrong site:"
  },
  {
    "objectID": "2-Descriptive_Statistics/3-Quantitative_Data_Summaries_Multiple_Groups.html",
    "href": "2-Descriptive_Statistics/3-Quantitative_Data_Summaries_Multiple_Groups.html",
    "title": "Quantitative Data Summaries - Multiple Groups",
    "section": "",
    "text": "In this section, we will demonstrate how to numerically and visually summarize a quantitative response variable for each level of a categorical explanatory variable.\n\n\n\nCreate a table of summary statistics (favstats()) for multiple groups\n\nCreate side-by-side boxplots comparing multiple groups\n\nInterpret side-by-side boxplots for group comparisons\n\n\n\n\nWe will use the Big 5 Personality data of a random sample of Brother Cannon’s students.\n\nlibrary(rio)\nlibrary(mosaic)\nlibrary(tidyverse)\nlibrary(car)\n\nbig5 &lt;- import('https://raw.githubusercontent.com/byuistats/Math221D_Cannon/master/Data/All_class_combined_personality_data.csv')"
  },
  {
    "objectID": "2-Descriptive_Statistics/3-Quantitative_Data_Summaries_Multiple_Groups.html#lesson-outcomes",
    "href": "2-Descriptive_Statistics/3-Quantitative_Data_Summaries_Multiple_Groups.html#lesson-outcomes",
    "title": "Quantitative Data Summaries - Multiple Groups",
    "section": "",
    "text": "Create a table of summary statistics (favstats()) for multiple groups\n\nCreate side-by-side boxplots comparing multiple groups\n\nInterpret side-by-side boxplots for group comparisons"
  },
  {
    "objectID": "2-Descriptive_Statistics/3-Quantitative_Data_Summaries_Multiple_Groups.html#load-the-data-and-libraries",
    "href": "2-Descriptive_Statistics/3-Quantitative_Data_Summaries_Multiple_Groups.html#load-the-data-and-libraries",
    "title": "Quantitative Data Summaries - Multiple Groups",
    "section": "",
    "text": "We will use the Big 5 Personality data of a random sample of Brother Cannon’s students.\n\nlibrary(rio)\nlibrary(mosaic)\nlibrary(tidyverse)\nlibrary(car)\n\nbig5 &lt;- import('https://raw.githubusercontent.com/byuistats/Math221D_Cannon/master/Data/All_class_combined_personality_data.csv')"
  },
  {
    "objectID": "2-Descriptive_Statistics/3-Quantitative_Data_Summaries_Multiple_Groups.html#summary-statistics",
    "href": "2-Descriptive_Statistics/3-Quantitative_Data_Summaries_Multiple_Groups.html#summary-statistics",
    "title": "Quantitative Data Summaries - Multiple Groups",
    "section": "Summary Statistics",
    "text": "Summary Statistics\nWe can easily extend favstats() to output our favorite statistics for multiple groups.\nWe first must identify the quantitative response variable we want to compare, then tell R which categorical explanatory variable we would like to compare.\nFor example, we could compare agreeableness between the sexes. In this case, Agreeableness is the quantitative response variable and Sex(M/F) is the categorical explanatory variable.\n\n# This gives us the summary statistics for Agreeableness across all groups\nfavstats(big5$Agreeableness)\n\n min Q1 median Q3 max     mean       sd   n missing\n  21 67     75 81 100 73.43457 13.24909 405       0\n\n# Adding the '~' tells R to break the data into groups (determined by the right side of the '~') and calculate the means of the variable on the left\nfavstats(big5$Agreeableness ~ big5$`Sex(M/F)`)\n\n  big5$`Sex(M/F)` min Q1 median Q3 max     mean       sd   n missing\n1               F  21 69     77 85 100 75.92035 12.94640 226       0\n2               M  25 63     73 79  94 70.29609 12.99218 179       0"
  },
  {
    "objectID": "2-Descriptive_Statistics/3-Quantitative_Data_Summaries_Multiple_Groups.html#visual-summaries-by-group",
    "href": "2-Descriptive_Statistics/3-Quantitative_Data_Summaries_Multiple_Groups.html#visual-summaries-by-group",
    "title": "Quantitative Data Summaries - Multiple Groups",
    "section": "Visual Summaries by Group",
    "text": "Visual Summaries by Group\nWe can use the exact same formula used for boxplot() as we used for favstats():\n\nboxplot(big5$Agreeableness ~ big5$`Sex(M/F)`)\n\n\n\n\n\n\n\n\nNOTE: We will use the formula data$response ~ data$explanatory for LOTS of functions this semester. They will always take the form y ~ x."
  },
  {
    "objectID": "2-Descriptive_Statistics/3-Quantitative_Data_Summaries_Multiple_Groups.html#improving-graphs",
    "href": "2-Descriptive_Statistics/3-Quantitative_Data_Summaries_Multiple_Groups.html#improving-graphs",
    "title": "Quantitative Data Summaries - Multiple Groups",
    "section": "Improving Graphs",
    "text": "Improving Graphs\nThroughout this course, we will ease into making better visualizations. For now, here are some basic techniques that will usually apply to all graphing functions in R:\n\n# Changing color by sepecifying the `col = c()`\nboxplot(big5$Agreeableness ~ big5$`Sex(M/F)`, col = c(\"red\", \"blue\"))\n\n\n\n\n\n\n\n# R also assigns a numerical value to `col = `.  Try different numbers\nboxplot(big5$Agreeableness ~ big5$`Sex(M/F)`, col = c(2,3))\n\n\n\n\n\n\n\nboxplot(big5$Agreeableness ~ big5$`Sex(M/F)`, col = c(4,6))\n\n\n\n\n\n\n\n# Adding better axis labels using `xlab = ` and `ylab = `:\nboxplot(big5$Agreeableness ~ big5$`Sex(M/F)`, xlab = \"Biosex\", ylab = \"Trait Agreeableness\")\n\n\n\n\n\n\n\n# Adding a title:\nboxplot(big5$Agreeableness ~ big5$`Sex(M/F)`, main = \"Comparing Agreeableness by Biosex\")\n\n\n\n\n\n\n\n# Putting it all together:\nboxplot(big5$Agreeableness ~ big5$`Sex(M/F)`,main = \"Comparing Agreeableness by Biosex\", xlab = \"Biosex\", ylab = \"Trait Agreeableness\", col = c(3, 4))"
  },
  {
    "objectID": "2-Descriptive_Statistics/5-Univariate_Categorical_Data_Summaries.html",
    "href": "2-Descriptive_Statistics/5-Univariate_Categorical_Data_Summaries.html",
    "title": "Introducing Categorical Data Summaries",
    "section": "",
    "text": "In this section, you will learn:\n\nHow to create a table in R with counts\nHow to create a table in R with percentages\nHow to create bar charts for counts\n\nWe will use the creativity data collected in class.\nDon’t forget to load the libraries and the data by running the code chunk below:\n\n# Load the libraries\nlibrary(rio)\nlibrary(mosaic)\nlibrary(tidyverse)\nlibrary(car)\n\n# Load the data\ncreativity &lt;- import('https://github.com/byuistats/Math221D_Course/raw/main/Data/creativity_scores.csv')"
  },
  {
    "objectID": "2-Descriptive_Statistics/5-Univariate_Categorical_Data_Summaries.html#creating-a-table",
    "href": "2-Descriptive_Statistics/5-Univariate_Categorical_Data_Summaries.html#creating-a-table",
    "title": "Introducing Categorical Data Summaries",
    "section": "Creating a Table",
    "text": "Creating a Table\nTo get a table of counts for a categorical variable, we use the table() function. For example, if we want to see a summary for “Major_Category”:\n\ntable(creativity$Major_Category)\n\n\n      CS       DS       LA     Math    Psych      SCI Wildlife \n       8       15        3        3       28       11       23 \n\n\nWARNING: I hope it isn’t too much of a stretch at this point in the semester to show that you can nest functions. I will build this up step by step. The tricky part is keeping track of the parentheses so that all the input line up. To help make sure things line up, I will sometimes but extra spaces inside the parentheses so I can see the input more clearly.\nYou can put the table created above inside of the sort() function to order the table from smallest to lowest:\n\nsort( table(creativity$Major_Category) )\n\n\n      LA     Math       CS      SCI       DS Wildlife    Psych \n       3        3        8       11       15       23       28 \n\n\nIf we want to reverse the order to make it largest to smallest, we have to tell the sort() function to arrange the numbers from largest to smallest:\n\nsort( table(creativity$Major_Category) , decreasing =TRUE)\n\n\n   Psych Wildlife       DS      SCI       CS       LA     Math \n      28       23       15       11        8        3        3 \n\n\nTo represent this data visually, you can use the barplot()\n\nbarplot(   sort( table(creativity$Major_Category) , decreasing=TRUE)   )\n\n\n\n\n\n\n\n\nNesting multiple functions as demonstrated above can get a little messy. To clean this up a bit, we can name the sorted table and refer back to it as needed.\n\nmaj_cat_table &lt;- sort(table(creativity$Major_Category), decreasing =TRUE)\n\nbarplot(maj_cat_table)\n\n\n\n\n\n\n\n\nSometimes the category labels are long and crowd out other names. If we want to change the font size of the labels, we can input the las = 2 argument into the barplot():\n\nbarplot(maj_cat_table, las=2)\n\n\n\n\n\n\n\n# NOTE: I believe `las` stands for \"label axis style\""
  },
  {
    "objectID": "2-Descriptive_Statistics/7-AA_Descriptive_Statistics.html",
    "href": "2-Descriptive_Statistics/7-AA_Descriptive_Statistics.html",
    "title": "Application Activity: Descriptive Statistics",
    "section": "",
    "text": "In this activity, you will practice everything we’ve covered up to this point including:\n\nReviewing a dataset\n\nDetermining response and explanatory variables\n\nDetermining data types (quantitative, categorical)\n\nCreating descriptive statistics:\n\nQuantitative data (one and two variables)\n\nSummary statistics for a single variable (favstats(data$y))\nHistogram for one variable (histogram(data$y))\n\nSummary statistics for multiple groups (favstats(data$y ~ data$x))\n\nSide-by-side boxplots (boxplot(data$y ~ data$x))\n\nScatter plots, continuous \\(x\\) (plot(data$y ~ data$x))\n\nCorrelation between 2 quantitative variables (correlation coefficient, \\(r\\))\n\n\nCategorical data (one and two variables)\n\nIdentifying levels of a categorical variable (unique())\n\nFrequency and proportion tables (table(data$variable), prop.table(table(data$variable)))\n\nUnivariate, sorted bar charts (barplot(sorted_table_name))\n\nContingency tables (table(row, column))\n\nRow/Column Percents (prop.table(tbl_name, margin = ))\n\nBivariate bar charts (barplot(table_name, beside=TRUE, col=c(2,3,4,...)))\n\n\n\nWe will be using data collected about students in 2 Portuguese schools including their final grade. The goal is to answer research questions using statistical methods to see what factors significantly impact final grades."
  },
  {
    "objectID": "2-Descriptive_Statistics/7-AA_Descriptive_Statistics.html#response-variable-univariate-summary-statistics",
    "href": "2-Descriptive_Statistics/7-AA_Descriptive_Statistics.html#response-variable-univariate-summary-statistics",
    "title": "Application Activity: Descriptive Statistics",
    "section": "Response Variable: Univariate Summary Statistics",
    "text": "Response Variable: Univariate Summary Statistics\nQUESTION: What is the primary response variable? (What are we trying to predict?)\nANSWER:\nQUESTION: Create a table of summary statistics for the primary response variable:\n\nfavstats()\n\nError in favstats(): argument \"x\" is missing, with no default\n\n\nQUESTION: What is the mean grade?\nANSWER:\nQUESTION: What is the median grade?\nANSWER:\nQUESTION: What is the Standard Deviation of grades?\nANSWER:\nQUESTION: Interpret in words what the standard deviation means?\nANSWER:\nQUESTION: What is the 75th percentile of grades?\nANSWER:\nQUESTION: Interpret in words what the 75th percentile means?\nANSWER:\nCreate a histogram of the primary response variable:\nQUESTION: What is the basic shape of the distribution?\nANSWER:\nQUESTION: What anomalies, if any, do you notice?\nANSWER:"
  },
  {
    "objectID": "2-Descriptive_Statistics/7-AA_Descriptive_Statistics.html#explanatory-variables",
    "href": "2-Descriptive_Statistics/7-AA_Descriptive_Statistics.html#explanatory-variables",
    "title": "Application Activity: Descriptive Statistics",
    "section": "Explanatory Variables",
    "text": "Explanatory Variables\nList 5 potential explanatory variables and whether or not they are quantitative or categorical:\n\n\n\n\n\n\n\n\nRelationship Between 2 Variables\n\nCategorical X\nCreate a table of summary statistics for final grade for each level of sex:\n\nfavstats(~)\n\nError in parse(text = input): &lt;text&gt;:2:11: unexpected ')'\n1: \n2: favstats(~)\n             ^\n\n\nQUESTION: What is the mean grade for females?\nANSWER:\nCreate a side-by-side boxplot of final grades for each level of father’s job (Fjob):\nQUESTION: Which level of Fjob tends to have students with the highest final grades?\nANSWER:\nCreate a side-by-side boxplot of final grades for each level of mother’s job (Mjob):\nQUESTION: Which level of Mjob appears to have students with the highest final grades?\nANSWER:\n\n\nQuantitative X\nCreate a scatter plot looking at the relationship between mid-term grade (G2) and final grade (G3):\nQUESTION: Does the relationship appear roughly linear?\nANSWER:\nQUESTION: What anomalies, if any, do you notice?\nANSWER:\nQUESTION: Based on the scatter plot, guess the correlation coefficient:\nANSWER:\nCalculate the correlation coefficient, r:"
  },
  {
    "objectID": "2-Descriptive_Statistics/7-AA_Descriptive_Statistics.html#categorical-descriptive-statistics",
    "href": "2-Descriptive_Statistics/7-AA_Descriptive_Statistics.html#categorical-descriptive-statistics",
    "title": "Application Activity: Descriptive Statistics",
    "section": "Categorical Descriptive Statistics",
    "text": "Categorical Descriptive Statistics\nQUESTION: What are the levels of mother’s profession?\n\nunique()\n\nError in unique.default(): argument \"x\" is missing, with no default\n\n\nCreate a proportion table table for sex\nQUESTION: What percent of respondents are female?\nANSWER:\nCreate a sorted bar chart of the counts of reasons students chose their school (reason):\nQUESTION: Which reason appears to be the most popular?\nANSWER:\nCreate and name a contingency table looking at the relationship between gender and whether or not they are in a romantic relationship.\nThen calculate the proportion for each gender in a romantic relationship (should sum to 1 for each gender):\nQUESTION: What percent of female respondents are in a romantic relationship?\nANSWER:\nQUESTION: What percent of male respondents are in a romantic relationship?\nANSWER:\nCreate a bar chart that groups bars based on sex and colors the bars by romantic.\n\n# Start by naming the table\ntbl_relationship &lt;- table()\n\nError in table(): nothing to tabulate"
  },
  {
    "objectID": "3-Data_Wrangling_Visualization/02-Filter.html",
    "href": "3-Data_Wrangling_Visualization/02-Filter.html",
    "title": "filter()",
    "section": "",
    "text": "Good scientists NEVER delete data from original records. The tidyverse allows us to create a new, clean dataset with a transparent set of steps from which we can create graphs, visualizations and analyses without losing any of the original data.\nWe here demonstrate how to define criteria for choosing which rows to include from the data. Consider the High School survey data which consists of responses to 60 questions from 312 high school students.\n\n# Load libraries and data\n\nlibrary(rio)\nlibrary(mosaic)\nlibrary(tidyverse)\nlibrary(car)\n\nsurvey &lt;- import('https://github.com/byuistats/Math221D_Cannon/raw/master/Data/HighSchoolSeniors_subset.csv') %&gt;% tibble()\n\n\n\nLogical operators are used extensively in computer programming to evaluate if a specified condition is met. They return a “True” or a “False” but are often treated as 1’s and 0’s respectively.\nThe most common logical operators used to filter rows are:\n\n&lt; and &lt;= means “less than” and “less than or equal to” respectively\n&gt; and &gt;= means “greater than” and “greater than or equal to” respectively\n== means “equal to” (NOTE: we use double equals because in most computer languages, a single = is an assignment operator. This avoids ambiguity)\n!= means “not equal to”; this one is useful if you want to eliminate one level of a variable\n%in% selects specified levels you want to include"
  },
  {
    "objectID": "3-Data_Wrangling_Visualization/02-Filter.html#logical-operators",
    "href": "3-Data_Wrangling_Visualization/02-Filter.html#logical-operators",
    "title": "filter()",
    "section": "",
    "text": "Logical operators are used extensively in computer programming to evaluate if a specified condition is met. They return a “True” or a “False” but are often treated as 1’s and 0’s respectively.\nThe most common logical operators used to filter rows are:\n\n&lt; and &lt;= means “less than” and “less than or equal to” respectively\n&gt; and &gt;= means “greater than” and “greater than or equal to” respectively\n== means “equal to” (NOTE: we use double equals because in most computer languages, a single = is an assignment operator. This avoids ambiguity)\n!= means “not equal to”; this one is useful if you want to eliminate one level of a variable\n%in% selects specified levels you want to include"
  },
  {
    "objectID": "3-Data_Wrangling_Visualization/02-Filter.html#filtering-on-categorical-data",
    "href": "3-Data_Wrangling_Visualization/02-Filter.html#filtering-on-categorical-data",
    "title": "filter()",
    "section": "Filtering on Categorical Data",
    "text": "Filtering on Categorical Data\nSuppose for some reason, we only want to include right- or left-handed people (excluding ambidextrous). We can add multiple conditions in the filter() function separated by a comma:\n\n# See what the distinct values are in the Handed column\nunique(survey$Handed)\n\n[1] \"Left-Handed\"  \"Right-Handed\" \"Ambidextrous\"\n\nsurvey %&gt;%\n  filter(Height_cm &lt; 250,\n         Handed != \"Ambidextrous\")\n\n# A tibble: 302 × 60\n   Country Region DataYear ClassGrade Gender Ageyears Handed       Height_cm\n   &lt;chr&gt;   &lt;chr&gt;     &lt;int&gt;      &lt;int&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;            &lt;dbl&gt;\n 1 USA     FL         2022         12 Male         18 Left-Handed        182\n 2 USA     IN         2022         12 Male         17 Right-Handed       190\n 3 USA     GA         2022         12 Female       17 Right-Handed       172\n 4 USA     NC         2022         11 Female       15 Right-Handed       163\n 5 USA     CO         2022         12 Female       17 Left-Handed         51\n 6 USA     MO         2022         11 Male         17 Right-Handed       181\n 7 USA     SC         2022         11 Female       18 Right-Handed       160\n 8 USA     WA         2022         11 Female       16 Right-Handed       156\n 9 USA     WA         2022         12 Female       17 Right-Handed       169\n10 USA     WA         2022         11 Male         18 Right-Handed       160\n# ℹ 292 more rows\n# ℹ 52 more variables: Footlength_cm &lt;dbl&gt;, Armspan_cm &lt;dbl&gt;,\n#   Languages_spoken &lt;dbl&gt;, Travel_to_School &lt;chr&gt;,\n#   Travel_time_to_School &lt;int&gt;, Reaction_time &lt;dbl&gt;,\n#   Score_in_memory_game &lt;dbl&gt;, Favourite_physical_activity &lt;chr&gt;,\n#   Imprtance_reducing_pllutin &lt;int&gt;, Imprtance_recycling_rubbish &lt;int&gt;,\n#   Imprtance_cnserving_water &lt;int&gt;, Imprtance_saving_energy &lt;int&gt;, …\n\n# We could also try using %in% instead of  \"!=\"\n\nsurvey %&gt;%\n  filter(Height_cm &lt; 250,\n         Handed %in% c(\"Left-Handed\", \"Right-Handed\"))\n\n# A tibble: 302 × 60\n   Country Region DataYear ClassGrade Gender Ageyears Handed       Height_cm\n   &lt;chr&gt;   &lt;chr&gt;     &lt;int&gt;      &lt;int&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;            &lt;dbl&gt;\n 1 USA     FL         2022         12 Male         18 Left-Handed        182\n 2 USA     IN         2022         12 Male         17 Right-Handed       190\n 3 USA     GA         2022         12 Female       17 Right-Handed       172\n 4 USA     NC         2022         11 Female       15 Right-Handed       163\n 5 USA     CO         2022         12 Female       17 Left-Handed         51\n 6 USA     MO         2022         11 Male         17 Right-Handed       181\n 7 USA     SC         2022         11 Female       18 Right-Handed       160\n 8 USA     WA         2022         11 Female       16 Right-Handed       156\n 9 USA     WA         2022         12 Female       17 Right-Handed       169\n10 USA     WA         2022         11 Male         18 Right-Handed       160\n# ℹ 292 more rows\n# ℹ 52 more variables: Footlength_cm &lt;dbl&gt;, Armspan_cm &lt;dbl&gt;,\n#   Languages_spoken &lt;dbl&gt;, Travel_to_School &lt;chr&gt;,\n#   Travel_time_to_School &lt;int&gt;, Reaction_time &lt;dbl&gt;,\n#   Score_in_memory_game &lt;dbl&gt;, Favourite_physical_activity &lt;chr&gt;,\n#   Imprtance_reducing_pllutin &lt;int&gt;, Imprtance_recycling_rubbish &lt;int&gt;,\n#   Imprtance_cnserving_water &lt;int&gt;, Imprtance_saving_energy &lt;int&gt;, …\n\n\nNOTE: The %in% and the != will not always give you the same results. If there are misspellings or other options, using %in% will limit the data to only those with the exact spelling in the list provided. For example, someone responding “left handed” (all lower case), would not be included in the clean data. Misspellings would, however, be included if I use != \"Ambitextrous\" because that only removes rows written exactly that way. Things like, ambidextrious or RightHanded would still be included.\nI could further limit my data to students from Florida and Missouri:\n\nnew_data &lt;- survey %&gt;%\n  filter(Height_cm &lt; 250,\n         Handed %in% c(\"Left-Handed\", \"Right-Handed\"),\n         Region %in% c(\"MO\", \"FL\"))\n\ndim(new_data)\n\n[1] 27 60\n\n\nHow many rows does our latest dataset have?"
  },
  {
    "objectID": "3-Data_Wrangling_Visualization/04-Tidyverse_Fundamentals.html",
    "href": "3-Data_Wrangling_Visualization/04-Tidyverse_Fundamentals.html",
    "title": "Introducing the Tidyverse - Part 1",
    "section": "",
    "text": "In statistics classes, you are typically provided simple, clean datasets to load and analyze with ease. This is a terrible disservice to anyone who will deal with data outside of the classroom.\nAnyone who works with data will have to do some data wrangling. Data wrangling is an appropriate description of cleaning, sorting, filtering, summarizing, transforming, and a whole host of other activities to make data usable for a specific purpose.\nIn this document, we introduce a moderately messy dataset and demonstrate basic programming commands to help us get data ready for analysis or visualization."
  },
  {
    "objectID": "3-Data_Wrangling_Visualization/04-Tidyverse_Fundamentals.html#additional-resources",
    "href": "3-Data_Wrangling_Visualization/04-Tidyverse_Fundamentals.html#additional-resources",
    "title": "Introducing the Tidyverse - Part 1",
    "section": "Additional Resources",
    "text": "Additional Resources\nBelow are 2 great resources for digging a little deeper into data manipulation in R.\nTidyverse Cheat Sheet\nR for Data Science\nNext, we will explain a few programming fundamentals that will help make"
  },
  {
    "objectID": "3-Data_Wrangling_Visualization/04-Tidyverse_Fundamentals.html#logical-operators",
    "href": "3-Data_Wrangling_Visualization/04-Tidyverse_Fundamentals.html#logical-operators",
    "title": "Introducing the Tidyverse - Part 1",
    "section": "Logical Operators",
    "text": "Logical Operators\nLogical operators are used extensively in computer programming to determine if a certain condition is met. They always return a “True” or a “False”, but we can treat them like a 0 for false and 1 for true.\nWe can tell a computer to determine a conditional statement (typically “less than”, “greater than” or “not equal to”) for specific variables, and it will return a TRUE if the statement is true and FALSE if not.\n\nQuantitative Variables\nLet’s examine the height_cm column in the survey data.\n\nfavstats(survey$Height_cm)\n\n  min  Q1 median      Q3 max     mean       sd   n missing\n 1.68 161    170 178.125 999 169.2412 53.54382 312       0\n\nhist(survey$Height_cm)\n\n\n\n\n\n\n\n\nThe maximum is 999 cm, which is around 33 Feet! We know this is not a possible value.\nIt is very unlikely that a high school student is taller than 7 feet. We can use a logical operator to see which students are taller than 7 feet (213.36 cm):\n\nsurvey$Height_cm &gt; 213.36\n\n  [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [13] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [25] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [37] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [49] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [61] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [73] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE\n [85] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [97] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[109] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[121] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[133] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[145] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[157] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[169] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[181] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[193] FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[205] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[217] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[229] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[241] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[253] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[265] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[277] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[289] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[301] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n\n# To illustrate, the below code puts the Survey column, Height_cm, along with the TRUE/FALSE logical\ndata.frame(Height_cm =survey$Height_cm, logical = survey$Height_cm &gt; 213.36)[73:84,]\n\n   Height_cm logical\n73     162.0   FALSE\n74     172.0   FALSE\n75     160.0   FALSE\n76     175.0   FALSE\n77     182.8   FALSE\n78     153.0   FALSE\n79     184.0   FALSE\n80     170.0   FALSE\n81     150.0   FALSE\n82     177.8   FALSE\n83     999.0    TRUE\n84     172.7   FALSE\n\n\nWhat does the above code return?\nA list of TRUE and FALSE for every line of the data. It is as long as the number of rows in the dataset.\n\n\nCategorical Variables\nWe can also use logical operators for categorical data. For example, if we wanted to see how many people are ambidextrous, we can run the following:\n\n# What are unique values in of the respondents?\n\nunique(survey$Handed)\n\n[1] \"Left-Handed\"  \"Right-Handed\" \"Ambidextrous\"\n\n# Use a logical operator to get TRUE and FALSE for students who responded \"Ambidextrous\" on the survey question about handedness\n\nsum(survey$Handed == \"Ambidextrous\")\n\n[1] 9\n\ndata.frame(Handed = survey$Handed, logical = survey$Handed == 'Ambidextrous')\n\n          Handed logical\n1    Left-Handed   FALSE\n2   Right-Handed   FALSE\n3   Right-Handed   FALSE\n4   Right-Handed   FALSE\n5    Left-Handed   FALSE\n6   Right-Handed   FALSE\n7   Ambidextrous    TRUE\n8   Right-Handed   FALSE\n9   Right-Handed   FALSE\n10  Right-Handed   FALSE\n11  Right-Handed   FALSE\n12   Left-Handed   FALSE\n13  Right-Handed   FALSE\n14  Right-Handed   FALSE\n15  Right-Handed   FALSE\n16  Right-Handed   FALSE\n17  Right-Handed   FALSE\n18  Right-Handed   FALSE\n19  Right-Handed   FALSE\n20   Left-Handed   FALSE\n21  Right-Handed   FALSE\n22   Left-Handed   FALSE\n23  Right-Handed   FALSE\n24  Right-Handed   FALSE\n25  Right-Handed   FALSE\n26  Right-Handed   FALSE\n27  Right-Handed   FALSE\n28  Ambidextrous    TRUE\n29  Right-Handed   FALSE\n30  Right-Handed   FALSE\n31  Right-Handed   FALSE\n32  Right-Handed   FALSE\n33  Right-Handed   FALSE\n34  Right-Handed   FALSE\n35   Left-Handed   FALSE\n36  Right-Handed   FALSE\n37  Right-Handed   FALSE\n38   Left-Handed   FALSE\n39  Right-Handed   FALSE\n40  Right-Handed   FALSE\n41  Right-Handed   FALSE\n42  Right-Handed   FALSE\n43  Right-Handed   FALSE\n44  Right-Handed   FALSE\n45  Right-Handed   FALSE\n46  Right-Handed   FALSE\n47  Right-Handed   FALSE\n48  Ambidextrous    TRUE\n49  Right-Handed   FALSE\n50  Right-Handed   FALSE\n51  Right-Handed   FALSE\n52  Right-Handed   FALSE\n53  Right-Handed   FALSE\n54  Right-Handed   FALSE\n55  Right-Handed   FALSE\n56   Left-Handed   FALSE\n57  Right-Handed   FALSE\n58  Right-Handed   FALSE\n59  Right-Handed   FALSE\n60  Right-Handed   FALSE\n61  Right-Handed   FALSE\n62  Right-Handed   FALSE\n63  Right-Handed   FALSE\n64  Right-Handed   FALSE\n65  Right-Handed   FALSE\n66  Right-Handed   FALSE\n67  Right-Handed   FALSE\n68  Right-Handed   FALSE\n69  Right-Handed   FALSE\n70  Right-Handed   FALSE\n71  Right-Handed   FALSE\n72  Right-Handed   FALSE\n73  Right-Handed   FALSE\n74   Left-Handed   FALSE\n75   Left-Handed   FALSE\n76  Right-Handed   FALSE\n77  Right-Handed   FALSE\n78  Right-Handed   FALSE\n79  Right-Handed   FALSE\n80   Left-Handed   FALSE\n81  Right-Handed   FALSE\n82   Left-Handed   FALSE\n83  Ambidextrous    TRUE\n84  Right-Handed   FALSE\n85  Right-Handed   FALSE\n86   Left-Handed   FALSE\n87  Right-Handed   FALSE\n88  Right-Handed   FALSE\n89   Left-Handed   FALSE\n90  Right-Handed   FALSE\n91   Left-Handed   FALSE\n92  Right-Handed   FALSE\n93  Right-Handed   FALSE\n94  Right-Handed   FALSE\n95  Right-Handed   FALSE\n96  Right-Handed   FALSE\n97  Right-Handed   FALSE\n98  Right-Handed   FALSE\n99  Right-Handed   FALSE\n100 Right-Handed   FALSE\n101 Right-Handed   FALSE\n102 Right-Handed   FALSE\n103 Right-Handed   FALSE\n104 Right-Handed   FALSE\n105 Right-Handed   FALSE\n106 Right-Handed   FALSE\n107 Right-Handed   FALSE\n108 Right-Handed   FALSE\n109 Right-Handed   FALSE\n110 Right-Handed   FALSE\n111 Right-Handed   FALSE\n112 Right-Handed   FALSE\n113 Right-Handed   FALSE\n114 Right-Handed   FALSE\n115 Right-Handed   FALSE\n116 Right-Handed   FALSE\n117 Right-Handed   FALSE\n118 Right-Handed   FALSE\n119 Right-Handed   FALSE\n120 Right-Handed   FALSE\n121 Right-Handed   FALSE\n122 Right-Handed   FALSE\n123 Right-Handed   FALSE\n124 Right-Handed   FALSE\n125 Right-Handed   FALSE\n126 Right-Handed   FALSE\n127 Right-Handed   FALSE\n128 Right-Handed   FALSE\n129 Right-Handed   FALSE\n130 Right-Handed   FALSE\n131  Left-Handed   FALSE\n132 Right-Handed   FALSE\n133 Right-Handed   FALSE\n134 Right-Handed   FALSE\n135 Right-Handed   FALSE\n136 Right-Handed   FALSE\n137 Right-Handed   FALSE\n138 Right-Handed   FALSE\n139 Right-Handed   FALSE\n140 Right-Handed   FALSE\n141 Right-Handed   FALSE\n142 Right-Handed   FALSE\n143 Ambidextrous    TRUE\n144 Right-Handed   FALSE\n145 Right-Handed   FALSE\n146 Right-Handed   FALSE\n147 Right-Handed   FALSE\n148 Right-Handed   FALSE\n149  Left-Handed   FALSE\n150 Right-Handed   FALSE\n151 Right-Handed   FALSE\n152 Right-Handed   FALSE\n153 Right-Handed   FALSE\n154 Right-Handed   FALSE\n155 Right-Handed   FALSE\n156 Right-Handed   FALSE\n157 Right-Handed   FALSE\n158 Right-Handed   FALSE\n159 Right-Handed   FALSE\n160 Right-Handed   FALSE\n161 Right-Handed   FALSE\n162 Right-Handed   FALSE\n163  Left-Handed   FALSE\n164 Right-Handed   FALSE\n165 Right-Handed   FALSE\n166 Right-Handed   FALSE\n167  Left-Handed   FALSE\n168 Right-Handed   FALSE\n169  Left-Handed   FALSE\n170  Left-Handed   FALSE\n171 Right-Handed   FALSE\n172 Right-Handed   FALSE\n173 Right-Handed   FALSE\n174 Right-Handed   FALSE\n175 Right-Handed   FALSE\n176 Right-Handed   FALSE\n177 Right-Handed   FALSE\n178 Right-Handed   FALSE\n179 Right-Handed   FALSE\n180 Right-Handed   FALSE\n181 Right-Handed   FALSE\n182 Right-Handed   FALSE\n183 Right-Handed   FALSE\n184 Right-Handed   FALSE\n185 Ambidextrous    TRUE\n186 Right-Handed   FALSE\n187 Right-Handed   FALSE\n188 Right-Handed   FALSE\n189 Right-Handed   FALSE\n190 Right-Handed   FALSE\n191 Right-Handed   FALSE\n192 Right-Handed   FALSE\n193 Right-Handed   FALSE\n194 Right-Handed   FALSE\n195 Right-Handed   FALSE\n196 Right-Handed   FALSE\n197 Right-Handed   FALSE\n198 Right-Handed   FALSE\n199 Right-Handed   FALSE\n200 Right-Handed   FALSE\n201 Right-Handed   FALSE\n202 Right-Handed   FALSE\n203 Right-Handed   FALSE\n204 Right-Handed   FALSE\n205 Right-Handed   FALSE\n206  Left-Handed   FALSE\n207 Ambidextrous    TRUE\n208 Right-Handed   FALSE\n209 Ambidextrous    TRUE\n210 Right-Handed   FALSE\n211 Right-Handed   FALSE\n212 Right-Handed   FALSE\n213 Right-Handed   FALSE\n214 Right-Handed   FALSE\n215 Right-Handed   FALSE\n216 Right-Handed   FALSE\n217 Right-Handed   FALSE\n218 Right-Handed   FALSE\n219 Right-Handed   FALSE\n220 Right-Handed   FALSE\n221 Right-Handed   FALSE\n222 Right-Handed   FALSE\n223 Right-Handed   FALSE\n224  Left-Handed   FALSE\n225  Left-Handed   FALSE\n226 Right-Handed   FALSE\n227 Right-Handed   FALSE\n228 Right-Handed   FALSE\n229 Right-Handed   FALSE\n230 Right-Handed   FALSE\n231 Right-Handed   FALSE\n232 Right-Handed   FALSE\n233 Right-Handed   FALSE\n234 Right-Handed   FALSE\n235 Right-Handed   FALSE\n236  Left-Handed   FALSE\n237 Right-Handed   FALSE\n238  Left-Handed   FALSE\n239 Right-Handed   FALSE\n240  Left-Handed   FALSE\n241 Right-Handed   FALSE\n242 Right-Handed   FALSE\n243 Right-Handed   FALSE\n244  Left-Handed   FALSE\n245 Right-Handed   FALSE\n246 Right-Handed   FALSE\n247 Right-Handed   FALSE\n248 Right-Handed   FALSE\n249 Right-Handed   FALSE\n250 Right-Handed   FALSE\n251  Left-Handed   FALSE\n252 Right-Handed   FALSE\n253 Right-Handed   FALSE\n254 Right-Handed   FALSE\n255 Right-Handed   FALSE\n256 Right-Handed   FALSE\n257 Right-Handed   FALSE\n258 Right-Handed   FALSE\n259 Right-Handed   FALSE\n260 Right-Handed   FALSE\n261 Right-Handed   FALSE\n262 Right-Handed   FALSE\n263 Right-Handed   FALSE\n264 Right-Handed   FALSE\n265 Right-Handed   FALSE\n266 Right-Handed   FALSE\n267 Right-Handed   FALSE\n268 Right-Handed   FALSE\n269 Right-Handed   FALSE\n270 Right-Handed   FALSE\n271 Right-Handed   FALSE\n272 Right-Handed   FALSE\n273 Right-Handed   FALSE\n274 Right-Handed   FALSE\n275 Right-Handed   FALSE\n276  Left-Handed   FALSE\n277 Right-Handed   FALSE\n278 Right-Handed   FALSE\n279 Right-Handed   FALSE\n280  Left-Handed   FALSE\n281 Right-Handed   FALSE\n282 Right-Handed   FALSE\n283 Right-Handed   FALSE\n284 Right-Handed   FALSE\n285  Left-Handed   FALSE\n286 Right-Handed   FALSE\n287 Right-Handed   FALSE\n288 Ambidextrous    TRUE\n289  Left-Handed   FALSE\n290 Right-Handed   FALSE\n291  Left-Handed   FALSE\n292 Right-Handed   FALSE\n293 Right-Handed   FALSE\n294 Right-Handed   FALSE\n295 Right-Handed   FALSE\n296 Right-Handed   FALSE\n297 Right-Handed   FALSE\n298 Right-Handed   FALSE\n299  Left-Handed   FALSE\n300 Right-Handed   FALSE\n301 Right-Handed   FALSE\n302 Right-Handed   FALSE\n303 Right-Handed   FALSE\n304 Right-Handed   FALSE\n305 Right-Handed   FALSE\n306 Right-Handed   FALSE\n307  Left-Handed   FALSE\n308 Right-Handed   FALSE\n309 Right-Handed   FALSE\n310 Right-Handed   FALSE\n311 Right-Handed   FALSE\n312 Right-Handed   FALSE\n\n\nWe will show you why this is useful when we introduce tidyverse functions."
  },
  {
    "objectID": "3-Data_Wrangling_Visualization/04-Tidyverse_Fundamentals.html#ceci-nest-pas-une-pipe",
    "href": "3-Data_Wrangling_Visualization/04-Tidyverse_Fundamentals.html#ceci-nest-pas-une-pipe",
    "title": "Introducing the Tidyverse - Part 1",
    "section": "Ceci n’est pas une pipe",
    "text": "Ceci n’est pas une pipe\nThe tidyverse organizes actions to data sequentially. We separate steps by what is called a “pipe” which is programmed %&gt;%.\nHINT: The shortkey for adding a “pipe” is ctrl+shift+m for Windows, and cmd+shift+m on Mac. Learn this because we use them a lot!"
  },
  {
    "objectID": "3-Data_Wrangling_Visualization/04-Tidyverse_Fundamentals.html#removing-rows---filter",
    "href": "3-Data_Wrangling_Visualization/04-Tidyverse_Fundamentals.html#removing-rows---filter",
    "title": "Introducing the Tidyverse - Part 1",
    "section": "Removing Rows - filter()",
    "text": "Removing Rows - filter()\nLogical operators are useful when removing rows from a dataset. The most common logical operators used to filter rows are:\n\n&lt; and &lt;= means “less than” and “less than or equal to” respectively\n&gt; and &gt;= means “greater than” and “greater than or equal to” respectively\n== means “equal to” (NOTE: we use double equals because in most computer languages, a single = is an assignment operator. This avoids ambiguity)\n!= means “not equal to”; this one is useful if you want to eliminate one level of a variable\n%in% is useful for defining a list of levels that you want to include\n\nWe typically begin with the raw dataset, then “pipe” that dataset into a sequence of functions using the “pipe” operator, %&gt;%.\nLet’s begin by filtering out rows we think have legitimate heights:\n\nsurvey %&gt;% \n  filter(Height_cm &lt; 214)\n\n# A tibble: 310 × 60\n   Country Region DataYear ClassGrade Gender Ageyears Handed       Height_cm\n   &lt;chr&gt;   &lt;chr&gt;     &lt;int&gt;      &lt;int&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;            &lt;dbl&gt;\n 1 USA     FL         2022         12 Male         18 Left-Handed        182\n 2 USA     IN         2022         12 Male         17 Right-Handed       190\n 3 USA     GA         2022         12 Female       17 Right-Handed       172\n 4 USA     NC         2022         11 Female       15 Right-Handed       163\n 5 USA     CO         2022         12 Female       17 Left-Handed         51\n 6 USA     MO         2022         11 Male         17 Right-Handed       181\n 7 USA     NC         2022         11 Male         17 Ambidextrous       175\n 8 USA     SC         2022         11 Female       18 Right-Handed       160\n 9 USA     WA         2022         11 Female       16 Right-Handed       156\n10 USA     WA         2022         12 Female       17 Right-Handed       169\n# ℹ 300 more rows\n# ℹ 52 more variables: Footlength_cm &lt;dbl&gt;, Armspan_cm &lt;dbl&gt;,\n#   Languages_spoken &lt;dbl&gt;, Travel_to_School &lt;chr&gt;,\n#   Travel_time_to_School &lt;int&gt;, Reaction_time &lt;dbl&gt;,\n#   Score_in_memory_game &lt;dbl&gt;, Favourite_physical_activity &lt;chr&gt;,\n#   Imprtance_reducing_pllutin &lt;int&gt;, Imprtance_recycling_rubbish &lt;int&gt;,\n#   Imprtance_cnserving_water &lt;int&gt;, Imprtance_saving_energy &lt;int&gt;, …\n\n\nThe above code will return a new dataset without the outliers.\nHow many rows does the original dataset have?\nHow many rows does the filtered dataset have?\nSuppose for some reason, we only want to include right- or left-handed people (excluding ambidextrous). We can add multiple conditions in the filter() function separated by a comma:\n\nunique(survey$Handed)\n\n[1] \"Left-Handed\"  \"Right-Handed\" \"Ambidextrous\"\n\nsurvey %&gt;%\n  filter(Height_cm &lt; 214,\n         Handed != \"Ambidextrous\")\n\n# A tibble: 302 × 60\n   Country Region DataYear ClassGrade Gender Ageyears Handed       Height_cm\n   &lt;chr&gt;   &lt;chr&gt;     &lt;int&gt;      &lt;int&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;            &lt;dbl&gt;\n 1 USA     FL         2022         12 Male         18 Left-Handed        182\n 2 USA     IN         2022         12 Male         17 Right-Handed       190\n 3 USA     GA         2022         12 Female       17 Right-Handed       172\n 4 USA     NC         2022         11 Female       15 Right-Handed       163\n 5 USA     CO         2022         12 Female       17 Left-Handed         51\n 6 USA     MO         2022         11 Male         17 Right-Handed       181\n 7 USA     SC         2022         11 Female       18 Right-Handed       160\n 8 USA     WA         2022         11 Female       16 Right-Handed       156\n 9 USA     WA         2022         12 Female       17 Right-Handed       169\n10 USA     WA         2022         11 Male         18 Right-Handed       160\n# ℹ 292 more rows\n# ℹ 52 more variables: Footlength_cm &lt;dbl&gt;, Armspan_cm &lt;dbl&gt;,\n#   Languages_spoken &lt;dbl&gt;, Travel_to_School &lt;chr&gt;,\n#   Travel_time_to_School &lt;int&gt;, Reaction_time &lt;dbl&gt;,\n#   Score_in_memory_game &lt;dbl&gt;, Favourite_physical_activity &lt;chr&gt;,\n#   Imprtance_reducing_pllutin &lt;int&gt;, Imprtance_recycling_rubbish &lt;int&gt;,\n#   Imprtance_cnserving_water &lt;int&gt;, Imprtance_saving_energy &lt;int&gt;, …\n\n# Equivalently we can use %in% instead of the !=\n\nsurvey %&gt;%\n  filter(Height_cm &lt; 214,\n         Handed %in% c(\"Left-Handed\", \"Right-Handed\"),\n         Region %in% c(\"MO\", \"FL\"))\n\n# A tibble: 27 × 60\n   Country Region DataYear ClassGrade Gender Ageyears Handed       Height_cm\n   &lt;chr&gt;   &lt;chr&gt;     &lt;int&gt;      &lt;int&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;            &lt;dbl&gt;\n 1 USA     FL         2022         12 Male         18 Left-Handed       182 \n 2 USA     MO         2022         11 Male         17 Right-Handed      181 \n 3 USA     MO         2022         11 Female       17 Right-Handed      151 \n 4 USA     FL         2022         12 Female       18 Right-Handed      170 \n 5 USA     FL         2022         12 Male         18 Right-Handed      180 \n 6 USA     FL         2022         12 Female       18 Right-Handed      173.\n 7 USA     FL         2022         12 Male         18 Right-Handed      175 \n 8 USA     FL         2022         12 Male         17 Right-Handed      180 \n 9 USA     MO         2022         11 Male         17 Right-Handed      164 \n10 USA     FL         2022         12 Female       17 Right-Handed      166 \n# ℹ 17 more rows\n# ℹ 52 more variables: Footlength_cm &lt;dbl&gt;, Armspan_cm &lt;dbl&gt;,\n#   Languages_spoken &lt;dbl&gt;, Travel_to_School &lt;chr&gt;,\n#   Travel_time_to_School &lt;int&gt;, Reaction_time &lt;dbl&gt;,\n#   Score_in_memory_game &lt;dbl&gt;, Favourite_physical_activity &lt;chr&gt;,\n#   Imprtance_reducing_pllutin &lt;int&gt;, Imprtance_recycling_rubbish &lt;int&gt;,\n#   Imprtance_cnserving_water &lt;int&gt;, Imprtance_saving_energy &lt;int&gt;, …\n\nsurvey$Handed %in% c(\"Left-Handed\", \"Right-Handed\")\n\n  [1]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE\n [13]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [25]  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [37]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE\n [49]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [61]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [73]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE\n [85]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [97]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[109]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[121]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[133]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE\n[145]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[157]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[169]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[181]  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[193]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[205]  TRUE  TRUE FALSE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[217]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[229]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[241]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[253]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[265]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[277]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE\n[289]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[301]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n\n\nHow many rows does our latest dataset have?"
  },
  {
    "objectID": "3-Data_Wrangling_Visualization/04-Tidyverse_Fundamentals.html#adding-columns---mutate",
    "href": "3-Data_Wrangling_Visualization/04-Tidyverse_Fundamentals.html#adding-columns---mutate",
    "title": "Introducing the Tidyverse - Part 1",
    "section": "Adding Columns - mutate()",
    "text": "Adding Columns - mutate()\nThe mutate() statement is used to add new columns to a dataset.\nTo create a new column, “pipe” the previous steps into the mutate() statement. Inside the parentheses, give the new column a name and set it equal to what you want that column to be.\nEXAMPLE: Create a column of the ratio of Height to armspan called, ht_to_span, by using a mutate() statement:\n\nclean &lt;- survey %&gt;%\n  filter(Height_cm &lt; 214,\n         Handed != \"Ambidextrous\") %&gt;%\n  mutate(ht_to_span = Height_cm / Armspan_cm,\n         ht_in = Height_cm / 2.54) %&gt;%\n  select(Handed, ht_to_span, ht_in)\n\nView(clean)\n\nNotice we no longer have to use $ to access specific columns! The tidyverse lives up to its name!"
  },
  {
    "objectID": "3-Data_Wrangling_Visualization/04-Tidyverse_Fundamentals.html#selecting-columns---select",
    "href": "3-Data_Wrangling_Visualization/04-Tidyverse_Fundamentals.html#selecting-columns---select",
    "title": "Introducing the Tidyverse - Part 1",
    "section": "Selecting Columns - select()",
    "text": "Selecting Columns - select()\nThere are now over 60 columns in this dataset. Suppose we are only interested in reaction times and height-to-armspan ratio as they related to handedness. To tidy up the data even further, select only the columns we are interested in (Handed, Reaction_time, and ht_to_span):\n\nsurvey %&gt;%\n  filter(Height_cm &lt; 214,\n         Handed != \"Ambidextrous\") %&gt;%\n  mutate(ht_to_span = Height_cm / Armspan_cm) %&gt;%\n  select(Handed, Reaction_time, ht_to_span)\n\n# A tibble: 302 × 3\n   Handed       Reaction_time ht_to_span\n   &lt;chr&gt;                &lt;dbl&gt;      &lt;dbl&gt;\n 1 Left-Handed          0.349      1.28 \n 2 Right-Handed         0.358      0.990\n 3 Right-Handed         0.447      1.03 \n 4 Right-Handed         0.438      1.02 \n 5 Left-Handed          0.542      0.981\n 6 Right-Handed         0.428      0.968\n 7 Right-Handed         0.427      1.01 \n 8 Right-Handed         0.412      1.10 \n 9 Right-Handed         0.346      1.04 \n10 Right-Handed         0.391      1    \n# ℹ 292 more rows\n\n\nSee how much we can do in just a few short, sequential lines of code? Let’s name out clean dataset, clean, and create a boxplot of reaction times comparing left and right handed students:\n\nclean &lt;- survey %&gt;%\n  filter(Height_cm &lt; 214,\n         Handed != \"Ambidextrous\") %&gt;%\n  mutate(ht_to_span = Height_cm / Armspan_cm) %&gt;%\n  select(Handed, Reaction_time, ht_to_span)\n\nboxplot(clean$Reaction_time ~ clean$Handed)\n\n\n\n\n\n\n\n# Modify the code to remove outliers in Reaction_time and remake the boxplot\n\nclean &lt;- survey %&gt;%\n  filter(Height_cm &lt; 214,\n         Handed != \"Ambidextrous\") %&gt;%\n  mutate(ht_to_span = Height_cm / Armspan_cm) %&gt;%\n  select(Handed, Reaction_time, ht_to_span)\n\nboxplot(clean$Reaction_time ~ clean$Handed)"
  },
  {
    "objectID": "3-Data_Wrangling_Visualization/04-Tidyverse_Fundamentals.html#summarising-data---group_by-summarise",
    "href": "3-Data_Wrangling_Visualization/04-Tidyverse_Fundamentals.html#summarising-data---group_by-summarise",
    "title": "Introducing the Tidyverse - Part 1",
    "section": "Summarising Data - group_by() + summarise()",
    "text": "Summarising Data - group_by() + summarise()\nThe above data might be adequate for a visualization or analysis, but we can calculate summary statistics tables like we did with favstats() using the tidyverse.\nThe summarise() (or equivalently, summarize()) function is like the mutate statement. We create a name for the new column and set it equal to what we want.\nLet’s name the new dataset, clean, and see how to make summaries using tidyverse.\n\nboxplot(survey$Reaction_time)\n\n\n\n\n\n\n\nclean &lt;- survey %&gt;%\n  filter(Height_cm &lt; 214, \n         Handed != \"Ambidextrous\", \n         Reaction_time &lt; 1,\n         Armspan_cm &gt; 0,\n         ClassGrade == 12) %&gt;%\n  mutate(ht_to_span = Height_cm / Armspan_cm) %&gt;%\n  select(Handed, Reaction_time, ht_to_span)\n\nboxplot(clean$Reaction_time ~ clean$Handed)\n\n\n\n\n\n\n\nclean %&gt;%\n  summarise(\n    mn_react_time = mean(Reaction_time, na.rm=TRUE),\n    med_react_time = median(Reaction_time, na.rm=TRUE),\n    mn_ratio = mean(ht_to_span, na.rm=TRUE)\n  )\n\n# A tibble: 1 × 3\n  mn_react_time med_react_time mn_ratio\n          &lt;dbl&gt;          &lt;dbl&gt;    &lt;dbl&gt;\n1         0.397          0.366     1.27\n\n\nNotice that the mn_ratio is Inf.\nWhy might that be the case?\nModify the code chunk to exclude rows where arm span is 0:\n\nclean &lt;- survey %&gt;%\n  filter(Height_cm &lt; 214,\n         Handed != \"Ambidextrous\",\n         Reaction_time &lt; 1) %&gt;%\n  mutate(ht_to_span = Height_cm / Armspan_cm) %&gt;%\n  select(Handed, Reaction_time, ht_to_span)\n\n\n\nclean %&gt;%\n  summarise(\n    `Mean Reaction Time` = mean(Reaction_time, na.rm=TRUE),\n    med_react_time = median(Reaction_time, na.rm=TRUE),\n    mn_ratio = mean(ht_to_span, na.rm=TRUE),\n    min_react_time = min(Reaction_time, na.rm=TRUE)\n  ) %&gt;% knitr::kable()\n\n\n\n\nMean Reaction Time\nmed_react_time\nmn_ratio\nmin_react_time\n\n\n\n\n0.4205068\n0.388\nInf\n0.067\n\n\n\n\nfavstats(clean$Reaction_time ~ clean$Handed)\n\n  clean$Handed   min      Q1 median      Q3   max      mean        sd   n\n1  Left-Handed 0.274 0.34825 0.4415 0.53850 0.895 0.4741471 0.1691585  34\n2 Right-Handed 0.067 0.33825 0.3845 0.44775 0.995 0.4134380 0.1302318 258\n  missing\n1       0\n2       0\n\nfavstats(clean$ht_to_span)\n\n    min        Q1   median       Q3 max mean  sd   n missing\n 0.0168 0.9841579 1.006519 1.052681 Inf  Inf NaN 292       0\n\n\nIf we want to get means for separate groups, we can add a group_by() statement to tell which variable(s) we want to group by:\n\nclean &lt;- survey %&gt;%\n  filter(Height_cm &lt; 214,\n         Handed != \"Ambidextrous\",\n         Reaction_time &lt; 1,\n         Armspan_cm &gt; 0) %&gt;%\n  mutate(ht_to_span = Height_cm / Armspan_cm) %&gt;%\n  select(Handed, Reaction_time, ht_to_span)\n\n\n\nclean %&gt;%\n  group_by(Handed) %&gt;%\n  summarise(\n    mn_react_time = mean(Reaction_time, na.rm=TRUE),\n    med_react_time = median(Reaction_time, na.rm=TRUE),\n    mn_ratio = mean(ht_to_span, na.rm=TRUE),\n    max_react = max(Reaction_time),\n    count = n()\n  ) %&gt;%\n  knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\nHanded\nmn_react_time\nmed_react_time\nmn_ratio\nmax_react\ncount\n\n\n\n\nLeft-Handed\n0.4741471\n0.4415\n1.266468\n0.895\n34\n\n\nRight-Handed\n0.4133891\n0.3840\n1.260190\n0.995\n257\n\n\n\n\n\nThe n() is very useful for counting up the number of observations in each group.\nIf we were only interested in the summary statistics table, we can do everything in one series of steps:\n\nsummary_stats_table &lt;-  survey %&gt;%\n  filter(\n    Height_cm &lt; 214,\n    Handed != \"Ambidextrous\",\n    Reaction_time &lt; 1,\n    Armspan_cm &gt; 0) %&gt;%\n  mutate(ht_to_span = Height_cm / Armspan_cm) %&gt;%\n  select(Handed, Reaction_time, ht_to_span) %&gt;%\n  group_by(Handed) %&gt;%\n  summarise(\n    mn_react_time = mean(Reaction_time, na.rm=TRUE),\n    med_react_time = median(Reaction_time, na.rm=TRUE),\n    mn_ratio = mean(ht_to_span, na.rm=TRUE),\n    max_react = max(Reaction_time),\n    count = n()\n  )\n\nsummary_stats_table\n\n# A tibble: 2 × 6\n  Handed       mn_react_time med_react_time mn_ratio max_react count\n  &lt;chr&gt;                &lt;dbl&gt;          &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;int&gt;\n1 Left-Handed          0.474          0.442     1.27     0.895    34\n2 Right-Handed         0.413          0.384     1.26     0.995   257"
  },
  {
    "objectID": "3-Data_Wrangling_Visualization/06-GGPlot_Intro.html",
    "href": "3-Data_Wrangling_Visualization/06-GGPlot_Intro.html",
    "title": "Introducing GGPlot!",
    "section": "",
    "text": "GGPlot is a data visualization library that follows Leland Wilkinson’s Grammar of Graphics. The Grammar of Graphics is a systematic approach to how we think about connecting raw data to visual elements.\nThink about a basic sentence in English: The boy threw the ball. This sentence has a subject (the boy), a verb (threw), and a direct object (the ball). While not all sentences include every part of speech, virtually all sentences have at least a subject and a verb.\nThe grammar of graphics has 3 essential components of distinct graphical elements that are needed to make basic “sentences.” They are like the subjects and verbs of English sentences. These elements are:\n\nData layer\nAesthetic mappings\nGeometry layers\n\nThe data layer identifies the data we wish to express visually.\nThe Aesthetic Mapping is a description of how we map specific data elements to specific chart elements. For example, what variable in the data do we want expressed on the X axis or Y axis. We can also map a data variable to the color element.\nLastly, the Geometry Layer tells the computer how to express those Aesthetic Mappings, such as a scatter plot, boxplot, bar chart, etc.\nAs in English, we can make more complex sentences with other graphical elements, but the three mentioned above will be common to all.\nThis sounds more complicated than it is in practice. So let’s look at a familiar example: the Personality data.\n\nlibrary(tidyverse)\nlibrary(mosaic)\nlibrary(rio)\nlibrary(ggplot2)\n\nbig5 &lt;- import('https://raw.githubusercontent.com/byuistats/Math221D_Cannon/master/Data/All_class_combined_personality_data.csv')"
  },
  {
    "objectID": "3-Data_Wrangling_Visualization/06-GGPlot_Intro.html#adding-color",
    "href": "3-Data_Wrangling_Visualization/06-GGPlot_Intro.html#adding-color",
    "title": "Introducing GGPlot!",
    "section": "Adding Color",
    "text": "Adding Color\nTo see the relationship between sepal length and width for each species separately, map the column Species onto the color element in the aesthetic mapping.\nBecause Species is a variable inside the dataset, we put it INSIDE the aes(). This maps Species onto the chart element, color.\n\nggplot(data = iris, mapping = aes(x = Sepal.Length, y = Sepal.Width, color = Species)) +\n  geom_point()\n\n\n\n\n\n\n\n\nTo change ALL the points to a single color, include a “color” statement in the geometry layer:\n\nggplot(data = iris, mapping = aes(x = Sepal.Length, y = Sepal.Width)) +\n  geom_point(color = \"purple\")"
  },
  {
    "objectID": "3-Data_Wrangling_Visualization/06-GGPlot_Intro.html#more-additions",
    "href": "3-Data_Wrangling_Visualization/06-GGPlot_Intro.html#more-additions",
    "title": "Introducing GGPlot!",
    "section": "More Additions",
    "text": "More Additions\nIt is easy to make more interesting graphs that combine multiple geometries or even multiple data layers. For example, if I want to include a trend line on top of the points, simply add a new geometry. The geom_smooth() geometry can add different types of trend lines. We can specify method = 'lm' meaning “linear model” to get a simple line.\n\nggplot(data = iris, mapping = aes(x = Sepal.Length, y = Sepal.Width, color = Species)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")"
  },
  {
    "objectID": "3-Data_Wrangling_Visualization/06-GGPlot_Intro.html#further-customizations",
    "href": "3-Data_Wrangling_Visualization/06-GGPlot_Intro.html#further-customizations",
    "title": "Introducing GGPlot!",
    "section": "Further Customizations",
    "text": "Further Customizations\nWithout changing the underlying “grammar” we can change the “font,” so to speak. To modify the axis labels or add a title and a subtitle, use a labs() layer.\n\nggplot(data = iris, mapping = aes(x = Sepal.Length, y = Sepal.Width, color = Species)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  labs(\n    x = \"Sepal Length\",\n    y = \"Sepal Width\",\n    title = \"Comparing Sepal Length and Sepal Width by Species\"\n  )"
  },
  {
    "objectID": "3-Data_Wrangling_Visualization/06-GGPlot_Intro.html#themes",
    "href": "3-Data_Wrangling_Visualization/06-GGPlot_Intro.html#themes",
    "title": "Introducing GGPlot!",
    "section": "Themes",
    "text": "Themes\nAn easy way to change many visual elements all at once, ggplot() has several pre-packaged themes() you can add to a plot.\nWe typically want high contrast between data points and the background. This makes it easier to perceive differences. Changing the theme of the chart can make lots of changes all at once. theme_bw() is a useful theme which drops the gray default background.\n\nggplot(iris, mapping = aes(x = Sepal.Length, y = Sepal.Width, color = Species)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  labs(\n    x = \"Sepal Length\",\n    y = \"Sepal Width\",\n    title = \"Comparing Sepal Length and Sepal Width by Species\"\n  ) +\n  theme_bw()\n\n\n\n\n\n\n\n\nThere are more themes to try. If you begin typing theme_ you will see a drop down with several other themes.\nExplore some of the themes. Who can come up with the wildest visualization?"
  },
  {
    "objectID": "3-Data_Wrangling_Visualization/06-GGPlot_Intro.html#facets",
    "href": "3-Data_Wrangling_Visualization/06-GGPlot_Intro.html#facets",
    "title": "Introducing GGPlot!",
    "section": "Facets",
    "text": "Facets\nSometimes adding more things to a graph makes it too cluttered. When dealing with multiple groups, you may want to split the graph into several panels, one for each group.\nFacets allow us to split a graph up based on a variable in the data. For example, if we wanted a separate regression plot for each species, we could “add” a facet:\n\nggplot(iris, mapping = aes(x = Sepal.Length, y = Sepal.Width, color = Species)) +\n  geom_point() +\n  facet_grid(~Species) +\n  geom_smooth(method = \"lm\") +\n  labs(\n    x = \"Sepal Length\",\n    y = \"Sepal Width\",\n    title = \"Comparing Sepal Length and Sepal Width by Species\"\n  ) +\n  theme_bw()\n\n\n\n\n\n\n\n\nNotice that the x-axes are the same for each group by default. That is often how we want to visualize data. Sometimes, though, we want to have each graph only cover the range of the data. We can allow the x and y axes to accommodate different ranges of data by setting the “scales” parameter inside the facet_grid to “free”:\n\nggplot(iris, mapping = aes(x = Sepal.Length, y = Sepal.Width, color = Species)) +\n  geom_point() +\n  facet_grid(~Species, scales = \"free\") +\n  geom_smooth(method = \"lm\") +\n  labs(\n    x = \"Sepal Length\",\n    y = \"Sepal Width\",\n    title = \"Comparing Sepal Length and Sepal Width by Species\"\n  ) +\n  theme_bw()\n\n\n\n\n\n\n\n\nNOTE: When we want to “add” something to a graph, we simply include a + and tell it what we want to add. If we want to learn more about any of the graphing elements and their customization, we can always use the question mark help prompts (eg. ?facet_grid)."
  },
  {
    "objectID": "3-Data_Wrangling_Visualization/06-GGPlot_Intro.html#your-turn",
    "href": "3-Data_Wrangling_Visualization/06-GGPlot_Intro.html#your-turn",
    "title": "Introducing GGPlot!",
    "section": "Your Turn",
    "text": "Your Turn\nCreate a side-by-side boxplot using the iris dataset that looks at the distribution of Sepal.Length for each species type.\nBe sure to:\n1. Color the boxes by Species 2. Add theme_bw() to make the chart more high contrast 3. Add a title Sepal Length by Species\nNOTE: Using color = Species with boxplots doesn’t look great. Try fill = Species instead. Using BOTH is not a good idea, but different combinations and see.\n\nggplot() + \n\nError in parse(text = input): &lt;text&gt;:5:0: unexpected end of input\n3: \n4: \n  ^"
  },
  {
    "objectID": "3-Data_Wrangling_Visualization/06-GGPlot_Intro.html#conclusion",
    "href": "3-Data_Wrangling_Visualization/06-GGPlot_Intro.html#conclusion",
    "title": "Introducing GGPlot!",
    "section": "Conclusion",
    "text": "Conclusion\nGGplot provides many options for easily making complex visualizations. While there is far too much to cover in one lesson, the basic framework is fairly intuitive once you get the hang of it."
  },
  {
    "objectID": "3-Data_Wrangling_Visualization/06-GGPlot_Intro.html#histograms-and-density-plots",
    "href": "3-Data_Wrangling_Visualization/06-GGPlot_Intro.html#histograms-and-density-plots",
    "title": "Introducing GGPlot!",
    "section": "Histograms and Density Plots",
    "text": "Histograms and Density Plots\nWhen we want to look at the distribution of a single variable, we typically use histograms. Because this is a single variable, we only define an x without a y.\n\nggplot(iris, mapping = aes(x = Sepal.Length)) +\n  geom_histogram() +\n  theme_bw() +\n  labs(\n    title = \"Distribution of Sepal Length\",\n    x = \"Sepal Length\"\n  )\n\n\n\n\n\n\n\n# We can modify the number of bins in a histogram:  Play around with the \"bin\" Parameter\n\nggplot(iris, mapping = aes(x = Sepal.Length)) +\n  geom_histogram(bins = 20) +\n  theme_bw() +\n  labs(\n    title = \"Distribution of Sepal Length\",\n    x = \"Sepal Length\"\n  )\n\n\n\n\n\n\n\n\nThe above histogram includes data from all species. We can distinguish species in several ways. One is to color the bars by species. Compare the difference between “color=Species” and “fill=Species” inside the aesthetic.\nWARNING: Would not recommend:\n\nggplot(iris, mapping = aes(x = Sepal.Length, fill=Species)) +\n  geom_histogram(bins = 20) +\n  theme_bw() +\n  labs(\n    title = \"Distribution of Sepal Length\",\n    x = \"Sepal Length\"\n  )\n\n\n\n\n\n\n\n\nIt’s not usually a good idea to layer histograms like this because it can obscure what is happening behind the covered layers. This is a situation where faceting can be useful.\nRecall that by default the x-axis will be fixed to the same values for each facet. We can let the x axis scale be different for each group by including scales = \"free\" into the facet_grid argument as above.\nTry both and see which tells a more compelling story:\n\nggplot(iris, mapping = aes(x = Sepal.Length, fill = Species)) +\n  geom_histogram(bins = 10) +\n  facet_grid(~Species) +\n  #facet_grid(~Species, scales = \"free\") +\n  theme_bw() +\n  labs(\n    title = \"Distribution of Sepal Length\",\n    x = \"Sepal Length\"\n  )\n\n\n\n\n\n\n\n\n\nA Better Histogram\nWhile histograms are a fine way to express the distribution of quantitative variables, it is not the only way. A Density plot is a smooth version of a histogram. Density plots use data to calculate a smooth line that expresses the quantitative variable as a continuous value rather than using crude bins.\nBy making the smooth line, it is much easier to compare between groups on the same plot:\n\nggplot(iris, mapping = aes(x = Sepal.Length, color = Species)) +\n  geom_density(linewidth = 1.2) +\n  theme_bw() +\n  labs(\n    title = \"Distribution of Sepal Length\",\n    x = \"Sepal Length\"\n  )"
  },
  {
    "objectID": "3-Data_Wrangling_Visualization/Bonus_GroupBy_Summarise.html",
    "href": "3-Data_Wrangling_Visualization/Bonus_GroupBy_Summarise.html",
    "title": "group_by() + summarise()",
    "section": "",
    "text": "Summarizing Data\nConsider the High School survey data with 60 columns and 312 respondents.\n\n# Load libraries and data\n\nlibrary(rio)\nlibrary(mosaic)\nlibrary(tidyverse)\nlibrary(car)\n\nsurvey &lt;- import('https://github.com/byuistats/Math221D_Cannon/raw/master/Data/HighSchoolSeniors_subset.csv') %&gt;% tibble()\n\nThe mosaic library is great for numerical summaries of quantitative variables using the favstats() function. We can create tables of the 5 number summary, mean, standard deviation, sample size, and number of missing values with one line of code:\n\nfavstats(survey$Height_cm)\n\n  min  Q1 median      Q3 max     mean       sd   n missing\n 1.68 161    170 178.125 999 169.2412 53.54382 312       0\n\n\nWe can add a grouping variable to get the same summary for each level of a group, using ~\n\nfavstats(survey$Height_cm ~ survey$Gender)\n\n  survey$Gender  min  Q1 median     Q3   max     mean       sd   n missing\n1        Female 5.50 160  162.5 167.15 182.8 158.6461 24.53056 152       0\n2          Male 1.68 172  177.9 182.80 999.0 179.3065 69.47610 160       0\n\n\nThis works great if you want to do one response/dependent variable at a time. But we often want specific summaries of data (often by groups) of more than one variable in the dataset.\nWe can use a combination of tidyverse functions, group_by() and summarise() to create custom summary tables.\nThe group_by() signals to R that whatever follows should be done for each level of the column(s) identified inside the parentheses. We can then “pipe” the grouped dataset into a summarize function and define what summary statistics we would like. summarise() works very much like the mutate() function in that we create a name for our summary and tell R how to make it.\nEXAMPLE: Let’s calculate the means for Height_cm, Reaction_time, and Social_Websites_Hours for Males and Females:\n\nclean &lt;- survey %&gt;%\n  group_by(Gender) %&gt;%\n  summarise(\n    mean_height = mean(Height_cm, na.rm=TRUE),\n    mean_react_time = mean(Reaction_time, na.rm=TRUE),\n    mean_social_media_hrs = mean(Social_Websites_Hours, na.rm=TRUE)\n  )\n\nclean\n\n# A tibble: 2 × 4\n  Gender mean_height mean_react_time mean_social_media_hrs\n  &lt;chr&gt;        &lt;dbl&gt;           &lt;dbl&gt;                 &lt;dbl&gt;\n1 Female        159.           0.717                  14.4\n2 Male          179.           0.639                  14.0\n\n\nPro Tip: Recall that the mean() function returns “NA” when there are missing values in the data. Adding na.rm=TRUE to your functions will make sure that you get a mean value.\nEXAMPLE: Let’s do the same means but for handedness:\n\nclean &lt;- survey %&gt;%\n  group_by(Handed) %&gt;%\n  summarise(\n    mean_height = mean(Height_cm, na.rm=TRUE),\n    mean_react_time = mean(Reaction_time, na.rm=TRUE),\n    mean_social_media_hrs = mean(Social_Websites_Hours, na.rm=TRUE)\n  )\n\nclean\n\n# A tibble: 3 × 4\n  Handed       mean_height mean_react_time mean_social_media_hrs\n  &lt;chr&gt;              &lt;dbl&gt;           &lt;dbl&gt;                 &lt;dbl&gt;\n1 Ambidextrous        255.           0.352                  20.2\n2 Left-Handed         164.           1.45                   14.5\n3 Right-Handed        167.           0.583                  13.9\n\n\n\nCombining Tidy Functions\n\n\n\nClick to see how to filter outliers for reaction times (reaction times greater than 1 second), and height outliers (taller than 7 feet tall), and social media hours (more than 100 hours).\n\n\n\nClick to see\n\n\nclean &lt;- survey %&gt;%\n  filter(Height_cm &lt; 214,\n         Reaction_time &lt; 1,\n         Social_Websites_Hours &lt; 100) %&gt;%\n  select(Gender, Height_cm, Reaction_time, Social_Websites_Hours)\n\n# Pipe the new clean dataset into the group_by() and summarise() as above:\n\nclean %&gt;%\n  group_by(Gender) %&gt;%\n  summarise(\n    mean_height = mean(Height_cm, na.rm=TRUE),\n    sd_ht = sd(Height_cm, na.rm=TRUE),\n    mean_react_time = mean(Reaction_time, na.rm=TRUE),\n    sd_react_time = sd(Reaction_time, na.rm=TRUE),\n    mean_social_media_hrs = mean(Social_Websites_Hours, na.rm=TRUE),\n    sd_social_hrs = sd(Social_Websites_Hours, na.rm=TRUE)\n  )\n\n# A tibble: 2 × 7\n  Gender mean_height sd_ht mean_react_time sd_react_time mean_social_media_hrs\n  &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt;           &lt;dbl&gt;         &lt;dbl&gt;                 &lt;dbl&gt;\n1 Female        158.  25.1           0.444         0.134                  12.9\n2 Male          174.  23.4           0.395         0.139                  12.5\n# ℹ 1 more variable: sd_social_hrs &lt;dbl&gt;\n\n\n\n\n\n\nGrouping by Multiple Variables\nIt is simple to get summary statistics for multiple grouping factors.\nEXAMPLE: Suppose we want the same means calculated above, but for gender and handedness:\n\nclean &lt;- survey %&gt;%\n  group_by(Gender, Handed) %&gt;%\n  summarise(\n    mean_height = mean(Height_cm, na.rm=TRUE),\n    mean_react_time = mean(Reaction_time, na.rm=TRUE),\n    mean_social_media_hrs = mean(Social_Websites_Hours, na.rm=TRUE)\n  )\n\nclean\n\n# A tibble: 6 × 5\n# Groups:   Gender [2]\n  Gender Handed       mean_height mean_react_time mean_social_media_hrs\n  &lt;chr&gt;  &lt;chr&gt;              &lt;dbl&gt;           &lt;dbl&gt;                 &lt;dbl&gt;\n1 Female Ambidextrous        134.           0.361                  27  \n2 Female Left-Handed         160.           0.524                  12.8\n3 Female Right-Handed        159.           0.750                  14.3\n4 Male   Ambidextrous        315.           0.348                  16.8\n5 Male   Left-Handed         167.           2.29                   16.1\n6 Male   Right-Handed        175.           0.420                  13.5\n\n\nI can also use the n() function without any inputs to count the number of observations in each group:\n\nclean &lt;- survey %&gt;%\n  group_by(Gender, Handed) %&gt;%\n  summarise(\n    mean_height = mean(Height_cm, na.rm=TRUE),\n    mean_react_time = mean(Reaction_time, na.rm=TRUE),\n    mean_social_media_hrs = mean(Social_Websites_Hours, na.rm=TRUE),\n    N = n()\n  )\n\nclean\n\n# A tibble: 6 × 6\n# Groups:   Gender [2]\n  Gender Handed       mean_height mean_react_time mean_social_media_hrs     N\n  &lt;chr&gt;  &lt;chr&gt;              &lt;dbl&gt;           &lt;dbl&gt;                 &lt;dbl&gt; &lt;int&gt;\n1 Female Ambidextrous        134.           0.361                  27       3\n2 Female Left-Handed         160.           0.524                  12.8    17\n3 Female Right-Handed        159.           0.750                  14.3   132\n4 Male   Ambidextrous        315.           0.348                  16.8     6\n5 Male   Left-Handed         167.           2.29                   16.1    19\n6 Male   Right-Handed        175.           0.420                  13.5   135\n\n\nThis shows me that there are only 3 female ambidextrous students in the sample and 6 male ambidextrous students."
  },
  {
    "objectID": "3-Data_Wrangling_Visualization/Bonus_Mutate.html",
    "href": "3-Data_Wrangling_Visualization/Bonus_Mutate.html",
    "title": "mutate()",
    "section": "",
    "text": "Adding Columns\nIf we want to create a new column in our dataset, we use the tidy function, mutate(). Consider again the High School survey data with 60 columns and 312 respondents.\n\n# Load libraries and data\n\nlibrary(rio)\nlibrary(mosaic)\nlibrary(tidyverse)\nlibrary(car)\n\nsurvey &lt;- import('https://github.com/byuistats/Math221D_Cannon/raw/master/Data/HighSchoolSeniors_subset.csv') %&gt;% tibble()\n\nTo create a new column, “pipe” the raw data into the mutate() statement. Inside the parentheses, give the new column a name and set it equal to what you want that column to be.\nEXAMPLE: It is widely known that arm span is typically very close to one’s height. Let’s create a column of the ratio of Height (Height_cm) to armspan (Armspan_cm) and call the new column, ht_to_span. If common knowledge is correct, we would expect the ratio to be close to 1 on average.\n\nclean &lt;- survey %&gt;%\n  mutate(ht_to_span = Height_cm / Armspan_cm)\n\nmean(clean$ht_to_span)\n\n[1] Inf\n\n\nNotice that the mean is Inf which means infinity, or undefined. This is likely due to R attempting to divide a number by zero, meaning someone answered that their arm span was zero. This is where filter() comes in handy. We can filter out the rows where Armspan_cm is 0:\n\nclean &lt;- survey %&gt;%\n  filter(Armspan_cm &gt; 0) %&gt;%\n  mutate(ht_to_span = Height_cm / Armspan_cm)\n\nhistogram(clean$ht_to_span, xlab = \"Height / Armspan\", main = \"Distribution of Height to Armspan Ratio\")\n\n\n\n\n\n\n\n\nIt’s difficult to imagine someone who is 40 times taller than his or her arm span. But this is sufficient to illustrate the mutate() function.\nEXAMPLE: Now let’s make a new column that converts Height_cm into inches:\n\nclean &lt;- survey %&gt;%\n  mutate(ht_to_span = Height_cm / Armspan_cm,\n         Height_in = Height_cm / 2.54)\n\n\nCombining Tidy Functions\n\n\n\nClick to see how to filter outliers of ht_to_wt ratio and select only the columns of interest.\n\n\n\nClick to see\n\n\nclean &lt;- survey %&gt;%\n  filter(Armspan_cm &gt; 0) %&gt;%\n  mutate(ht_to_span = Height_cm / Armspan_cm,\n         Height_in = Height_cm / 2.54) %&gt;%\n  select(Height_cm, Armspan_cm, ht_to_span, Height_in) %&gt;% \n  filter(ht_to_span &lt; 1.5,\n         ht_to_span &gt; .5)\nclean\n\n# A tibble: 268 × 4\n   Height_cm Armspan_cm ht_to_span Height_in\n       &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;\n 1       182       142       1.28       71.7\n 2       190       192       0.990      74.8\n 3       172       167       1.03       67.7\n 4       163       160       1.02       64.2\n 5        51        52       0.981      20.1\n 6       181       187       0.968      71.3\n 7       160       159       1.01       63.0\n 8       156       142.      1.10       61.4\n 9       169       162       1.04       66.5\n10       160       160       1          63.0\n# ℹ 258 more rows\n\nhistogram(clean$ht_to_span, xlab = \"Height / Armspan\", main = \"Distribution of Height to Armspan Ratio\")\n\n\n\n\n\n\n\n\nI decided on 1.5 and .5 as the filter values by trial and error. There are better ways to determine outliers. But I suspect it is rare indeed for someone to be 50% taller than their armspan or 50% shorter."
  },
  {
    "objectID": "4-Foundations_Statistical_Inference/02-The_Normal_Distribution_Textbook.html",
    "href": "4-Foundations_Statistical_Inference/02-The_Normal_Distribution_Textbook.html",
    "title": "The Normal Distribution (Reading)",
    "section": "",
    "text": "By the end of this lesson, you should be able to:\n\nState the properties of a normal distribution\nCalculate the z-score for an individual observation, given the population mean and standard deviation\nInterpret a z-score\nUse the normal distribution to calculate probabilities for one observation\nApproximate probabilities from a normal distribution using the 68-95-99.7 rule\nCalculate a percentile using the normal distribution"
  },
  {
    "objectID": "4-Foundations_Statistical_Inference/02-The_Normal_Distribution_Textbook.html#lesson-outcomes",
    "href": "4-Foundations_Statistical_Inference/02-The_Normal_Distribution_Textbook.html#lesson-outcomes",
    "title": "The Normal Distribution (Reading)",
    "section": "",
    "text": "By the end of this lesson, you should be able to:\n\nState the properties of a normal distribution\nCalculate the z-score for an individual observation, given the population mean and standard deviation\nInterpret a z-score\nUse the normal distribution to calculate probabilities for one observation\nApproximate probabilities from a normal distribution using the 68-95-99.7 rule\nCalculate a percentile using the normal distribution"
  },
  {
    "objectID": "4-Foundations_Statistical_Inference/02-The_Normal_Distribution_Textbook.html#normal-distributions-and-normal-computations",
    "href": "4-Foundations_Statistical_Inference/02-The_Normal_Distribution_Textbook.html#normal-distributions-and-normal-computations",
    "title": "The Normal Distribution (Reading)",
    "section": "Normal Distributions and Normal Computations",
    "text": "Normal Distributions and Normal Computations\n\nBaseball Batting Averages\n\nIn baseball, a player called the “pitcher” throws a ball to a player called the “batter.” The batter swings a wooden or metal bat and tries to hit the ball. A “hit” is made when the batter successfully hits the ball and runs to a point in the field called first base. A player’s batting average is calculated as the ratio of the number of hits a player makes divided by the number of times the player has attempted to hit the ball or in other words, been “at bat.” Sean Lahman reported the batting averages of several professional baseball players in the United States. (Lahman, 2010) The file BattingAverages.xlsx contains his data.\nThe following histogram summarizes the batting averages for these professional baseball players:\n\nNotice the bell-shaped distribution of the data.\nSuppose we want to estimate the probability that a randomly selected player will have a batting average that is greater than 0.280. One way to do this would be to find the proportion of players in the data set who have a batting average above 0.280. We can do this by finding the number of players who fall into each of the red-colored bins below and dividing this number by the total number of players.\n\nIn other words, we could find the proportion of the total area of the bars that is shaded red out of the combined area of all the bars. This gives us the proportion of players whose batting averages are greater than 0.280.\nOut of the 446 players listed, there are a total of 133 players with batting averages over 0.280. This suggests that the proportion of players whose batting average exceeds 0.280 is:\n\\[\\displaystyle{\\frac{133}{446}} = 0.298\\]\nAlternatively, we can use the fact that the data follow a bell-shaped distribution to find the probability that a player has a batting average above 0.280.\n\n\nDensity Curves\nThe bell-shaped curve superimposed on the histogram above is called a density curve. It is essentially a smooth histogram. Notice how closely this curve follows the observed data.\nThe density curve illustrated on the histogram of the batting average data is special. It is called a normal density curve. This density curve is symmetric and has a bell-shape.\nThe normal density curve is also referred to as a normal distribution or a “Gaussian” distribution (after Carl Friedrich Gauss.)\nThe normal density curve appears in many applications in business, nature, medicine, psychology, sociology, and more. We will use the normal density curve extensively in this course.\nAll density curves, including normal density curves, have two basic properties:\n\nThe total area under the curve equals 1.\nThe density curve always lies on or above the horizontal axis.\n\nBecause of these two properties, the area under the curve can be treated as a probability. If we want to find the probability that a randomly selected baseball player will have a batting average between some range of values, we only need to find the area under the curve in that range. This is illustrated by the region shaded in blue in the figure below.\n\nA normal density curve is uniquely determined by its mean, \\(\\mu\\), and its standard deviation, \\(\\sigma\\). So, if random variables follow a normal distribution with a known mean and standard deviation, then we can calculate any probabilities related to that variable by finding the area under the curve.\nWhen the mean of a normal distribution is 0 and its standard deviation is 1, we call it the standard normal distribution.\nWe will return to this example later, and we will find the area shaded in blue.\n\nCharacteristics of the Normal Curve\n\nIntroduction to \\(z\\)-scores\nIn Ghana, the mean height of young adult women is normally distributed with mean \\(159.0\\) cm and standard deviation \\(4.9\\) cm. (Monden & Smits, 2009) Serwa, a female BYU-Idaho student from Ghana, is \\(169.0\\) cm tall. Her height is \\(169.0 - 159.0 = 10\\) cm greater than the mean. When compared to the standard deviation, she is about two standard deviations (\\(\\approx 2 \\times 4.9\\) cm) taller than the mean.\nThe heights of men are also normally distributed. The mean height of young adult men in Brazil is \\(173.0\\) cm (“Oramento,” 2010), and the standard deviation for the population is \\(6.3\\) cm. (Castilho & Lahr, 2001) A Brazilian BYU-Idaho student, Gustavo, is \\(182.5\\) cm tall. Compared to other Brazilians, he is taller than the mean height of Brazilian men.\n\nAnswer the following question:\n\n\n\nApproximately how many standard deviations above the mean is Gustavo’s height?\n\n\n\nShow/Hide Solution\n\n\nGustavo’s height is \\(182.5 - 173.0 = 9.5 \\text{ cm  }\\) above the mean. The standard deviation of the height of Brazilian men is 6.3 cm, so his height is \\(\\displaystyle{ \\frac{9.5}{6.3} =1.508 }\\) standard deviations above the mean.\n\n\n\n\n\n\nComputing \\(z\\)-scores\nWhen we examined the heights of Serwa and Gustavo, we compared their height to the standard deviation. If we look carefully at the steps we did, we subtracted the mean height for people of the same gender and nationality from each individual’s height, respectively.\nThis shows how much taller or shorter the person is than the mean height. In order to compare the height difference to the standard deviation, we divide the difference by the standard deviation. This gives the number of standard deviations the individual is above or below the mean.\nFor example, Serwa’s height is 169.0 cm. If we subtract the mean and divide by the standard deviation, we get \\[z = \\frac{169.0 - 159.0}{4.9} = 2.041\\] We call this number a \\(z\\)-score. The \\(z\\)-score for a data value tells how many standard deviations away from the mean the observation lies. If the \\(z\\)-score is positive, then the observed value lies above the mean. A negative \\(z\\)-score implies that the value was below the mean.\nWe compute the \\(z\\)-score for Gustavo’s height similarly, and obtain \\[z = \\frac{182.5 - 173.0}{6.3} = 1.508\\] Gustavo’s \\(z\\)-score is 1.508. As noted above, this is about one-and-a-half standard deviations above the mean. In general, if an observation \\(x\\) is taken from a random process with mean \\(\\mu\\) and standard deviation \\(\\sigma\\), then the \\(z\\)-score is \\[z = \\frac{x -\\mu }{\\sigma}\\]\nThe \\(z\\)-score can be computed for data from any distribution, but it is most commonly applied to normally distributed data.\n\n\n68-95-99.7% Rule for Bell-shaped Distributions\nHeights of women (or men) in a particular population follow a normal distribution. Most people’s heights are close to the mean. A few are very tall or very short. We would like to make a more precise statement than this.\n\nFor any bell-shaped distribution,\n\n68% of the data will lie within 1 standard deviation of the mean,\n95% of the data will lie within 2 standard deviations of the mean, and\n99.7% of the data will lie within 3 standard deviations of the mean.\n\nThis is called the 68-95-99.7% Rule for Bell-shaped Distributions. Some statistics books refer to this as the Empirical Rule. \n\n\n\nApproximately 68% of the observations from a bell-shaped distribution will be between the values of \\(\\mu -~\\sigma~\\)and \\(\\mu +~\\sigma\\). Consider the heights of young adult women in Ghana. We expect that about 68% of Ghanaian women have a height between the values of \\[\\mu -~\\sigma = 159.0 - 4.9 = 154.1~\\text{cm}\\] and \\[\\mu +~\\sigma = 159.0 + 4.9 = 163.9~\\text{cm}.\\]\nSo, if a female is chosen at random from all the young adult women in Ghana, about 68% of those chosen will have a height between 154.1 and 163.9 cm. Similarly, 95% of the women’s heights will be between the values of \\[\\mu - 2\\sigma = 159.0 - 2(4.9) = 149.2~\\text{cm}\\] and \\[\\mu + 2\\sigma = 159.0 + 2(4.9) = 168.8~\\text{cm}.\\]\nFinally, 99.7% of the women’s heights will be between \\[\\mu - 3\\sigma = 159.0 - 3(4.9) = 144.3~\\text{cm}\\] and \\[\\mu + 3\\sigma = 159.0 + 3(4.9) = 173.7~\\text{cm}.\\]\n\n\n\nUnusual Events\nIf a \\(z\\)-score is extreme (either a large positive number or a large negative number), then that suggests that that observed value is very far from the mean. The 68-95-99.7% rule states that 95% of the observed data values will be within two standard deviations of the mean. This means that 5% of the observations will be more than 2 standard deviations away from the mean (either to the left or to the right).\nWe define an unusual observation to be something that happens less than 5% of the time. For normally distributed data, we determine if an observation is unusual based on its \\(z\\)-score. We call an observation unusual if \\(z &lt; -2\\) or if \\(z &gt; 2\\). In other words, we will call an event unusual if the absolute value of its \\(z\\)-score is greater than 2.\n\nAnswer the following questions:\n\n\n\nOut of Serwa and Gustavo, who is physically taller?\n\n\n\nShow/Hide Solution\n\n\nGustavo is taller. He is 182.5 cm tall, and Serwa is 169.0 cm tall.\n\n\n\nRelative to their own gender and nationality, who is relatively taller?\n\n\n\nShow/Hide Solution\n\n\nRelative to other Ghanaian women, Serwa is very tall. Gustavo is tall relative to Brazilian men, but relative to people of his gender and nationality, he is not relatively taller than Serwa. Serwa has a higher \\(z\\)-score.\n\n\n\nAre either of these heights unusual?\n\n\n\nShow/Hide Solution\n\n\nSerwa’s height is unusual. Her \\(z\\)-score is: \\(z = 2.041\\) This is more than two standard deviations away from the mean. Gustavo’s height is not unusual. His \\(z\\)-score is less than two standard deviations away from the mean.\n\n\n\n\n“Piled Higher and Deeper” by Jorge Cham"
  },
  {
    "objectID": "4-Foundations_Statistical_Inference/02-The_Normal_Distribution_Textbook.html#normal-probability-computations",
    "href": "4-Foundations_Statistical_Inference/02-The_Normal_Distribution_Textbook.html#normal-probability-computations",
    "title": "The Normal Distribution (Reading)",
    "section": "Normal Probability Computations",
    "text": "Normal Probability Computations\nAn important part of the practice of statistics is finding areas under a normal curve. The area under a normal curve, say, to the left of a value, gives the probability of obtaining an observation less than (or equal to) that value. This is an example of converting a value to an area. It is also important to convert an area to a value. For example, if you want to find the 40th percentile for data that follow a normal distribution, you find the value of the observation such that the area (under the curve) to the left of this value is 0.40.\n\nIntroduction to the Normal Probability Applet\nThe Normal Probability Applet is a visualization program offering statistics students insights and computations for the relationship between \\(z\\)-scores and areas under the standard normal curve. You can find a link to this applet here. This app is also compatible to use on your phone, iPad, and other mobile devices. The app is stored at . If you want a copy you can open from your desktop, just right click the link and save it to your computer.\nTo use this applet, follow these instructions:\n\nClick on an area below the curved line to shade/unshade the region.\nClick and drag the red lines to adjust the \\(z\\)-scores and obtain the area.\nType a \\(z\\)-score into one of the bottom input boxes and hit “Enter” to get an area.\nType in an area and hit “Enter” to get a \\(z\\)-score.\n\n\n\n\n\n\n\n\n\nConverting a \\(z\\)-score to a Probability\nUsing this applet we can calculate proportions and probabilities based on the area under the normal curve. For the following examples, please open the Normal Probability Applet and practice using it to find areas under the curve.\nThe Normal Probability Applet is nice for visualizing areas under the curve. However, it has significant limitations. In this class, we will use R to do the heavy lifting. The pnorm(x, \\(\\mu\\), \\(\\sigma\\)) function calculates areas under the curve the normal distribution with mean, \\(\\mu\\), and standard deviation, \\(\\sigma\\), for a given x. The p in pnorm() stands for probability and norm obviously stands for the normal distribution.\nBy default, pnorm() gives the area of the curve to the LEFT of the value, \\(x\\), for a normal distribution with a mean, \\(\\mu\\), and standard deviation, \\(\\sigma\\).\nWe can calculate probabilities directly in the original units of the data, or use the z-score with a \\(\\mu=0\\) and \\(\\sigma=1\\): pnorm(z).\nNOTE: pnorm() has a default value for the mean, \\(\\mu = 0\\) and standard deviation, \\(\\sigma = 1\\) so we don’t have to input those values when we use a z-score with the standard normal distribution.\n\nHeights of Ghanaian Women\nWe will use the example of Serwa’s height to find the proportion of young Ghanaian women who are shorter than Serwa. Recall that for the height of young Ghanaian women, the population mean is 159.0 cm and the population standard deviation is 4.9 cm. Serwa’s height is 169.0 cm. We found the \\(z\\)-score of Serwa’s height as:\n\\[z = \\frac{x -\\mu}{\\sigma} = \\frac{169.0- 159.0}{4.9} = 2.041\\]\nWhat proportion of young Ghanaian women reach a height that is at or below 169 cm? To answer this question, we need to find the area under a normal density curve (i.e. the probability) that is to the left of \\(z = 2.041\\).\nTo find the area under a normal curve corresponding to a \\(z\\)-score of \\(2.041\\), do the following:\n\npnorm(2.041)\n\n[1] 0.9793746\n\n\n\n\n\n\n\n\n\n\nNOTE: The area to the left of our chosen \\(z\\)-score is also the probability that a randomly selected woman will be shorter than Serwa. The probability that a randomly selected Ghanaian woman will be shorter than Serwa is \\(0.979\\), or \\(97.9\\%\\).\n*Remember that in this course, unless otherwise specified, we round to three decimal places.\n\n\nBaseball Averages\nWe now return to the example of the baseball batting averages. We want to find the probability that a randomly selected player will have a batting average that is above 0.280. The population mean is 0.261 and the population standard deviation is 0.034. We can use this information to find a \\(z\\)-score. Then we use the applet to find the area under the normal curve to the right of this \\(z\\)-score.\n\\[z = \\frac{x -\\mu}{\\sigma} = \\frac{0.280 - 0.261}{0.034} = 0.559\\]\n\nx &lt;- 0.280\nmu &lt;- 0.261\nsigma &lt;- 0.034\n\nz &lt;- (x-mu)/sigma\n\n# Area to the LEFT:\npnorm(x, mu, sigma)\n\n[1] 0.7118589\n\n## Equivilantly:\npnorm(z)\n\n[1] 0.7118589\n\n# Area to the RIGHT\n1-pnorm(x, mu, sigma)\n\n[1] 0.2881411\n\n1-pnorm(z)\n\n[1] 0.2881411\n\n\nUsing the Applet, type the \\(z\\)-score of \\(0.559\\) in one of the boxes below the horizontal axis in the applet. Click on the areas under the curve until only the region on the right is highlighted in blue.\n\nThe area under the curve to the right of \\(z = 0.559\\) is \\(0.288\\). This is the probability that a randomly selected player will have a batting average that is greater than 0.280. (Note: It is a coincidence that the area, 0.288, is close to the batting average of 0.280. There is no significance in this.)\nNotice that the area shaded in blue above 0.288 is very close to the area we found when we looked at the area represented by the bars of the histogram 0.298 that was shaded in red above.\n\n\n\nFinding the Probability of Being Between Two Values\nThe normal probability applet allows us to find the probability of being between two values as long as they are on opposite sides of the mean and equally distanced from the mean. By calculating two z-scores, one for each value, and then shading the area between them on the applet, we can find the probability. This is severely limiting.\nBelow, we demonstrate how to find the area between any two values in a normal distribution.\n\nHeights of Ghanaian Women\nWhat is the probability that a randomly selected young Ghanaian women will be between 150.0 cm and 163.0 cm tall? Recall that the average height for young Ghanaian women is \\(\\mu=159.0\\) cm and the population standard deviation is \\(\\sigma=4.9\\) cm.\nWe want to find the probability that a randomly selected woman’s height is between \\(150.0\\) cm and \\(165.0\\) cm. To do this we find the \\(z\\)-score for both values:\n\\[z_1 = \\frac{x- \\mu}{\\sigma} = \\frac{150.0 - 159.0}{4.9} = -1.837\\] \\[z_2 = \\frac{x - \\mu}{\\sigma} = \\frac{165.0 -159.0}{4.9} = 1.22\\]\nWe now answer the question by finding the area under the normal density curve (i.e. the probability) to the left of \\(z = 1.22\\) which is \\(0.8888\\) and also the area under the normal density curve to the left of \\(z = -1.837\\) which is \\(0.033\\). To find the area between \\(z = 1.22\\) and \\(z = -1.837\\), we subtract the smaller area from the larger.\n\\[0.8888 - 0.0331 = 0.8557\\]\nSo the probability that the height of a randomly selected young Ghanaian woman will be between 150.0 cm and 165.0 cm is \\(0.8558\\). This is the same as the proportion of all young Ghanaian women who are between 150.0 and 165.0 cm tall.\nTo find the probability of being between any 2 numbers for a normal distribution with mean, \\(\\mu\\), and standard deviation, \\(\\sigma\\), we can use the following R code:\n\nvalue1 &lt;- 150\nvalue2 &lt;- 165\nmu &lt;- 159\nsigma &lt;- 4.9\n\npnorm(value2, mean = mu, sd = sigma) - pnorm(value1, mean = mu, sd = sigma)\n\n[1] 0.8564917\n\n\nNOTE: Value 1 is the lower of the two values and Value 2 is the higher value. Also, this answer is slightly different than above due to when you round. This answer is more accurate because we rounded at the end of the probability calculations rather than rounding the z-scores, then calculating probabilities.\n\n\n\nCalculating Percentiles using a Normal Distribution\nA percentile is a number such that a specified percentage of the population are at or below this number. For example, the 25th percentile is the number in a data set that is greater than or equal to 25% of all the values in the data set.\nWe can find percentiles for a given dataset by using the quantile() function as described in the chapter on summarizing data\nHowever, to calculate percentiles from a normal distribution, we use the qnorm(percentile, mu, sigma) function.\nNOTE: R typically uses the word quantile when referring to percentiles. So the \\(q\\) in qnorm() stands for quantile.\nTo find the height of a Ghanaian woman corresponding to the 90th percentile:\n\nqnorm(.90, mean = 159, sd = 4.9)\n\n[1] 165.2796\n\n\nThe 90th percentile of the heights averages is 165.2796. That means that 90% of the Ghanaian women are shorter than 165.2796 cm tall."
  },
  {
    "objectID": "4-Foundations_Statistical_Inference/02-The_Normal_Distribution_Textbook.html#roughly-assessing-normality-using-a-histogram",
    "href": "4-Foundations_Statistical_Inference/02-The_Normal_Distribution_Textbook.html#roughly-assessing-normality-using-a-histogram",
    "title": "The Normal Distribution (Reading)",
    "section": "Roughly Assessing Normality Using a Histogram",
    "text": "Roughly Assessing Normality Using a Histogram\n\nBaseball Batting Averages\nConsider the data on the batting averages of Major League Baseball players. The histogram of the batting averages showed a distinct bell-shaped curve.\n\nLater in the course, we will learn a better way to assess normality. For now, we use histograms as a rough way to look for symmetry.\n\nR Instructions\n\n\nHere is a refresher of how to make a Histogram in R\nFor more detailed instructions revisit Summarizing Data\n\nRead in the data\n\n\nlibrary(rio)\nlibrary(mosaic)\nbat_avg &lt;- import(\"https://github.com/byuistats/Math221D_Course/raw/main/Data/BattingAverages.xlsx\")\n\n\nUse histogram(data$column_name) where column_name refers to the column you would like to use to create a histogram.\n\n\nhistogram(bat_avg$BattingAvg)\n\n\n\n\n\n\n\n\n\nCustomize your graph to make it presentation worthy by adding labels and modifying colors:\n\n\nhistogram(bat_avg$BattingAvg, main = \"Distribution of Batting Averages\", xlab = \"Batting Average\", ylab = \"\", col = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\nBody Temperatures\nA group of researchers led by Philip A. Mackowiak, MD, conducted a study to assess the true mean body temperatures of healthy adults. They selected n = 148 subjects between the ages of 18 and 40 years old, representative of the general population.\nEach volunteer was given a physical to assure that they were not ill at the time of the data collection. Their axillary (under the arm) body temperature was measured and reported in a paper published in the Journal of the American Medical Association. [1] These data were extracted and are presented in the file BodyTemp. The body temperatures are given in degrees Fahrenheit.\n\nAnswer the following questions:\n\n\n\nMake a histogram of the body temperature data.\n\n\n\nShow/Hide Solution\n\n\nlibrary(rio)\nlibrary(mosaic)\nbody_temp &lt;- import(\"https://github.com/byuistats/Math221D_Course/raw/main/Data/BodyTemp.xlsx\")\nhistogram(body_temp$BodyTemp)\n\n\n\n\n\n\n\n\n\n\nBased on your histogram, what is the shape of the distribution of these data?\n\n\n\nShow/Hide Solution\n\n\nThe data appear to be normally distributed.\n\n\n\nCalculate the following numerical summaries of the data: sample mean, sample standard deviation, and sample size.\n\n\n\nShow/Hide Solution\n\n\nfavstats(body_temp$BodyTemp)\n\n  min   Q1 median   Q3   max     mean        sd   n missing\n 96.2 97.8   98.3 98.7 100.8 98.23446 0.7375924 148       0\n\n\nThe sample mean is 98.23, sample standard deviation is 0.738, and sample size is n = 148."
  },
  {
    "objectID": "4-Foundations_Statistical_Inference/02-The_Normal_Distribution_Textbook.html#summary",
    "href": "4-Foundations_Statistical_Inference/02-The_Normal_Distribution_Textbook.html#summary",
    "title": "The Normal Distribution (Reading)",
    "section": "Summary",
    "text": "Summary\n\nRemember…\n\n\n\nA normal density curve is symmetric and bell-shaped with a mean of \\(\\mu\\) and a standard deviation of \\(\\sigma\\). The curve lies above the horizontal axis and the total area under the curve is equal to 1. A standard normal distribution has a mean of 0 and a standard deviation of 1.\nA z-score is calculated as:\n\n\\[\\displaystyle{z = \\frac{\\text{value}-\\text{mean}}{\\text{standard deviation}} = \\frac{x-\\mu}{\\sigma}}\\]\n\nA z-score tells us how many standard deviations above (\\(+Z\\)) or below (\\(-Z\\)) the mean (\\(\\mu\\)) a given value (\\(x\\)) is.\nTo calculate probabilities for a normal distribution with mean, \\(\\mu\\) and standard deviation \\(\\sigma\\) for a given observation \\(x\\), use pnorm(x, mu, sigma) or 1-pnorm(x, mu, sigma) to get the desired probability (below, above). Alternatively, calculate the \\(z\\)-score and use pnorm(z) or 1-pnorm(z). In every case, the probability is given by the Area under the curve.\nThe 68-95-99.7% rule states that when data are normally distributed, approximately 68% of the population lies within \\(z=1\\) standard deviation (\\(\\sigma\\)) from the mean, approximately 95% of the data lie within \\(z=2\\) standard deviations from the mean, and approximately 99.7% of the data lie within \\(z=3\\) standard deviations from the mean. For example, this rule approximates that 2.5% of observations will be less than a z-score of \\(z=-2\\).\nPercentiles can be calculated using the qnorm(percentile, mu, sigma) function."
  },
  {
    "objectID": "4-Foundations_Statistical_Inference/04-Normal_Probability_Practice.html",
    "href": "4-Foundations_Statistical_Inference/04-Normal_Probability_Practice.html",
    "title": "Normal Probability Practice",
    "section": "",
    "text": "Recall that the normal distribution is a probability distribution defined by its center (\\(\\mu\\)) and its spread (\\(\\sigma\\))."
  },
  {
    "objectID": "4-Foundations_Statistical_Inference/04-Normal_Probability_Practice.html#brisket-competition",
    "href": "4-Foundations_Statistical_Inference/04-Normal_Probability_Practice.html#brisket-competition",
    "title": "Normal Probability Practice",
    "section": "Brisket Competition",
    "text": "Brisket Competition\nCompetition was tight at a Saint Louis BBQ competition. Brisket scores were normally distributed with an average score of 5.941 with a standard deviation of 0.04.\nQuestion: What’s the probability of getting a score GREATER than 6?\nAnswer:\nQuestion: What’s the probability of getting a score LESS than 4?\nAnswer:\nQuestion: What is the 99th percentile of this Brisket Competition?\nAnswer:"
  },
  {
    "objectID": "4-Foundations_Statistical_Inference/04-Normal_Probability_Practice.html#prbability-of-a-false-start",
    "href": "4-Foundations_Statistical_Inference/04-Normal_Probability_Practice.html#prbability-of-a-false-start",
    "title": "Normal Probability Practice",
    "section": "Prbability of a “False Start”",
    "text": "Prbability of a “False Start”\n“At high level meets, the time between the gun and first kick against the starting block is measured electronically, via sensors built in the gun and the blocks. A reaction time less than 0.1 s is considered a false start. The 0.2-second interval accounts for the sum of the time it takes for the sound of the starter’s pistol to reach the runners’ ears, and the time they take to react to it.” (https://en.wikipedia.org/wiki/100_metres) Let’s suppose that reaction times are normally distributed with a mean of 0.2 seconds and a standard deviation of 0.03.\nWhat’s the probability of a false start? (meaning a reaction time LESS than 0.1)"
  },
  {
    "objectID": "4-Foundations_Statistical_Inference/06-Central_Limit_Theorem_Practice.html",
    "href": "4-Foundations_Statistical_Inference/06-Central_Limit_Theorem_Practice.html",
    "title": "CLT Practice",
    "section": "",
    "text": "Instructions\nAnswer the following questions, render the document and submit the .html report.\n\n\nQuestions\nSuppose you take a sample of size \\(n=5\\) from a right skewed distribution with a population mean, \\(\\mu=125\\) and a standard deviation, \\(\\sigma=12\\)\nQuestion: What is the mean of the distribution of sample means (\\(\\mu_{\\bar{x}}\\))?\nAnswer:\nQuestion: What is the standard deviation of sample means (\\(\\sigma_{\\bar{x}}\\))?\nAnswer:\nQuestion: What is the shape of the distribution of sample means and why?\nAnswer:\nNow suppose you increase the sample size to \\(n=100\\). What is:\nMean of sample means:\nStandard deviation of sample means:\nShape of the distribution of sample means:\nDescribe the difference between the Law of Large Numbers and the Central Limit Theorem.\nAnswer:"
  },
  {
    "objectID": "4-Foundations_Statistical_Inference/08-CLT_Normal_Prob_for_Means.html",
    "href": "4-Foundations_Statistical_Inference/08-CLT_Normal_Prob_for_Means.html",
    "title": "Probability Calculations for Means (Class)",
    "section": "",
    "text": "The Central Limit Theorem states that for a large enough sample size (\\(n&gt;30\\)) the distribution of sample means is approximately normal with mean, \\(\\mu_{\\bar{x}} = \\mu\\) and \\(\\sigma_{\\bar{x}} = \\frac{\\sigma}{\\sqrt{n}}\\) regardless of the distribution of the population.\nWe can assume the distribution of sample means is approximately normal if:\n\nThe population is normally distributed\nn &gt; 30\n\nDon’t forget, that if the population is normally distributed, so is the distribution of sample means regardless of sample size."
  },
  {
    "objectID": "4-Foundations_Statistical_Inference/08-CLT_Normal_Prob_for_Means.html#gpas",
    "href": "4-Foundations_Statistical_Inference/08-CLT_Normal_Prob_for_Means.html#gpas",
    "title": "Probability Calculations for Means (Class)",
    "section": "GPA’s",
    "text": "GPA’s\nSuppose the mean GPA of BYU-Idaho students is 3.5 and the standard deviation is 0.7. It is well known that this distribution is left-skewed. A random sample of n = 45 students will be drawn.\nQuestion: What is the mean of the distribution of the sample means (sampling distribution) for all possible samples of size 45 that could be drawn from the parent population of GPAs?\nAnswer:\nQuestion: What is the standard deviation of the distribution of the sample means (sampling distribution) for all possible samples of size 45 that could be drawn from the parent population?\nAnswer:\nQuestion: What is the probability that the mean GPA for 45 randomly selected BYU-Idaho students will be less than 3.3?\nAnswer:\nQuestion: What is the shape of the distribution of sample means, \\(\\bar{x}\\), when 45 students are selected?\nAnswer:"
  },
  {
    "objectID": "4-Foundations_Statistical_Inference/08-CLT_Normal_Prob_for_Means.html#gre-scores",
    "href": "4-Foundations_Statistical_Inference/08-CLT_Normal_Prob_for_Means.html#gre-scores",
    "title": "Probability Calculations for Means (Class)",
    "section": "GRE Scores",
    "text": "GRE Scores\n\nxbar &lt;- 160\n\n# Sample size\nn &lt;- 18\n  \n#population mean, mu\nmu &lt;- 150.8\n  \n# Pop. SD\nsigma &lt;- 8.8\n  \n# Standard Deviation of XBAR:\nsigma_xbar &lt;- sigma / sqrt(n)\nsigma_xbar\n\n[1] 2.07418\n\nz &lt;- (xbar - mu) / sigma_xbar\nz\n\n[1] 4.435488\n\n# LEFT tail probability (probability of getting a sample mean LESS than xbar)\npnorm(z)\n\n[1] 0.9999954\n\n# RIGHT tail; prob value GREATER than xbar\n1-pnorm(z)\n\n[1] 4.593198e-06\n\n\nScores on the quantitative portion of the GRE are approximately normally distributed with mean, \\(\\mu=150.8\\), and standard deviation \\(\\sigma = 8.8\\).\nQuestion: Dianne earned a score of 160 on the quantitative portion of the GRE. What is the z-score corresponding to Dianne’s score?\nAnswer:\nQuestion: What is the probability that a randomly selected student will score above 160 on the quantitative portion of the GRE?\nAnswer:\nQuestion: What is the probability that a randomly selected group of 18 students will have an average less than 160 on the quantitative portion of the GRE?\nAnswer:\nQuestion: What is the probability that a randomly selected group of 18 students will have an average between 145 and 160 on the quantitative portion of the GRE?\nAnswer:"
  },
  {
    "objectID": "4-Foundations_Statistical_Inference/10-Assessing_Normality.html",
    "href": "4-Foundations_Statistical_Inference/10-Assessing_Normality.html",
    "title": "Assessing Normality",
    "section": "",
    "text": "Assessing Normality\nIn practice, we must confirm that the distribution of sample means is normally distributed. This is true when:\n\nThe population is normally distributed\n\\(n &gt; 30\\) because of the Central Limit Theorem\n\nBut how do you know if a population is normally distributed? In the real world, there is no teacher to tell you when to assume a population is normal.\nIf our sample size is large enough, we don’t have to worry. We can trust the Central Limit Theorem.\nIf our sample size is &lt; 30, we can assess the normality of our sample to decide if we can still trust output of our hypothesis tests and confidence intervals.\nPreviously, we’ve used histograms to help visualize the distribution of a sample. However, when sample sizes are small, even samples from a standard normal distribution can look skewed.\nAll of the examples below are histograms of random samples from actual standard normal distributions:\n\n\nA New Way to Assess Normality\nStatisticians use something called a QQPlot which works better at assessing normality. QQPlots plot the sorted data of each point in a dataset with the theoretical percentile from a normal distribution. If the data and theoretical percentiles line up, then we can be reasonably sure the population is normally distributed.\nThese are easier to use than to explain. We use the car library and the function qqPlot() to create a chart. (Note the Capital P in the middle.)\nKey Point: If most of the data points line up in the shaded region, we can consider the population as normally distributed.\nBelow are examples of QQPlots for a normal distribution and a right skewed distribution.\n\n\nThese work much better for small sample sizes. Below are several examples of QQPlots for small sample sizes:\n\nWhile not perfect, these are a vastly better tool to assess normality than a histogram.\n\n\n\nPractice\nLet’s try assessing the normality of some data. Below are 3 datasets. Find the response variable(s) from each and determine if the data are sufficiently normally distributed:\n\nlibrary(rio)\nlibrary(tidyverse)\nlibrary(car)\n\nold_faithful &lt;- import('https://github.com/byuistats/Math221D_Cannon/raw/master/Data/OldFaithful.xlsx')\n\nqqPlot(old_faithful$Duration)\n\n\n\n\n\n\n\n\n[1] 19 58\n\nqqPlot(old_faithful$Wait)\n\n\n\n\n\n\n\n\n[1] 265 127\n\nrent &lt;- import('https://github.com/byuistats/Math221D_Cannon/raw/master/Data/Rent.csv')\n\n\nmcat_gpa &lt;- import('https://github.com/byuistats/Math221D_Cannon/raw/master/Data/mcat_gpa.csv')"
  },
  {
    "objectID": "4-Foundations_Statistical_Inference/From_Sample_to_Population.html",
    "href": "4-Foundations_Statistical_Inference/From_Sample_to_Population.html",
    "title": "From Sample to Population",
    "section": "",
    "text": "General hypothesis: There is no relationship between X and Y. Different data types lead to different hypotheses, but the common way is no relationship.\nWhat would no relationship look like for 2 groups? ANOVA?\nWhat do you do with no explanatory variable?\n1-sample an exception; is parameter different from a specified value\nTest statistic: Probability distributions generally (normal, t, F, chi-square - Null hypothesis driven\nWhat is a test statistic?\nWhat are their null probability distributions?\nIf Ho is true, and there is no relationship, we may see test statistics in the tail by chance, but the further it is into the tail, the less likely it is that the null hypothesis is true.\nIf there was no relationship between x and y, most of the time you’d get a value close to the null value (must have already talked about sampling distributions)\nFrom Chat:\nA test statistic is a numerical value that is calculated from sample data during a statistical hypothesis test. It helps us make decisions about the validity of a hypothesis based on the data we have collected.\nIn simpler terms, when we want to test a claim or hypothesis about a population, we use sample data to determine whether to reject that claim or not. The test statistic measures how far our sample result is from what we would expect if the null hypothesis (the hypothesis we are testing) were true.\nDepending on the nature of the data and the hypothesis, different types of test statistics may be used, such as z-scores, t-scores, or chi-square values.\nOnce the test statistic is calculated, it is compared against a critical value from a statistical distribution (like the normal or t-distribution) to decide whether the observed result is statistically significant, meaning it’s not likely to have occurred by random chance.\nend of chat\nShow Null Hypothesis Distributions for Test Statistics; these are quantities calculated typically be seeing how different out sample stats are away from the hypothesized value relative to the average deviation\nWhat’s the average deviation? Standard Deviation\nF: How far the means vary relative to how far the individuals vary from their means\nSignal/Noise\nStart With 1-sample t"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/001-Introduction_to_Inference.html",
    "href": "5-Statistical_Tests_Part1/001-Introduction_to_Inference.html",
    "title": "Introducing Inference",
    "section": "",
    "text": "By the end of this lesson, you should be able to:\n\nRecognize when a one mean (sigma known) hypothesis test is appropriate\nDefine a P-value (one-sided and two-sided)\nPerform a hypothesis test for a single mean with σ known using the following steps:\n\nState the null and alternative hypotheses\nCalculate the test-statistic by hand\nDetermine the P-value using the normal distribution\nAssess statistical significance in order to state the appropriate conclusion for the hypothesis test\nCheck the requirements for the hypothesis test\n\nInterpret Type I and II errors in the context of a hypothesis test\nExplain the meaning of the level of significance (\\(\\alpha\\))"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/001-Introduction_to_Inference.html#lesson-outcomes",
    "href": "5-Statistical_Tests_Part1/001-Introduction_to_Inference.html#lesson-outcomes",
    "title": "Introducing Inference",
    "section": "",
    "text": "By the end of this lesson, you should be able to:\n\nRecognize when a one mean (sigma known) hypothesis test is appropriate\nDefine a P-value (one-sided and two-sided)\nPerform a hypothesis test for a single mean with σ known using the following steps:\n\nState the null and alternative hypotheses\nCalculate the test-statistic by hand\nDetermine the P-value using the normal distribution\nAssess statistical significance in order to state the appropriate conclusion for the hypothesis test\nCheck the requirements for the hypothesis test\n\nInterpret Type I and II errors in the context of a hypothesis test\nExplain the meaning of the level of significance (\\(\\alpha\\))"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/001-Introduction_to_Inference.html#introduction",
    "href": "5-Statistical_Tests_Part1/001-Introduction_to_Inference.html#introduction",
    "title": "Introducing Inference",
    "section": "Introduction",
    "text": "Introduction\nHave you ever wondered how it was determined that the true mean body temperature of healthy adults is 98.6° ?\nIt is not exactly clear who first reported this value, but this temperature has been used since the 1800’s.  One of the most influential researchers in this area is Carl Reinhold August Wunderlich. He reported measuring over 1,000,000 body temperatures on over 20,000 patients. When the temperature is measured in the arm pit, it is called an axillary temperature measurement. Based on his research, Wunderlich stated, “The axillary temperature of 98.6° F = 37° C\\(\\ldots\\)is considered the central thermic point of health”.  In other words, the mean body temperature of healthy adults is 98.6° F (or 37° C.)\n\nAnswer the following question:\n\n\n\nHow would you design a study to determine if the mean body temperature of healthy adults is 98.6° ?\n\n\n\nShow/Hide Solution\n\n\nTake the temperature of a large number of healthy individuals in a clinical setting and compare your mean to the assumed mean body temperature of 98.6 degrees F.\n\n\n\n\n\n\nData on Body Temperatures\nA group of researchers led by Philip A. Mackowiak, MD, conducted a study to assess the true mean body temperatures of healthy adults. They selected \\(n=148\\) subjects between the ages of 18 and 40 years old, representative of the general population. Each volunteer was given a physical to assure that they were not ill at the time of the data collection. Their axillary body temperature was measured and reported in a paper published in the “Journal of the American Medical Association”.  These data were extracted and are presented in the file BodyTemp. The body temperatures in the file are given in degrees Fahrenheit. Based on historical data, the standard deviation of body temperatures is assumed to be \\(\\sigma = 0.675\\)° F.\n\nAnswer the following questions:\n\n\n\nHow could you use the body temperature data collected by Dr. Mackowiak to determine if the mean body temperature is really 98.6° F?\n\n\n\nShow/Hide Solution\n\n\nWe can compare his sample results to the assumed population mean of 98.6° F.\n\n\n\n\nFind the mean of the \\(n=148\\) body temperatures.\n\n\n\nShow/Hide Solution\n\n\n\\(\\bar{x} = 98.23~^\\circ\\text{F}\\).\n\n\n\n\nCreate a histogram illustrating the body temperatures of the individuals in the Mackowiak study.\n\n\n\nShow/Hide Solution\n\n\nlibrary(rio)\n\ntemps &lt;- import('https://github.com/byuistats/Math221D_Course/raw/refs/heads/main/Data/BodyTemp.xlsx')\n\nhist(temps$BodyTemp, main = \"Body Temperatures (F)\", xlab=\"Body Temperature\", ylab=\"Count\")\n\nabline(v=98.6, col=\"red\", lty=2, lwd=2)\n\n\n\n\n\n\n\n\n\n\n\nBased on the mean of the observations (Question 3) and the histogram of the data (Question 4,) does it appear that the mean body temperature of healthy adults is significantly different from 98.6° F?\n\n\n\nShow/Hide Solution\n\n\nAnswers may vary. However, many students are likely to say that the sample mean and population mean are very close to each other.\n\n\n\n\nWhat is the shape of the distribution of all possible sample means? How do we know?\n\n\n\nShow/Hide Solution\n\n\nThe distribution of all possible sample means will be approximately normally distributed. Since the sample size is large, the Central Limit Theorem guarantees this result.\n\n\n\n\nAssuming that the true mean body temperature of healthy adults is \\(\\mu=98.6\\)° F, and the population standard deviation is \\(\\sigma = 0.675\\)° F, find the mean and standard deviation of the random variable \\(\\bar{x}\\).\n\n\n\nShow/Hide Solution\n\n\nThe random variable \\(\\bar{x}\\) will have a mean of \\(\\mu = 98.6\\) and a standard deviation of \\(\\displaystyle{ \\frac{\\sigma}{\\sqrt{n}} = \\frac{0.675}{\\sqrt{148}} = 0.05548 }\\).\n\n\n\n\nUse the information in Question 7 to find the \\(z\\)-score for the mean you calculated in Question 3.\n\n\n\nShow/Hide Solution\n\n\n\\(z = \\frac{\\bar{x} - \\mu}{\\sigma / \\sqrt{n} } = \\frac{98.23 - 98.6}{0.675 / \\sqrt{148} } = -6.6685\\)\n\n\n\n\nWhat is the probability of observing a \\(z\\)-score that is as extreme or more extreme (further away from 0) than the \\(z\\)-score you calculated in Question 8?\n\n\n\nShow/Hide Solution\n\n\nThis is a two-tailed test, so we need to shade both tails of the applet. This gives a \\(P\\)-value of \\(2.5931\\times 10^{-11}\\).\n\n\n\n\nAssuming the mean body temperature really is 98.6° F, how likely would it be for a random sample of \\(n=148\\) people in the population to have a mean body temperature that is as extreme as was observed in Question 3?\n\n\n\nShow/Hide Solution\n\n\nThis is extremely unlikely.\n\n\n\n\nResults as unlikely as this demand an explanation. What do you think is the reason for a \\(z\\)-score as extreme as this?\n\n\nA. The group of volunteers included in the study had unusually low body temperatures.\nB. The researchers did not measure the temperature correctly.\nC. The true mean body temperature is different than 98.6° F.\nD. The sample size of \\(n=148\\) is not large enough.\nE. The data were collected on a cold day.\nF. The data are not normally distributed.\n\n\n\nShow/Hide Solution\n\n\nCorrect answer: C. The true mean is different than 98.6° F. The data were collected carefully with properly calibrated modern thermometers. The sample was representative of the population and the size of the sample was not an issue."
  },
  {
    "objectID": "5-Statistical_Tests_Part1/001-Introduction_to_Inference.html#hypothesis-test-for-the-true-mean-body-temperature",
    "href": "5-Statistical_Tests_Part1/001-Introduction_to_Inference.html#hypothesis-test-for-the-true-mean-body-temperature",
    "title": "Introducing Inference",
    "section": "Hypothesis Test for the True Mean Body Temperature",
    "text": "Hypothesis Test for the True Mean Body Temperature\n\nNull and Alternative Hypotheses\nIt is commonly believed that the true mean body temperature is 98.6° F (37° C). In science, a statement or claim such as this is called a hypothesis. We can use data to test a hypothesis. If there is enough evidence against a hypothesis, we reject it in favor of something else.\nThe claim representing the “status quo” or the commonly held belief or the usual value is called the null hypothesis. In the case of body temperatures, our null hypothesis is defined by the belief that the true mean body temperature of healthy adults is 98.6° F.\nWe present the null hypothesis in the following way:\n\\[H_0: \\mu = 98.6\\]\nIf we gather enough evidence against the null hypothesis and determine that it should be rejected, we need another hypothesis to propose in its place. This is called the alternative hypothesis. If 98.6° F is not the correct body temperature, then it is logical to propose the following alternative hypothesis:\n\\[H_a: \\mu \\ne 98.6\\]\nTo be brief in writing, we label the null hypothesis \\(H_0\\). The zero in the subscript represents “null,” “baseline,” “default,” “no effect,” etc. Similarly, we label the alternative hypothesis \\(H_a\\).\nOur alternative hypothesis could have been written as:\n\n\\(H_a: \\mu \\ne 98.6\\) (two-sided hypothesis; two-tailed)\n\\(H_a: \\mu &lt; 98.6\\) (one-sided hypothesis; left-tailed)\n\\(H_a: \\mu &gt; 98.6\\) (one-sided hypothesis; right-tailed)\n\nNotice that each of these is a viable alternative to the requirement that the mean is 98.6 degrees. We would only use \\(&lt;\\) or \\(&gt;\\) if we had a belief in advance that the mean was less than or greater than 98.6. If we do not have a strong reason to believe the mean is either smaller or larger than the stated value–before we collect our data–then, we use \\(\\ne\\) in our alternative hypothesis.\nIt is important that the null and alternative hypotheses be determined prior to collecting the data. It is not appropriate to use the data from your study to choose the alternative hypothesis that will be used to test the same data! This is an example of using data twice, once to choose the test and again to conduct the test. It is okay to use data from a previous study to determine your null and alternative hypotheses, but it is an improper use of the statistical procedures to use the data to define and conduct a hypothesis test.\nNotice that the null and alternative hypotheses are statements about a population parameter (e.g. \\(\\mu\\).) They will never involve a sample statistic (e.g. \\(\\bar{x}\\).) Population parameters are unknown, and we are trying to make a judgement about whether or not they are equal to a particular value. Sample statistics are calculated from our data, so there is no reason to do any test to assess what their value is.\n\nThe null hypothesis will always be a statement involving equality. This gives us a starting point in our analysis. In the reading for Normal Distributions, we assumed that the true mean body temperature was 98.6° F. This becomes the assumed value of the mean of the distribution of the random variable \\(\\bar{x}\\).\nThe scientific method demands that we make a hypothesis (a claim or an educated guess). We assume that the null hypothesis is true and gather evidence against it. In other words, we “do research” (e.g., collect data) to gather evidence against that hypothesis. If we can gather enough evidence to discredit our initial hypothesis, we conclude that it was false, and begin the process again. If we are unable to reject the original hypothesis, we do not conclude that it is correct. For example, we do not know that Einstein’s Theory of Relativity is correct. We have simply not been able to disprove it\\(\\ldots\\)yet.\nIn the same way, we never can prove a null hypothesis is true. We can only gather evidence against it. If we get enough evidence, we reject the null hypothesis.\n\n\nTest Statistic\nAssuming the null hypothesis is true, we assume that the true mean body temperature of healthy adults actually is \\(\\mu = 98.6^{\\circ}\\) F. Wanting to test this claim, researchers collected data on the mean body temperatures of \\(n=148\\) healthy adults. The mean of the observed values was \\(\\bar{x} = 98.23^{\\circ}\\) F.\nA histogram of the body temperature data shows a nice bell-shaped distribution. Also, the sample mean, \\(\\bar{x}=98.23^\\circ\\) F, appears to be reasonably close to the assumed population mean, \\(\\mu=98.6^\\circ\\) F. However, it is important to remember that the standard deviation of \\(\\bar{x}\\) is \\(\\frac{\\sigma}{\\sqrt{n}} = \\frac{0.675}{\\sqrt{148}}=0.055\\). We need to determine how far \\(\\bar{x}\\) is from \\(\\mu\\). Since we know \\(\\sigma\\) in this case, we can use the \\(z\\)-score to determine if \\(\\bar{x}\\) is far from the assumed value. This is called the test statistic: \\[\nz = \\frac{\\bar{x} - \\mu}{\\sigma / \\sqrt{n}} = \\frac{98.23 - 98.6}{0.675 / \\sqrt{148}} = -6.6685\n\\] Where \\(z\\) is the test statistic, \\(\\bar{x}\\) is the sample mean, \\(\\mu\\) is the population mean, \\(\\sigma\\) is the standard deviation and \\(n\\) is the sample size. Based on the results from the formula above, \\(\\bar{x}\\) is over 6 standard deviations below the mean!\n\n\n\\(P\\)-value\n\nDefinition\nThe \\(P\\)-value is the probability of obtaining a test statistic (such as \\(z\\)) at least as extreme as the one you calculated, assuming the null hypothesis is true. In other words, our \\(P\\)-value is the probability that we would get a \\(z\\)-score that is as extreme or more extreme than \\(z=-6.6685\\), assuming the true mean is 98.6° F.\nIn advance of collecting the data, we may have had no idea whether the true mean would be greater than 98.6 or less than 98.6, if 98.6° F was not the right value. This is why we used a two-sided alternative hypothesis (\\(H_a: \\mu \\ne 98.6\\).) When computing the \\(P\\)-value, we need to continue this logic. So, we would have considered a value of \\(z=-6.6685\\) to be equally as extreme as \\(z=6.6685\\).\nIn the case of a two-sided test for one mean with \\(\\sigma\\) known, the \\(P\\)-value is the area in the tails beyond \\(\\pm z\\), in both the left and right tails of the standard normal distribution.\n\nUsing the applet, we shade the area in both tails and then type in the value of \\(z=-6.669\\). The applet reports an area of “2.584E-11.” This is the way computers indicate scientific notation. Expressed using more traditional notation, we get\n\\[ P\\text{-value} = 2.584 \\times 10^{-11} = 0.000~000~000~026 \\]\nYou should always convert the computer’s notation involving “E” to scientific notation or decimal notation.\n\n  Scientific Notation:\n\nScientific notation is a method used to write very large or very small numbers without a lot of extra zeros. Computers express scientific notation using the letter “E.” For example, to write \\(7.8\\times 10^3\\), a computer would output the expression 7.8E3. For example, the number \\(7.8\\times 10^3\\) is \\(7.8 \\times 1000=7800\\), since \\(10^3 = 10 \\cdot 10 \\cdot 10 = 1000\\). Another way to think of this is to move the decimal place in the number 7.8 three places to the right, which gives you \\[ 7~\\underset{\\rightarrow}{8} \\underset{\\rightarrow}{0} \\underset{\\rightarrow}{0}. \\]\nThis is an example of how very large numbers can be represented. To express small numbers (close to zero,) we use negative exponents. The number \\(9.12 \\times 10^{-5}\\) is the same as \\(9.12 \\times (10^5)^{-1} = 9.12 \\times (10000)^{-1}\\). This can be written as \\(9.12 \\times 0.00001 = 0.0000912\\). Since the exponent on the 10 is \\(-5\\), you move the decimal place in the number 9.1 five places to the left:\n\n\\[0.\\underset{\\leftarrow}{0}\\underset{\\leftarrow}{0}\\underset{\\leftarrow}{0}\\underset{\\leftarrow}{0}\\underset{\\leftarrow}{9}~1~2\\]\n\nSimilarly, the value \\(2.57552 \\times 10^{-11}\\) can be written in regular notation as \\(0.000~000~000~025~755~2\\).\n\n\n\nIf the \\(P\\)-value is small, that implies that the \\(z\\)-score was very large (far away from 0.) In other words, \\(\\bar{x}\\) was far away from \\(\\mu\\). In this case, our \\(z\\)-score was 6.669 standard deviations below the mean. It is very unusual to get a value of \\(\\bar{x}\\) that is this far from the value of \\(\\mu\\) given in the null hypothesis. This implies that it is not plausible that the mean is equal to 98.6° F. We reject the null hypothesis and conclude that there is sufficient evidence that the true mean body temperature of healthy adults is not 98.6° F.\nIf this conclusion is correct, you may wonder why so many people think the true mean body temperature of healthy adults is 98.6° F (37° C). Good question!\n\n\nRelationship to the Alternative Hypothesis\nThe \\(P\\)-value is determined using the alternative hypothesis. If the alternative hypothesis is two-sided (i.e. if the alternative hypothesis involves \\(\\ne\\),) then the \\(P\\)-value is the area in the tails further than \\(\\pm z\\). That is, it is the area to the left of \\(z=-6.669\\) plus the area to the right of \\(z=+6.669\\).\nIf the alternative hypothesis involves less than (such as \\(\\mu&lt;98.6\\),) we call it a left-tailed test. For a left-tailed test, the \\(P\\)-value is the area under the standard normal curve to the left of the test statistic, \\(z\\).\nIn the case of a right-tailed test, the \\(P\\)-value is the area to the right of the test statistic, \\(z\\).\nIn cases of very large or very small \\(z\\) statistics sometimes the shaded area is so small it can’t be seen. This is the case for \\(z=-6.669\\). It is such an extreme value for \\(z\\), it is hard to see the actual shading. However, shading is happening and the applet is calculating the right p-value. To help you visually see how the shading works, the three possibilities for alternative hypotheses are illustrated in the figures below, using a less extreme value of \\(z=-1.6\\).\n\n\n\n\n\n\nTwo-tailed\n\n\n\n\nLeft-tailed\n\n\n\n\nRight-tailed\n\n\n\n\n\n\n\n\n\\(H_0:~~\\mu = 98.6\\)\n\n\n\n\n\\(H_0:~~\\mu = 98.6\\)\n\n\n\n\n\\(H_0:~~\\mu = 98.6\\)\n\n\n\n\n\n\n\\(H_a:~~\\mu \\ne 98.6\\)\n\n\n\n\n\\(H_a:~~\\mu \\lt 98.6\\)\n\n\n\n\n\\(H_a:~~\\mu \\gt 98.6\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf \\(\\bar{x}\\) is close to \\(\\mu\\), then the \\(z\\)-score will be small. If the \\(z\\)-score is small, then there will be a lot of area in the tails beyond \\(z\\). In other words, the \\(P\\)-value will be large. These two possibilities are illustrated for a two-tailed test in the figure below. Similar results hold for a one-tailed test.\n\n\n\n\n\n\nLarge \\(P\\)-value\n\n\n\n\nSmall \\(P\\)-value\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFail to reject \\(H_0\\)\n\n\n\n\nReject \\(H_0\\)\n\n\n\n\n\n\n\nIn the case of the body temperatures, a large \\(z\\)-score (\\(z=-6.669\\)) led to a small \\(P\\)-value (the area in the tails.) We observed a value for \\(\\bar{x}\\) that was very far from \\(\\mu=98.6\\). Strictly due to chance, it would be very unlikely to observe a mean of \\(\\bar{x}=98.23^\\circ\\) F in a random sample of \\(n=148\\) if the true mean is 98.6° F.\n\n\n\nLevel of Significance, \\(\\alpha\\)\nHow do we decide if the \\(P\\)-value is small enough to reject the null hypothesis? We need a way to determine if there is enough evidence to reject the null hypothesis that does not depend on the data. We need a number that can be used to determine if the \\(P\\)-value is small enough to reject the null hypothesis. This number is called the level of significance and is often denoted by the symbol \\(\\alpha\\) (pronounced “alpha”.)\nMemory Aid: Some students find it helpful to remember the decision rule using the couplet:\nIf the \\(P\\) is low, reject the null.\n\nWe will use the same decision rule for all hypothesis tests. If the \\(P\\)-value is less than \\(\\alpha\\), we reject the null hypothesis. Conversely, if the \\(P\\)-value is greater than \\(\\alpha\\), we fail to reject the null hypothesis. The level of significance, \\(\\alpha\\), must be chosen prior to collecting the data. This is our standard for determining if the null hypothesis should be rejected. It is important that the personal bias of the researcher is not imposed on the data. So, we choose the \\(\\alpha\\) level prior to collecting the data. If we wanted to reject the null hypothesis, we could compute the \\(P\\)-value and then unscrupulously choose the \\(\\alpha\\) level. In every case, we could choose a value of \\(\\alpha\\) that is larger than the computed \\(P\\)-value, and therefore always reject the null hypothesis. This practice would defeat the purpose of hypothesis testing.\n\n\nType I and Type II Errors\nIf the null hypothesis is true, for example, if the true body temperature of healthy adults really is 98.6° F, sometimes we will get a very large or very small value of \\(\\bar{x}\\). This will lead to a very extreme \\(z\\)-score, strictly due to chance. In this case, we would reject the null hypothesis, even though it is true!\nWhenever we reject a true null hypothesis, we say that a Type I error was committed. Assuming the null hypothesis is true, the level of significance (\\(\\alpha\\)) is the probability of getting a value of the test statistic that is extreme enough that the null hypothesis will be rejected. In other words, the level of significance (\\(\\alpha\\)) is the probability of committing a Type I error.\nWe choose \\(\\alpha\\) to be some small number so that the probability of committing a Type I error is low. The most common value for \\(\\alpha\\) is \\(\\alpha=0.05\\). This is equal to \\(\\frac{1}{20}\\). So, when the null hypothesis is true, there is a one-in-twenty chance that it will be rejected at the 0.05 level of significance. Other common choices for \\(\\alpha\\) include \\(\\alpha = 0.1\\) and \\(\\alpha=0.01\\).\nIf committing a Type I error is undesirable, why can’t we let \\(\\alpha=0\\)? This would make it impossible to commit a Type I error. Actually, this would also make it impossible to reject “any” null hypothesis! If \\(\\alpha=0\\), no matter what test we do, no matter what the data show, we always fail to reject the null hypothesis. This would be pointless.\nA Type I error is committed when we reject a true null hypothesis. Another problem that can arise is to fail to reject a false null hypothesis. This is called a Type II error. The probability of a Type II error is represented by \\(\\beta\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe do not want to commit Type I errors, so why not choose a very small value for \\(\\alpha\\)? If we choose \\(\\alpha\\) to be so small that we rarely commit a Type I error, what would happen to the probability of committing a Type II error? (Think about it.)\nIf the \\(\\alpha\\) value (the probability of committing a Type I error) is very small, the probability of committing a Type II error will be large. Conversely, if \\(\\alpha\\) is allowed to be very large, then the probability of committing a Type II error will be very small. A level of significance of \\(\\alpha=0.05\\) seems to strike a good balance between the probabilities of committing a Type I versus a Type II error. However, there may be instances where it will be important to choose a different value for \\(\\alpha\\). The important thing is to choose \\(\\alpha\\) before you collect your data. Typical choices of \\(\\alpha\\) are \\(0.05\\) (most common), \\(0.1\\), and \\(0.01\\).\nRead each of the following scenarios and answer the questions.\nScenario 1: The BYU-Idaho Honor Code specifically prohibits wearing flip-flops on campus.\nA student is seen on campus wearing shoes that look somewhat like flip-flops.\nConsider the following null and alternative hypotheses: \\[\n\\begin{align}\nH_0: & \\textrm{Shoes of this type are technically flip-flops}\\\\\nH_a: & \\textrm{Shoes of this type are not technically flip-flops}\\\\\n\\end{align}\n\\] You are considering approaching this student to discuss their footwear.\n\nAnswer the following questions:\n\n\n\nWhat action would lead to a Type I error in this case?\n\n\n\nShow/Hide Solution\n\n\nA Type I Error would be committed if you decided that they were not flip flops, when in reality they were flip-flops.\n\n\n\n\nWhat action corresponds to committing a Type II error?\n\n\n\nShow/Hide Solution\n\n\nA Type II Error would be committed if you concluded that they were technically flip flops, when in reality they were not flip-flops.\n\n\n\n\nIn this case, is a Type I error or a Type II error more serious? Justify your response.\n\n\n\nShow/Hide Solution\n\n\nAnswers may vary.\n\n\n \n\n\nScenario 2: The U. S. Food and Drug Administration (FDA) regulates all prescription medications dispensed in the United States.\nPart of their responsibility is to oversee the investigation of the safety and efficacy of new drugs. A new drug is under consideration for approval.\nThe following null and alternative hypotheses relate to the effectiveness of the proposed drug: \\[\n\\begin{align}\nH_0: & \\textrm{The drug does not improve patients' conditions}\\\\\nH_a: & \\textrm{The drug improves patients' conditions}\\\\\n\\end{align}\n\\]\n\nAnswer the following questions:\n\n\n\nWhat action by the FDA would lead to a Type I error in this case?\n\n\n\nShow/Hide Solution\n\n\nA Type I error would be committed if the FDA decided that the drug improves patient’s conditions, when in actuality it does not.\n\n\n\n\nWhat action by the FDA would be associated with committing a Type II error?\n\n\n\nShow/Hide Solution\n\n\nThe FDA would commit a Type II error if they decided that the drug does not improve patients’ conditions, when in reality it does.\n\n\n\n\nIn this case, is a Type I error or a Type II error more serious? Justify your response.\n\n\n\nShow/Hide Solution\n\n\nAnswers may vary. However, a Type I error is probably more serious, since patients would be exposed to the negative side effects of the drug without any benefit.\n\n\n \n\n\nScenario 3: With regard to a new drug under consideration for approval by the FDA, the following null and alternative hypotheses address the safety of the drug: \\[\n\\begin{align}\nH_0: & \\textrm{The drug does not cause serious side effects}\\\\\nH_a: & \\textrm{The drug causes serious side effects}\\\\\n\\end{align}\n\\]\n\nAnswer the following questions:\n\n\n\nWhat action by the FDA would lead to a Type I error in this case?\n\n\n\nShow/Hide Solution\n\n\nA Type I error would be committed if the FDA decided that the drug causes serious side effects, when it does not.\n\n\n\n\nWhat action by the FDA corresponds to committing a Type II error?\n\n\n\nShow/Hide Solution\n\n\nA Type II error would be committed if the FDA decided that the drug does not cause serious side effects, when in reality it does cause serious side effects.\n\n\n\n\nIn this case, is a Type I error or a Type II error more serious? Justify your response.\n\n\n\nShow/Hide Solution\n\n\nA type II error would be more serious, because the FDA could release a drug that they say has no serious side effects, when it actually does.\n\n\n \n\n\n\n\nRequirements\nCertain requirements are required for us to conduct the hypothesis test for a single mean with \\(\\sigma\\) known.\nFirst of all, we must assume that the data represent a simple random sample from a large population. In practice, this requirement is rarely satisfied. Frequently researchers working with humans rely on convenience samples, where the subjects volunteer to participate in a research project. The researcher may advertise using television, radio, and flyers in a clinic. If there is no relationship between the issue being studied and people’s desire to participate, this method works fairly well.\nA second requirement is that the data are drawn from a population that has a normal distribution with mean \\(\\mu\\) given in the null hypothesis (e.g. 98.6° F) and a known standard deviation (e.g. \\(\\sigma=0.675^\\circ\\) F.)\nIt turns out that the procedure actually works quite well–even if the data are not normally distributed–as long as \\(\\bar{x}\\) follows a normal distribution. Under what conditions will \\(\\bar{x}\\) be normally distributed? (Think about it.)\nIn the reading for Distribution of Sample Means & The Central Limit Theorem|Lesson 6, we learned that \\(\\bar{x}\\) will follow a normal distribution if the data are drawn from a normal distribution or if the sample size (\\(n\\)) is large.\nPutting this all together, we can summarize the requirements for a test for a single mean with \\(\\sigma\\) known as:\n\nThe data represent a simple random sample from a large population.\nThe sample mean \\(\\bar{x}\\) is normally distributed. This happens if either one of the following is true:\n\nThe population is normally distributed.\nThe sample size is large.\n\n\n\n\n\nWorked Example: Body Temperatures\n\nSummarize the relevant background information\nThe following example illustrates how to conduct the hypothesis test for a single mean with \\(\\sigma\\) known. We will conduct a hypothesis test to determine if the true mean body temperature of healthy adults is 98.6° F, using the \\(\\alpha=0.05\\) level of significance. We will assume that the population standard deviation is known to be \\(\\sigma=0.675^\\circ\\) F. This will summarize the work presented in the paragraphs above.\nState the null and alternative hypotheses and the level of significance\n\\[\n\\begin{align}\nH_0:& \\mu = 98.6 \\\\\nH_a:& \\mu \\ne 98.6\n\\end{align}\n\\] \\[ \\alpha = 0.05 \\]\n\nDescribe the data collection procedures\nThe body temperatures of \\(n = 148\\) healthy adults were measured using calibrated thermometers in a clinical setting.\n\nGive the relevant summary statistics\n\\[\n\\begin{array}{l}\n\\bar{x} = 98.23\\\\\n\\sigma = 0.675\\\\\nn = 148\n\\end{array}\n\\]\nMake an appropriate graph (e.g. a histogram) to illustrate the data\n\n\nVerify the requirements have been met\n\nWe assume that the individuals chosen to participate in the study represent a (simple) random sample from the population.\n\\(\\bar{x}\\) will be normally distributed because the sample size is large. (Note: We could have also noticed that the body temperature data appears to be normally distributed, so even with a small sample size, \\(\\bar{x}\\) would be normal.)\n\nGive the test statistic and its value\n\\[z=-6.669\\]\nMark the test statistic and \\(P\\)-value on a graph of the sampling distribution (i.e. the standard normal curve)\n\nFind the \\(P\\)-value and compare it to the level of significance\n\\[P\\textrm{-value} = 2.57552 \\times 10^{-11} = 0.000~000~000~026 &lt; 0.05 = \\alpha\\]\nState your decision\nSince the \\(P\\)-value is less than \\(\\alpha\\), we reject the null hypothesis.\n\nPresent your conclusion in an English sentence, relating the result to the context of the problem\nThere is sufficient evidence to suggest that the true mean body temperature of healthy adults is different from 98.6° F."
  },
  {
    "objectID": "5-Statistical_Tests_Part1/001-Introduction_to_Inference.html#worked-example-perceived-health-after-a-cardiac-arrest",
    "href": "5-Statistical_Tests_Part1/001-Introduction_to_Inference.html#worked-example-perceived-health-after-a-cardiac-arrest",
    "title": "Introducing Inference",
    "section": "Worked Example: Perceived Health after a Cardiac Arrest",
    "text": "Worked Example: Perceived Health after a Cardiac Arrest\nA group of researchers led by Dr. Jared Bunch studied the long-term effects suffered by patients who experienced a cardiac arrest outside a hospital. The long-term health of the patients was assessed using the Short-Form General Health Survey (SF-36) at the time of their last follow up visit. The SF-36 is normalized so the mean score in the general population is \\(\\mu=50\\) and the standard deviation is \\(\\sigma=10\\). The minimum score is 0 and the maximum score is 100. Lower scores on the SF-36 indicate a poorer quality of health.\nUsing the 0.05 level of significance, we will conduct a hypothesis test to determine if the mean perceived general health level among cardiac arrest survivors is less than 50. In other words, we want to know if the mean perceived overall health is lower among cardiac arrest survivors than in the general population.\nDr. Bunch summarized the responses in a figure,  from which the data were extracted. These data are given in the file CardiacArrestHealth.\n\nAnswer the following question:\n\n\n\n\nSummarize the relevant background information.\n\n\n\nShow/Hide Solution\n\n\nThe long-term health of patients who had previously suffered a cardiac arrest were studied. The researchers want to know if these patients will feel that their overall long-term health was worsened by the cardiac arrest.\n\nAfter patients concluded their treatment for the cardiac arrest, they were given the SF-36 survey to assess their overall health. In the general population, the mean score on the SF-36 is 50. Lower scores indicate poorer health. The researchers will test to see if the mean score of those who have had a cardiac arrest is less than 50.\n\n\n\n\nState the null and alternative hypotheses and the level of significance.\n\n\n\nShow/Hide Solution\n\n\n\\[\n\\begin{array}{lcl}\nH_0:~~\\mu = 50\\\\\nH_a:~~\\mu &lt; 50\\\\\n\\alpha = 0.05\n\\end{array}\n\\]\n\n\nRecall that we are testing to see if the mean is less than 50.\n\n\n\n\n\nDescribe the data collection procedures.\n\n\n\nShow/Hide Solution\n\n\nThe researchers collected data from \\(n=50\\) patients who had suffered a cardiac arrest outside a hospital. The data were collected using the SF-36 survey instrument.\n\n\n\n\n\nGive the relevant summary statistics.\n\n\n\nShow/Hide Solution\n\n\n\\[\n\\begin{array}{l}\n\\bar{x} = 47.82\\\\\n\\sigma = 10\\\\\nn = 50\n\\end{array}\n\\]\n\n\n\n\nMake an appropriate graph to illustrate the data.\n\n\n\nShow/Hide Solution\n\n\n\n\n\n\nVerify the requirements have been met.\n\n\n\nShow/Hide Solution\n\n\nWe assume that the individuals chosen to participate in the study represent a (simple) random sample from the population.\n\\(\\bar{x}\\) will be normally distributed, because the sample size is large.\n\n\n\n\nGive the test statistic and its value.\n\n\n\nShow/Hide Solution\n\n\n\\(\\displaystyle{ z=\\frac{\\bar{x} - \\mu}{\\sigma / \\sqrt{n}} = \\frac{47.82 - 50}{10 / \\sqrt{50}} = -1.5415 }\\)\n\n\n\n\nMark the test statistic and \\(P\\)-value on a graph of the sampling distribution.\n\n\n\nShow/Hide Solution\n\n\n\n\n\nFind the \\(P\\)-value and compare it to the level of significance.\n\n\n\nShow/Hide Solution\n\n\n\\(P\\textrm{-value} = 0.0616 &gt; 0.05 = \\alpha\\)\n\n\n\n\nState your decision.\n\n\n\nShow/Hide Solution\n\n\nSince the \\(P\\)-value is greater than \\(\\alpha\\), we fail to reject the null hypothesis.\n\n\n\n\n\nPresent your conclusion in an English sentence, relating the result to the context of the problem.\n\n\n\nShow/Hide Solution\n\n\nThere is insufficient evidence to suggest that the mean perceived general health of patients after a cardiac arrest is worse than the general population. Patients who suffer from a cardiac arrest typically have the potential to return to full health"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/001-Introduction_to_Inference.html#structure",
    "href": "5-Statistical_Tests_Part1/001-Introduction_to_Inference.html#structure",
    "title": "Introducing Inference",
    "section": "Structure",
    "text": "Structure\nWhen \\(\\sigma\\) is known, we conduct the hypothesis test by hand. (Note that this is not realistic, since \\(\\sigma\\) is not known in most interesting real situations. However, this will help you learn how to do hypothesis tests.)\nBe sure to always conduct a test using \\(P\\)-values. In this course, we will never compute or use critical values as part of a hypothesis test.\nIn the \\(P\\)-value method, you compare a computed \\(P\\)-value to your selected \\(\\alpha\\)-level. If the \\(P\\)-value is less than \\(\\alpha\\), you will reject the null hypothesis. Otherwise, you fail to reject the null hypothesis. We never say we “accept” the null hypothesis.\nHypothesis tests follow the same basic pattern.\n\n\nSummarize the relevant background information\nState the null and alternative hypotheses\n\n\n\nDescribe the data collection procedures\n\n\n\nGive the relevant summary statistics\nMake an appropriate graph to illustrate the data\n\n\n\nVerify the requirements have been met\nGive the test statistic and its value\nMark the test statistic and \\(P\\)-value on a graph of the sampling distribution\nFind the \\(P\\)-value and compare it to the level of significance\nState your decision\n\n\n\nPresent your conclusion in an English sentence, relating the result to the context of the problem"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/001-Introduction_to_Inference.html#requirements-1",
    "href": "5-Statistical_Tests_Part1/001-Introduction_to_Inference.html#requirements-1",
    "title": "Introducing Inference",
    "section": "Requirements",
    "text": "Requirements\nThere are three requirements that need to be checked when conducting a hypothesis test for a mean with \\(\\sigma\\) known:\n\nA simple random sample was drawn from the population\n\\(\\bar{x}\\) is normally distributed\n\\(\\sigma\\) is assumed to be known\n\nIt is rare for the first requirement to be satisfied in practice. If the results are to generalize to the population, the sample needs to be representative of the whole population. This is accomplished through random sampling.\nThe second requirement is satisfied if (a) the raw data are normally distributed or (b) the sample size is large. In practice, these tests tend to be fairly robust when the requirement of normality is violated. Even if the requirement of normality is not satisfied perfectly, it is usually okay to conduct the test.\nIn practice, we never really know \\(\\sigma\\). This procedure is primarily used to help you understand how to conduct a hypothesis test. When we do not know \\(\\sigma\\), there is a slightly different procedure, which you will learn soon."
  },
  {
    "objectID": "5-Statistical_Tests_Part1/001-Introduction_to_Inference.html#additional-worked-examples",
    "href": "5-Statistical_Tests_Part1/001-Introduction_to_Inference.html#additional-worked-examples",
    "title": "Introducing Inference",
    "section": "Additional Worked Examples",
    "text": "Additional Worked Examples\nConducting a hypothesis test involves the application of all of the processes we have learned in this course. Viewing additional examples can help your understanding. Click on the link at right to see two more examples.\n\n\nShow Additional Examples\n\n\n\n\nMean age of people who read the New York Times online\n\n\nSummarize the relevant background information\n\n\nThe New York Times is a large newspaper that is available both in the traditional print format and online. It is known that the mean age of readers of the print edition is \\(42\\) years. The standard deviation of the age of readers is known to be \\(12\\) years. The research question is: Is the mean age of readers of the online version of the Times less than \\(42\\) years?\nThe population is all people who read the Times online. The observational units are the individual customers. The age of each person who was selected and agreed to participate was measured. The customer’s age is a quantitative random variable.\n\n\nState the null and alternative hypotheses and the level of significance\n\n\\[\n\\begin{align}\nH_0:&~~\\mu = 42 \\\\\nH_a:&~~\\mu &lt; 42\n\\end{align}\n\\]\n\\[ \\alpha = 0.05 \\]\n\n\nDescribe the data collection procedures\n\n\n: Using a simple random sample, data were collected on \\(n = 25\\) people who read the Times online. Their ages were recorded.\n\n\n\n\nMake an appropriate graph to illustrate the data\n\n\nA boxplot shows the mean age of the online customers (represented by the x on the graph) and the median both appear to be between 30 and 40, less than \\(42\\) years.\n\n\n\nGive the relevant summary statistics\n\n\nThe mean age of the sampled subjects was \\(\\bar{x} = 35.32\\). There were \\(n = 25\\) people included in the sample.\n\n\n\nVerify the requirements have been met\n\n\nThe data represent a simple random sample from the population. A look at the histogram (not shown) would reveal that the data is not perfectly normal. However, the sample data is approximately normal. In other words, it is close enough to a normal shape to meet the requirement.\n\n\n\nGive the test statistic and its value\n\n\\[\\displaystyle{ z = \\frac{\\bar{x} - \\mu}{\\sigma/\\sqrt{n}} = \\frac{35.32-42}{12/\\sqrt{25}} = -2.7833 }\\]\n\nMark the test statistic and \\(P\\)-value on a graph of the sampling distribution\n\n\n\nFind the \\(P\\)-value and compare it to the level of significance\n\n\\[P\\text{-value} = 0.0027 &lt; 0.05 = \\alpha\\]\n\nState your decision\n\n\nSince the \\(P\\)-value is less than the level of significance, we reject the null hypothesis.\n\n\n\nPresent your conclusion in an English sentence, relating the result to the context of the problem\n\n\nReject Ho. There is sufficient evidence to suggest that the mean age of readers of the online edition of the Times is less than 42 years.\nIt is reasonable to conclude that the mean age of those who read the Times online is less than 42. There does appear to be a difference in the ages of those who read the print (\\(\\mu = 42\\)) years and the online editions of the paper.\n\n\n\n\n\nMean GPA of early risers\nIn Doctrine and Covenants 88:124, the Lord commands, “…cease to sleep longer than is needful; retire to thy bed early, that ye may not be weary; arise early, that your bodies and your minds may be invigorated.” Are there academic benefits from obeying this commandment? People who sleep late tend to be stereotyped as slackers. It is not fair to make these claims without any scientific evidence. The purpose of this study is used to address whether early risers earn better grades.\nResearcher K. Clay and others presented a paper at the Associated Professional Sleep Societies meeting [4]. They suggested that the mean GPA of students who are early risers tends to be higher than average. The mean grade point average (GPA) of all students at BYU-Idaho is \\(\\mu = 3.15\\). The population standard deviation for the grades of students at BYU-Idaho is \\(\\sigma= 0.68\\).\nA simple random sample of \\(n = 378\\) early-rising BYU-Idaho students was collected. The students were asked to report their BYU-Idaho GPA. The GPA of each person who was selected and signed the informed consent statement (to authorize the use of their data) was recorded.\n\n\nSummarize the relevant background information\n\n\nThe research question is: Is the true mean GPA of early risers greater than \\(3.15\\)?\nThe population of interest is all students at BYU-Idaho who are early risers. They will be compared to the overall population of BYU-Idaho. The observational units are the individual students. GPA is a quantitative random variable.\n\n\nState the null and alternative hypotheses and the level of significance\n\n\\[\n\\begin{align}\nH_0:&~~\\mu = 3.15 \\\\\nH_a:&~~\\mu &gt; 3.15\n\\end{align}\n\\]\n\\[\\alpha = 0.05\\]\n\n\nDescribe the data collection procedures\n\n\nA simple random sample of BYU-Idaho students was selected. Only those identified as early risers were included in the survey. Data were collected on \\(n = 378\\) students who tend to be early risers.\n\n\n\n\nGive the relevant summary statistics\n\n\nThe mean GPA of the subjects was \\(\\bar{x} = 3.21\\). The sample size is \\(n = 378\\).\n\n\nMake an appropriate graph to illustrate the data\n\nThe histogram shows that the data are left-skewed. That is, many of the sampled students have fairly high GPAs, but a few are very low.\n\n\n\nVerify the requirements have been met\n\n\n\n\nA simple random sample of \\(n = 378\\) early-rising students at BYU-Idaho was collected. It is reasonable to conclude that the mean of the GPAs are approximately normally distributed, since the sample size is large, even though the GPA data are left-skewed.\n\n\nGive the test statistic and its value\n\n\\[\\displaystyle{ z = \\frac{\\bar{x} - \\mu}{\\sigma/\\sqrt{n}} = \\frac{3.21-3.15}{0.68/\\sqrt{378}} = 1.7155 }\\]\n\nMark the test statistic and \\(P\\)-value on a graph of the sampling distribution\n\n\n\nFind the \\(P\\)-value and compare it to the level of significance\n\n\\[P\\text{-value} = 0.0431 &lt; 0.05 =  \\alpha\\]\n\nState your decision\n\n\nReject Ho. There is sufficient evidence to suggest that the mean GPA of early risers is greater than \\(3.15\\).\n\n\n\nPresent your conclusion in an English sentence, relating the result to the context of the problem\n\n\nIt is reasonable to conclude that the mean GPA of early risers is greater than \\(3.15\\). This suggests that there may be some benefit to getting out of bed early in the morning.\n\n\n\nSummary\nThe previous examples in this reading illustrate the three types of alternative hypotheses: two-tailed, left-tailed and right-tailed. In each case, the structure of the hypothesis test is basically the same. However, when the \\(z\\)-score is entered into the applet to compute the \\(P\\)-value, it is important to note whether to shade both tails, the left tail, or the right tail of the standard normal or (\\(z\\)) distribution."
  },
  {
    "objectID": "5-Statistical_Tests_Part1/001-Introduction_to_Inference.html#references",
    "href": "5-Statistical_Tests_Part1/001-Introduction_to_Inference.html#references",
    "title": "Introducing Inference",
    "section": "References",
    "text": "References\n[1] R. A. Fisher. Statistical tests. Nature, 136, 1935.\n[2] H. Kieler, O. Axelsson, Nilsson S., and U. Waldenstr. The length of human pregnancy as calculated by ultrasonographic measurement of the fetal biparietal diameter. Ultrasound in Obstetrics and Gynecology, 6(5):353–357, 1995.\n[3] Timothy Harper. All the news that’s fit to BYTE. Delta Sky Magazine, 106:92–96, May 2007.\n[4] K. Clay et. al. Morningness and eveningness relationship to college GPA. APSS Meeting, 2008."
  },
  {
    "objectID": "5-Statistical_Tests_Part1/002-Introduction_to_Inference.html",
    "href": "5-Statistical_Tests_Part1/002-Introduction_to_Inference.html",
    "title": "Inference for a Mean",
    "section": "",
    "text": "Statistical Inference is the practice of using data sampled from a population to make conclusions about population parameters.\nThe two primary methods of statistical inference are:\n\nHypothesis Testing\nConfidence Intervals\n\nHypothesis testing is the practice of gathering evidence in an attempt to disprove a hypothesis. We either reject the conventional wisdom or fail to reject it.\nConfidence intervals are a way to use data to estimate the unknown population parameter, often the population mean, \\(\\mu\\).\nThis chapter lays the foundation for Hypothesis Testing.\n\n\nHypothesis testing depends on the validity of the assumption that the distribution of the sample mean is normally distributed.\nRecall that the distribution of sample means is normal when:\n\nThe underlying population is normally distributed\nThe sample size, n, is sufficiently large (\\(n&lt;30\\) for this class) for the Central Limit Theorem to apply"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/002-Introduction_to_Inference.html#review-distribution-of-sample-means",
    "href": "5-Statistical_Tests_Part1/002-Introduction_to_Inference.html#review-distribution-of-sample-means",
    "title": "Inference for a Mean",
    "section": "",
    "text": "Hypothesis testing depends on the validity of the assumption that the distribution of the sample mean is normally distributed.\nRecall that the distribution of sample means is normal when:\n\nThe underlying population is normally distributed\nThe sample size, n, is sufficiently large (\\(n&lt;30\\) for this class) for the Central Limit Theorem to apply"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/002-Introduction_to_Inference.html#testing-fo-the-true-mean-length-of-a-footlong-sandwich",
    "href": "5-Statistical_Tests_Part1/002-Introduction_to_Inference.html#testing-fo-the-true-mean-length-of-a-footlong-sandwich",
    "title": "Inference for a Mean",
    "section": "Testing fo the True Mean Length of a Footlong Sandwich",
    "text": "Testing fo the True Mean Length of a Footlong Sandwich\nOne might expect a foot-long sandwich to be 12 inches long, at least on average. Consumers will naturally allow for some level of variation. But if enough customers are convinced they are getting consistently short-changed, they may loudly complain, or even attempt a lawsuit.\n\nThe Null Hypothesis\nProper scientific inquiry is based on attempts to disprove a claim. The claim representing the “status quo,” the commonly held belief or the usual value is called the null hypothesis. In the case of foot-long sandwiches, our null hypothesis is that the true mean length of foot-long sandwiches is 12 inches.\nWe present the null hypothesis in the following way:\n\\[H_0: \\mu = 12\\] We label the null hypothesis \\(H_0\\), often pronounced “H-nought”. The zero in the subscript represents “null,” “baseline,” “default,” “no effect,” etc.\nThe null hypothesis is standing trial. We seek evidence against \\(H_0\\) in favor of an alternative.\n\n\nThe Alternative Hypothesis\nThe alternative hypothesis, \\(H_a\\), is the proposed challenge to \\(H_0\\). In the case of foot-long sandwiches, customers will typically only get upset if they are getting less than advertised. We would propose an alternative:\n\\[H_a: \\mu &lt; 12\\]\nThere are other possible alternatives that will depend on the context of the research question. Sometimes we may want to test if something is higher than a proposed value. Sometimes we are not sure if something is higher or lower at the outset, so we could test if something is not equal to a proposed value.\nAlternative hypothesis could have been written as:\n\n\\(H_a: \\mu \\ne 12\\) (two-sided hypothesis; two-tailed)\n\\(H_a: \\mu &lt; 12\\) (one-sided hypothesis; left-tailed)\n\\(H_a: \\mu &gt; 12\\) (one-sided hypothesis; right-tailed)\n\nIt is important that the null and alternative hypotheses be determined prior to collecting the data. It is not appropriate to use the data from your study to choose the alternative hypothesis that will be used to test the same data! This is an example of using data twice, once to choose the test and again to conduct the test. It is okay to use data from a previous study to determine your null and alternative hypotheses, but it is an improper use of the statistical procedures to use the data to define and conduct a hypothesis test.\nKEY POINTS:\n\nHypotheses are statements about population parameters\nThe null hypothesis will always be a statement of equality\nWe never PROVE the null hypothesis is true, we can only fail to disprove it.\n\nThat last point is important. Data collected are only evidence against \\(H_0\\). This is similar to the situation in a courtroom where a defendant is accused of a crime. The defendant does not have to PROVE their innocence. It is up to the prosecution to prove guilt. If they fail to do so, we find the defendant “not guilty”, which is not the same thing as innocent. It just means there wasn’t enough evidence to convict.\n\n\nTest Statistic\nWe use Test Statistics to determine how likely our results are assuming the null hypothesis is true. We will use many different test statistics throughout this course, but the first one will be very familiar: the Z-score.\nWhen testing a null hypothesis, the \\(\\mu\\) in the z-score formula becomes the hypothesized population mean. The \\(z\\)-score is then interpreted as the number of standard deviations away from the hypothesized mean. If we know the population standard deviation, \\(\\sigma\\), then we can calculate the probability of getting a sample mean more extreme than the one we observed if the null hypothesis is true.\nSuppose for now that sandwich-to-sandwich lengths vary by about 0.5 inches and that the distribution is normal. If we sampled 21 random sandwiches from shops in a region and got a sample mean length of 11.82 inches, we can calculate a Z-score:\n\\[\nz = \\frac{\\bar{x} - \\mu_0}{\\sigma / \\sqrt{n}} = \\frac{11.82 - 12}{0.5 / \\sqrt{21}} = -1.649727\n\\]\nWhere \\(z\\) is the test statistic, \\(\\bar{x}\\) is the sample mean, \\(\\mu_0\\) is the null hypothesis mean, \\(\\sigma\\) is the population standard deviation and \\(n\\) is the sample size.\nBased on the results from the formula above, \\(\\bar{x}\\) is about 1.65 standard deviations below the mean.\nWe can then use this Z-score to calculate the probability of observing such a result or more extreme, if the null hypothesis is true. This is the evidence we are going to use to make a decision about the null hypothesis.\n\n\n\\(P\\)-value\n\\(P\\)-value: The probability of obtaining a test statistic (such as \\(z\\)) at least as extreme as the one you calculated, assuming the null hypothesis is true.\nIn other words, our \\(P\\)-value is the probability that we would get a \\(z\\)-score that is as extreme or more extreme than \\(z=-1.649727\\), assuming the true mean is 12 inches.\n\n\n\n\n\n\n\n\n\nKEY DEFINITION: A P-value is the probability of observing a test statistic as extreme, or more extreme than the one we observed in our sample, if the null hypothesis is true.\nWe use “as or more extreme” because the direction (greater than or less than) depends on our alternative hypothesis.\nIn the case against the sandwich shop, we can use pnorm() to get the P-value:\n\nz &lt;- (11.82-12) / (0.5/sqrt(21))\n\npnorm(z)\n\n[1] 0.04949937\n\n\nConclusion: If the true population mean was 12 inches, there is a 4.95% chance of obtaining a sample mean of 11.82 hours for a sample of size 21.\nAt this point we have to make a decision. Is that probability small enough to reject the null hypothesis in favor the alternative? Are foot-long sandwich is LESS than 12 inches on average?\nWhen the P-value is very small, we have strong evidence to reject the null hypothesis. But how small is small enough?\n\n\nLevel of Significance, \\(\\alpha\\)\nWe need a number that can be used to determine if the \\(P\\)-value is small enough to reject the null hypothesis that is not dependent on the data. This number is called the level of significance and is often denoted by the symbol \\(\\alpha\\) (pronounced “alpha”.)\nWe will use the same decision rule for all hypothesis tests:\n\nIf the \\(P\\)-value is less than \\(\\alpha\\), we reject the null hypothesis.\nIf the \\(P\\)-value is greater than \\(\\alpha\\), we fail to reject the null hypothesis.\n\nMemory Aid: Some students find it helpful to remember the decision rule using the couplet:\nIf \\(P\\) is low, reject \\(H_O\\)\nWhere “low” means less than \\(\\alpha\\).\nThe level of significance, \\(\\alpha\\), must be chosen prior to collecting the data. If we wanted to reject the null hypothesis, we could compute the \\(P\\)-value and then unscrupulously choose the \\(\\alpha\\) level. In every case, we could choose a value of \\(\\alpha\\) that is larger than the computed \\(P\\)-value, and therefore always reject the null hypothesis. This practice would defeat the purpose of hypothesis testing.\n\n\nType I and Type II Errors\nThis is not an infallible process. We may end up rejecting a null hypothesis that is, in fact, true. Or fail to reject a null hypothesis that is, in fact, false.\nThis is because researchers will sometimes get a very high or very low sample mean purely by chance. Perhaps their sampling methods were not as random as they had supposed. We may reject or fail to reject a null hypothesis because of dumb luck.\nType I Error: Rejecting a TRUE null hypothesis.\nAssuming the null hypothesis is true, the level of significance (\\(\\alpha\\)) is the probability of getting a value of the test statistic that is extreme enough that the null hypothesis will be rejected. In other words, the level of significance (\\(\\alpha\\)) is the probability of committing a Type I error.\nLet’s demonstrate \\(\\alpha = 0.01\\) and shade it in the graph below:\n\n\n\n\n\n\n\n\n\nThe red area, \\(\\alpha\\), shows the probability of getting a sample mean in that tail when the null hypothesis is true. There’s always a chance we get such an extreme value and end up rejecting the null when it is true.\nThe most common choice for \\(\\alpha\\) is \\(\\alpha=0.05\\), but other choices include \\(\\alpha = 0.1\\) and \\(\\alpha=0.01\\).\nWe can set \\(\\alpha\\) arbitrarily low to avoid Type I errors, but there’s a cost. The more difficult it is to reject the Null Hypothesis, the more likely we will be to make a Type II Error.\nType II Error: Failing to Reject a FALSE null hypothesis.\nIf the \\(\\alpha\\) value (the probability of committing a Type I error) is very small, the probability of committing a Type II error will be large. Conversely, if \\(\\alpha\\) is allowed to be very large, then the probability of committing a Type II error will be very small.\nA level of significance of \\(\\alpha=0.05\\) seems to strike a good balance between the probabilities of committing a Type I versus a Type II error. However, there may be instances where it will be important to choose a different value for \\(\\alpha\\). The important thing is to choose \\(\\alpha\\) before you collect your data. Typical choices of \\(\\alpha\\) are \\(0.05\\) (most common), \\(0.1\\), and \\(0.01\\).\n\nVisualizing Error\nThe below graph illustrates the relationship between Type I and Type II errors. The red distribution represents the Null Hypothesis, the sampling distribution of sample mean-lengths of foot-long sandwiches, assuming \\(\\mu_0=12\\).\nThe blue distribution represents the “TRUE” distribution with a mean, \\(\\mu_{truth}=11.2\\). We never know this in the real world, but to illustrate, here is a situation where the true population mean is, in fact, less than 12.\nIf we set \\(\\alpha = 0.01\\) (red shaded area), we can see that any sample mean to the right of the cutoff would fail to reject the null hypothesis. This mistake would be a Type II error.\n\n\n\n\n\n\n\n\n\nWhile we don’t know what the “TRUE” distribution is, we can see the relationship between moving the cutoff based on \\(\\alpha\\) would do to the probability of failing to reject the null hypothesis even when it is FALSE."
  },
  {
    "objectID": "5-Statistical_Tests_Part1/02-AA_Hypothesis_Conf_Int.html",
    "href": "5-Statistical_Tests_Part1/02-AA_Hypothesis_Conf_Int.html",
    "title": "Don’t Take it Personally",
    "section": "",
    "text": "Introduction\nIn this activity, you will execute statistical hypothesis tests and generate confidence intervals for each of the Big 5 personality traits using data collected from a random sample of Brother Cannon’s Math 221 students.\nQuestion: What is the population of this analysis?\nAnswer:\nFor each personality trait, include:\n\nA statement of the null and alternative hypotheses and why you chose the alternative you did.\nChoose alpha, $= $\nCheck that you can trust the normality of the mean (n &gt; 30 or qqPlot(respons_variable))\nRun the one-sample t-test and state your conclusion (technical and contextual explanation)\nCalculate a \\(1-\\alpha\\) level confidence interval and describe in words what it means in context of the research question\n\nRecall that we can use favstats() to get summary statistics, boxplot() and histogram() to get visualizations, and the t.test() function to get hypothesis tests and confidence intervals. Be sure to label your plots’ axes and include a title.\n\n# Load Libraries\n\nlibrary(tidyverse)\nlibrary(mosaic)\nlibrary(rio)\nlibrary(car)\n\n\n# Load Data\nbig5 &lt;- import('https://raw.githubusercontent.com/byuistats/Math221D_Cannon/master/Data/All_class_combined_personality_data.csv')\n\n\n\nExtroversion\nState your null and alternative hypotheses:\n\\[H_o:  \\mu = 50\\]\n\\[H_a:  \\mu &gt; 50\\]\n\\[\\alpha = 0.025 \\]\n1. Create a table of summary statistics:\n\n# Extroversion\n\nfavstats(big5$Extroversion) %&gt;% knitr::kable()\n\n\n\n\n\nmin\nQ1\nmedian\nQ3\nmax\nmean\nsd\nn\nmissing\n\n\n\n\n\n1\n42\n58\n73\n100\n56.98267\n21.09599\n404\n1\n\n\n\n\n\n\nCreate a histogram of Extroversion:\n\n\n### I'm making a single variable called \"extrov\" that drops the missing values.  You can do the same thing with the other traits to make analysis a little easier\nextrov &lt;- na.omit(big5$Extroversion) \n\nhistogram(extrov, xlab = \"Extroversion\", main = \"Histogram of Extroversion Percentiles\")\n\n\n\n\n\n\n\n\n\nPerform the one-sample t.test:\n\n\n# For Hypothesis Test:\nt.test(extrov, mu = 50, alternative = \"greater\")\n\n\n    One Sample t-test\n\ndata:  extrov\nt = 6.6529, df = 403, p-value = 4.697e-11\nalternative hypothesis: true mean is greater than 50\n95 percent confidence interval:\n 55.25232      Inf\nsample estimates:\nmean of x \n 56.98267 \n\n\n\nExplain your conclusion:\n\nTechnical: Because the p-value is less than \\(\\alpha\\), I reject the null hypothesis.\nContextual: I have sufficient evidence to suggest that Brother Cannon’s students are, on average, more Extroverted than the general population.\n\nCreate a Confidence Interval for the average extroversion of Brother Cannon’s students:\n\n\n# For a Confidence Interval:\nt.test(extrov, conf.level = 1-.025)$conf.int\n\n[1] 54.62135 59.34399\nattr(,\"conf.level\")\n[1] 0.975\n\n\n\nExplain your confidence interval:\n\nI am 97.5% confident that the true average extroversion of Brother Cannon’s students is somewhere between the 54.62 and 59.34 percentiles.\n\n\nAgreeableness\nState your null and alternative hypotheses. For example, do you think Brother Cannon’s students are more, less, or just different than the general population?\n\\[H_o:  \\mu  \\]\n\\[H_a:  \\mu \\]\n\\[\\alpha =  \\]\n1. Create a table of summary statistics for Agreeableness:\n\nCreate a histogram of Agreeableness:\n\n\nPerform the one-sample t.test:\n\n\nExplain your conclusion:\n\nTechnical:\nContextual:\n\nCreate a Confidence Interval for the average agreeableness of Brother Cannon’s students:\n\n\n# For a Confidence Interval:\nt.test()$conf.int\n\nError in t_test.default(): argument \"x\" is missing, with no default\n\n\n\nExplain your confidence interval:\n\n\n\nOpenness\nState your null and alternative hypotheses:\n\\[H_o:  \\mu \\]\n\\[H_a:  \\mu \\]\n\\[\\alpha = \\]\n\nCreate a table of summary statistics for Openness:\n\n\nCreate a histogram of Openness:\n\n\nPerform the one-sample t.test:\n\n\nExplain your conclusion:\n\nTechnical:\nContextual:\n\nCreate a Confidence Interval for the average openness of Brother Cannon’s students:\n\n\n# For a Confidence Interval:\nt.test()$conf.int\n\nError in t_test.default(): argument \"x\" is missing, with no default\n\n\n\nExplain your confidence interval:\n\n\n\nNeuroticism\nState your null and alternative hypotheses:\n\\[H_o:  \\mu \\]\n\\[H_a:  \\mu \\]\n\\[\\alpha =  \\]\n1. Create a table of summary statistics for Neuroticism:\n\nCreate a histogram of Neuroticism:\n\n\nPerform the one-sample t.test:\n\n\nExplain your conclusion:\n\nTechnical:\nContextual:\n\nCreate a Confidence Interval for the average neuroticism of Brother Cannon’s students:\n\n\n# For a Confidence Interval:\nt.test()$conf.int\n\nError in t_test.default(): argument \"x\" is missing, with no default\n\n\n\nExplain your confidence interval:\n\n\n\nConscientiousness\nState your null and alternative hypotheses:\n\\[H_o:  \\mu \\]\n\\[H_a:  \\mu \\]\n\\[\\alpha = \\]\n1. Create a table of summary statistics:\n\nCreate a histogram of Conscientiousness:\n\n\nPerform the one-sample t.test:\n\n\nExplain your conclusion:\n\nTechnical:\nContextual:\n\nCreate a Confidence Interval for the average conscientiousness of Brother Cannon’s students:\n\n\n# For a Confidence Interval:\nt.test()$conf.int\n\nError in t_test.default(): argument \"x\" is missing, with no default\n\n\n\nExplain your confidence interval:"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/04-Paired_ttest_Practice.html",
    "href": "5-Statistical_Tests_Part1/04-Paired_ttest_Practice.html",
    "title": "Paired T-Test Practice",
    "section": "",
    "text": "Here are several opportunities to practice analyzing dependent samples using R. For each question, you will:\n\nRead in data\nCreate data summaries (numerical and graphical)\nStatistically analyze the data\nCheck for the suitability of the statistical test (CLT, Normality)\nState your hypothesis test conclusions and interpret your confidence intervals"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/04-Paired_ttest_Practice.html#step-1-load-the-data",
    "href": "5-Statistical_Tests_Part1/04-Paired_ttest_Practice.html#step-1-load-the-data",
    "title": "Paired T-Test Practice",
    "section": "Step 1: Load the Data",
    "text": "Step 1: Load the Data\n\nlibrary(tidyverse)\nlibrary(mosaic)\nlibrary(rio)\nlibrary(car)\n\nhelmet_fit &lt;- import(\"https://byuistats.github.io/M221R/Data/quiz/R/helmet_fit.csv\")"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/04-Paired_ttest_Practice.html#step-2-explore-the-data-and-generate-hypotheses",
    "href": "5-Statistical_Tests_Part1/04-Paired_ttest_Practice.html#step-2-explore-the-data-and-generate-hypotheses",
    "title": "Paired T-Test Practice",
    "section": "Step 2: Explore the Data and Generate Hypotheses",
    "text": "Step 2: Explore the Data and Generate Hypotheses\nCreate histograms and summary statistic tables for the measurements for each type of tool:"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/04-Paired_ttest_Practice.html#step-3-prepare-the-data-for-analysis",
    "href": "5-Statistical_Tests_Part1/04-Paired_ttest_Practice.html#step-3-prepare-the-data-for-analysis",
    "title": "Paired T-Test Practice",
    "section": "Step 3: Prepare the data for analysis",
    "text": "Step 3: Prepare the data for analysis\nGive the summary statistics (favstats()) for the differences in the measured head diameters.\nQuestion: What does a negative number mean given the definition of the difference?\nAnswer:\nCreate a qqPlot() of the differences:"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/04-Paired_ttest_Practice.html#step-4-perform-the-appropriate-analyses",
    "href": "5-Statistical_Tests_Part1/04-Paired_ttest_Practice.html#step-4-perform-the-appropriate-analyses",
    "title": "Paired T-Test Practice",
    "section": "Step 4: Perform the Appropriate Analyses",
    "text": "Step 4: Perform the Appropriate Analyses\n\nHypothesis Test\nMake the following null and alternative hypothesis correct by deleting what doesn’t belong:\n\\[H_0: \\mu_d  &gt; &lt; = \\ne 0\\]\n\\[ H_a: \\mu_d &gt; &lt; = \\ne 0\\]\n\n# Perform a t-test for the mean of the differences between cardboard and caliper data\n\nQuestion: What is the value of the test statistic, \\(t\\)?\nAnswer:\nQuestion: How many degrees of freedom does this test statistics have?\nAnswer:\nQuestion: What is the P-value?\nAnswer:\nQuestion: Can we trust this P-value? (eg. How many differences in our sample? Check the qqPlot() of the differences for normality)\nAnswer:\nQuestion: State your conclusion about the hypothesis test.\nAnswer:\n\n\nConfidence Intervals\nCreate a 95% confidence interval for the true average difference between the cardboard and the metal measurement tools:\nQuestion: Give a one-sentence explanation of your confidence interval.\nAnswer:"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/04-Paired_ttest_Practice.html#step-1-load-the-data-1",
    "href": "5-Statistical_Tests_Part1/04-Paired_ttest_Practice.html#step-1-load-the-data-1",
    "title": "Paired T-Test Practice",
    "section": "Step 1: Load the Data",
    "text": "Step 1: Load the Data\n\ncholesterol &lt;- import(\"https://byuistats.github.io/M221R/Data/quiz/R/cholesterol.csv\")"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/04-Paired_ttest_Practice.html#step-2-review-the-data-generate-hypotheses",
    "href": "5-Statistical_Tests_Part1/04-Paired_ttest_Practice.html#step-2-review-the-data-generate-hypotheses",
    "title": "Paired T-Test Practice",
    "section": "Step 2: Review the Data Generate Hypotheses",
    "text": "Step 2: Review the Data Generate Hypotheses\nCreate histograms and summary statistics tables for the cholesterol measurements at 2 days and at 4 days."
  },
  {
    "objectID": "5-Statistical_Tests_Part1/04-Paired_ttest_Practice.html#step-3-prepare-the-data-for-analysis-1",
    "href": "5-Statistical_Tests_Part1/04-Paired_ttest_Practice.html#step-3-prepare-the-data-for-analysis-1",
    "title": "Paired T-Test Practice",
    "section": "Step 3: Prepare the data for analysis",
    "text": "Step 3: Prepare the data for analysis\nDecide how you are going to define the difference (chol_day2 - chol_day4 or chol_day4 - chol_day2).\nWhat does a negative number mean:\nWhat is your null and alternative hypotheses:\n\\[H_0: \\mu_d  &gt; &lt; = \\ne 0\\]\n\\[ H_a: \\mu_d &gt; &lt; = \\ne 0\\]\nCreate a qqPlot() of the differences and determine if you can trust the statistical tests."
  },
  {
    "objectID": "5-Statistical_Tests_Part1/04-Paired_ttest_Practice.html#step-4-perform-the-appropriate-analysis",
    "href": "5-Statistical_Tests_Part1/04-Paired_ttest_Practice.html#step-4-perform-the-appropriate-analysis",
    "title": "Paired T-Test Practice",
    "section": "Step 4: Perform the appropriate analysis",
    "text": "Step 4: Perform the appropriate analysis\n\nHypothesis Test\nPerform a t-test for the differences.\nQuestion: What is the value of the test statistic, \\(t\\)?\nAnswer:\nQuestion: How many degrees of freedom does this test statistics have?\nAnswer:\nQuestion: What is the P-value?\nAnswer:\nQuestion: Can we trust the P-value?\nAnswer:\nQuestion: State your conclusion about the hypothesis test.\nAnswer:\n\n\nConfidence Interval\nCreate a 95% confidence interval for the difference in cholesterol scores:\nQuestion: Explain your confidence interval in context of the research question\nAnswer:"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/06-Independent_2_sample_ttest_practice.html",
    "href": "5-Statistical_Tests_Part1/06-Independent_2_sample_ttest_practice.html",
    "title": "2-Sample Independent T-Test Practice",
    "section": "",
    "text": "Here are several opportunities to practice analyzing 2-sample independent t-tests using R. For each question, you will:\n\nRead in data\nIdentify the Response and Independent variable\nCreate data summaries (numerical and graphical)\nStatistically analyze the data\nCheck for the suitability of the statistical test (CLT, Normality)\nState your hypothesis test conclusions and interpret your confidence intervals\n\nWhen you finish, render this document and submit the .html in Canvas.\n\n# Load the libraries\n\nlibrary(rio)\nlibrary(mosaic)\nlibrary(tidyverse)\nlibrary(car)"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/06-Independent_2_sample_ttest_practice.html#load-the-data",
    "href": "5-Statistical_Tests_Part1/06-Independent_2_sample_ttest_practice.html#load-the-data",
    "title": "2-Sample Independent T-Test Practice",
    "section": "Load the Data",
    "text": "Load the Data\n\ndating &lt;- read_csv('https://github.com/byuistats/Math221D_Course/raw/main/Data/dating_attractive_longformat.csv')"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/06-Independent_2_sample_ttest_practice.html#explore-the-data",
    "href": "5-Statistical_Tests_Part1/06-Independent_2_sample_ttest_practice.html#explore-the-data",
    "title": "2-Sample Independent T-Test Practice",
    "section": "Explore the Data",
    "text": "Explore the Data\nQuestion: What is the response variable?\nAnswer:\nQuestion: What is the explanatory variable?\nAnswer:\nCreate a side-by-side boxplot for the amount of reported importance of attractiveness for each biosex.\nAdd a title and change the colors of the boxes.\nWhat do you observe?\nCreate a table of summary statistics for each group (favstats()):"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/06-Independent_2_sample_ttest_practice.html#hypothesis-test",
    "href": "5-Statistical_Tests_Part1/06-Independent_2_sample_ttest_practice.html#hypothesis-test",
    "title": "2-Sample Independent T-Test Practice",
    "section": "Hypothesis Test",
    "text": "Hypothesis Test\nState your null and alternative hypotheses (replace the ??? with the appropriate symbol):\n\\[H_0:  \\mu_{F}???\\mu_{M}\\]\n\\[H_a:\\mu_{F}???\\mu_{M}\\]\nNOTE: The default for R is to set group order alphabetically. This means Group 1 = Female.\nCheck that the samples for both groups are normally distributed\n\nqqPlot(dating$Importance~dating$Biosex)\n\n\n\n\n\n\n\n\nDo the data for each group appear normally distributed?\nWhy is it OK to continue with the analysis?\nPerform a t-test.\nQuestion: What is the value of the test statistic?\nAnswer:\nQuestion: How many degrees of freedom for this test?\nAnswer:\nQuestion: What is the p-value?\nAnswer:\nQuestion: What do you conclude?\nAnswer:\n\nConfidence Interval\nCreate a confidence interval for the difference of the average Importance Score between both groups:"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/06-Independent_2_sample_ttest_practice.html#review-the-data",
    "href": "5-Statistical_Tests_Part1/06-Independent_2_sample_ttest_practice.html#review-the-data",
    "title": "2-Sample Independent T-Test Practice",
    "section": "Review The Data",
    "text": "Review The Data\nQuestion: What is the response variable?\nAnswer:\nQuestion: What is the explanatory variable?\nAnswer:\nCreate summary statistics tables of dental costs for each office:\nCreate a side-by-side boxplot for dental costs for each office.\nCheck the normality of each group.\nQuestion: Do the samples from both groups appear to be normally distributed? If not, is it a cause for concern for our statistical inference?"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/06-Independent_2_sample_ttest_practice.html#hypothesis-test-1",
    "href": "5-Statistical_Tests_Part1/06-Independent_2_sample_ttest_practice.html#hypothesis-test-1",
    "title": "2-Sample Independent T-Test Practice",
    "section": "Hypothesis Test",
    "text": "Hypothesis Test\nState your null and alternative hypotheses (replace the question marks with the appropriate symbols):\n\\[H_0:  \\mu_{IF}???\\mu_{R}\\]\n\\[H_a:\\mu_{IF}???\\mu_{R}\\]\nPerform the appropriate analysis:\nQuestion: What is the test statistic?\nAnswer:\nQuestion: What is the P-value?\nAnswer:\nState your conclusion:"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/06-Independent_2_sample_ttest_practice.html#confidence-interval-1",
    "href": "5-Statistical_Tests_Part1/06-Independent_2_sample_ttest_practice.html#confidence-interval-1",
    "title": "2-Sample Independent T-Test Practice",
    "section": "Confidence Interval",
    "text": "Confidence Interval\nCreate a confidence interval for the difference in costs between the IF and Rexburg offices:\nExplain the confidence interval in context of the research question:"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/06-Independent_2_sample_ttest_practice.html#review-the-data-1",
    "href": "5-Statistical_Tests_Part1/06-Independent_2_sample_ttest_practice.html#review-the-data-1",
    "title": "2-Sample Independent T-Test Practice",
    "section": "Review The Data",
    "text": "Review The Data\nQuestion: What is the response variable?\nAnswer:\nQuestion: What is the explanatory variable?\nAnswer:\nCreate summary statistics tables of birth weights for each country:\nCreate a side-by-side boxplot for birth weights for each country:\nCheck the normality of each group.\nQuestion: Do the samples from both groups appear to be normally distributed? If not, is it a cause for concern for our statistical inference?"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/06-Independent_2_sample_ttest_practice.html#hypothesis-test-2",
    "href": "5-Statistical_Tests_Part1/06-Independent_2_sample_ttest_practice.html#hypothesis-test-2",
    "title": "2-Sample Independent T-Test Practice",
    "section": "Hypothesis Test",
    "text": "Hypothesis Test\nState your null and alternative hypotheses (replace the question marks with the appropriate symbols):\n\\[H_0:  \\mu_{A}???\\mu_{IL}\\]\n\\[H_a:\\mu_{A}???\\mu_{IL}\\]\nPerform the appropriate analysis:\nQuestion: What is the test statistic?\nAnswer:\nQuestion: What is the P-value?\nAnswer:\nState your conclusion:"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/06-Independent_2_sample_ttest_practice.html#confidence-interval-2",
    "href": "5-Statistical_Tests_Part1/06-Independent_2_sample_ttest_practice.html#confidence-interval-2",
    "title": "2-Sample Independent T-Test Practice",
    "section": "Confidence Interval",
    "text": "Confidence Interval\nCreate a confidence interval for the average difference in weights between babies born to mothers in Africa and Illinois:\nExplain the confidence interval in context of the research question:"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/08-ANOVA_Practice.html",
    "href": "5-Statistical_Tests_Part1/08-ANOVA_Practice.html",
    "title": "ANOVA Practice",
    "section": "",
    "text": "You are curious to compare life expectancy between female poets, novelists, and non-fiction writers.\nYou take a sample of female authors from each of the three groups to test if the average age at death is different between any of the three types of authors using a level of significance of, \\(\\alpha = 0.05\\).\n\n\n\n\nlibrary(rio)\nlibrary(tidyverse)\nlibrary(mosaic)\nlibrary(car)\n\nwomenpoet &lt;- rio::import(\"https://byuistats.github.io/BYUI_M221_Book/Data/womenpoet.xls\")\n\n\n\n\nCreate a side-by-side boxplot of the age at death of each of the different author styles.\nModify the colors of each of the boxes for each group.\nCreate a summary statistics table for age at death for each author type:\nList the mean and standard deviations of age at death for:\n\nNovelists:\nPoets:\nNon-fiction:\n\n\n\n\nState your null and alternative hypotheses:\nPerform an Analysis of Variance test including checking for the appropriateness of our analysis.\nQuestion: What is the test statistic?\nAnswer:\nQuestion: What are the degrees of freedom for your analysis?\na. Numerator (between Groups) Degrees of Freedom\nb. Denominator (within groups) Degrees of Freedom\nAnswer:\nQuestion: What is the P-value?\nAnswer:\nQuestion: Do you reject the null hypothesis? Why?\nAnswer:\nQuestion: State your conclusion in context of the problem.\nAnswer:\nQuestion: Can we trust the p-value? a. Check for equal standard deviation (is the ratio of the largest SD / smallest SD greater than 2?) b. Check Normality of the residuals (qqPlot())\nAnswer:"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/08-ANOVA_Practice.html#introduction",
    "href": "5-Statistical_Tests_Part1/08-ANOVA_Practice.html#introduction",
    "title": "ANOVA Practice",
    "section": "",
    "text": "You are curious to compare life expectancy between female poets, novelists, and non-fiction writers.\nYou take a sample of female authors from each of the three groups to test if the average age at death is different between any of the three types of authors using a level of significance of, \\(\\alpha = 0.05\\)."
  },
  {
    "objectID": "5-Statistical_Tests_Part1/08-ANOVA_Practice.html#load-the-data-and-libraries",
    "href": "5-Statistical_Tests_Part1/08-ANOVA_Practice.html#load-the-data-and-libraries",
    "title": "ANOVA Practice",
    "section": "",
    "text": "library(rio)\nlibrary(tidyverse)\nlibrary(mosaic)\nlibrary(car)\n\nwomenpoet &lt;- rio::import(\"https://byuistats.github.io/BYUI_M221_Book/Data/womenpoet.xls\")"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/08-ANOVA_Practice.html#explore-the-data",
    "href": "5-Statistical_Tests_Part1/08-ANOVA_Practice.html#explore-the-data",
    "title": "ANOVA Practice",
    "section": "",
    "text": "Create a side-by-side boxplot of the age at death of each of the different author styles.\nModify the colors of each of the boxes for each group.\nCreate a summary statistics table for age at death for each author type:\nList the mean and standard deviations of age at death for:\n\nNovelists:\nPoets:\nNon-fiction:"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/08-ANOVA_Practice.html#perform-the-appropriate-analysis",
    "href": "5-Statistical_Tests_Part1/08-ANOVA_Practice.html#perform-the-appropriate-analysis",
    "title": "ANOVA Practice",
    "section": "",
    "text": "State your null and alternative hypotheses:\nPerform an Analysis of Variance test including checking for the appropriateness of our analysis.\nQuestion: What is the test statistic?\nAnswer:\nQuestion: What are the degrees of freedom for your analysis?\na. Numerator (between Groups) Degrees of Freedom\nb. Denominator (within groups) Degrees of Freedom\nAnswer:\nQuestion: What is the P-value?\nAnswer:\nQuestion: Do you reject the null hypothesis? Why?\nAnswer:\nQuestion: State your conclusion in context of the problem.\nAnswer:\nQuestion: Can we trust the p-value? a. Check for equal standard deviation (is the ratio of the largest SD / smallest SD greater than 2?) b. Check Normality of the residuals (qqPlot())\nAnswer:"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/08-ANOVA_Practice.html#introduction-1",
    "href": "5-Statistical_Tests_Part1/08-ANOVA_Practice.html#introduction-1",
    "title": "ANOVA Practice",
    "section": "Introduction",
    "text": "Introduction\nA study was conducted to determine if different types of material can reduce the amount of mosquito human contact. The researchers evaluated five different types of patches 1=Odomos, 2=Deltamethrin, 3=Cyfluthrin, 4=D+O, 5=C+O.\nThe amount of mosquito human contact was measured to assess any differences between the five different types of material. Use a level of significance of 0.05."
  },
  {
    "objectID": "5-Statistical_Tests_Part1/08-ANOVA_Practice.html#load-the-data",
    "href": "5-Statistical_Tests_Part1/08-ANOVA_Practice.html#load-the-data",
    "title": "ANOVA Practice",
    "section": "Load the Data",
    "text": "Load the Data\n\nMosquitoPatch &lt;- rio::import(\"https://raw.githubusercontent.com/rdcromar/Math221D/main/MosquitoPatch.csv\") %&gt;% mutate(Treatment = factor(Treatment))"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/08-ANOVA_Practice.html#review-the-data",
    "href": "5-Statistical_Tests_Part1/08-ANOVA_Practice.html#review-the-data",
    "title": "ANOVA Practice",
    "section": "Review the Data",
    "text": "Review the Data\nCreate a side-by-side boxplot for human contact for each of the treatment groups.\nAdd a title and change the colors of the boxes.\nCreate a summary statistics table for human contact for each of the treatment groups:\nQuestion: What do you observe?\nAnswer:\nQuestion: What is the maximum standard deviation?\nAnswer:\nQuestion: What is the minimum standard deviation?\nAnswer:"
  },
  {
    "objectID": "5-Statistical_Tests_Part1/08-ANOVA_Practice.html#perform-the-appropriate-analysis-1",
    "href": "5-Statistical_Tests_Part1/08-ANOVA_Practice.html#perform-the-appropriate-analysis-1",
    "title": "ANOVA Practice",
    "section": "Perform the Appropriate Analysis",
    "text": "Perform the Appropriate Analysis\nState your null and alternative hypotheses:\nPerform an Analysis of Variance test including checking for the appropriateness of our analysis.\nQuestion: What is the test statistic (F-value)?\nAnswer:\nQuestion: What are the degrees of freedom for your analysis?\n\nNumerator (between Groups) Degrees of Freedom\n\nDenominator (within groups) Degrees of Freedom\nAnswer:\n\nQuestion: What is the P-value?\nAnswer:\nQuestion: Do you reject the null hypothesis? Why?\nAnswer:\nQuestion: State your conclusion in context of the problem.\nAnswer:\nQuestion: Can we trust the p-value? a. Check for equal standard deviation (is the ratio of the largest SD / smallest SD greater than 2?) b. Check Normality of the residuals (qqPlot())\nAnswer:"
  },
  {
    "objectID": "6-Statistical_Tests_Part2/01-Bivariate_Data_Intro.html",
    "href": "6-Statistical_Tests_Part2/01-Bivariate_Data_Intro.html",
    "title": "Introducing the Bivariate Data",
    "section": "",
    "text": "Bivariate data refers to situations where you have one quantitative response variable and one quantitative explanatory variable. This requires a different approach to analysis and visualization.\nBy the end of this lesson, you should be able to:\n\nCreate scatterplots for 2 quantitative variables in base R and GGPlot\n\nDescribe what the correlation coefficient, \\(r\\), quantifies\n\nCalculate \\(r\\) using the cor() function\n\nTwo datasets will be used to illustrate these concepts. The first contains self-reported confidence in mathematics and test scores. The second contains eruption duration and time between eruptions of Old Faithful geyser in Yellowstone National Park.\n\n# Load the libraries and data\nlibrary(tidyverse)\nlibrary(mosaic)\nlibrary(rio)\n\ngeyser &lt;- import('https://byuistats.github.io/BYUI_M221_Book/Data/OldFaithful.xlsx')\nnames(geyser)\n\n[1] \"Duration\" \"Wait\"     \"Source\"  \n\nmath &lt;- import('https://byuistats.github.io/BYUI_M221_Book/Data/MathSelfEfficacy.xlsx')\nnames(math)\n\n[1] \"Gender\"               \"Score\"                \"ConfidenceRatingMean\"\n[4] \"Comments\""
  },
  {
    "objectID": "6-Statistical_Tests_Part2/01-Bivariate_Data_Intro.html#scatter-plot",
    "href": "6-Statistical_Tests_Part2/01-Bivariate_Data_Intro.html#scatter-plot",
    "title": "Introducing the Bivariate Data",
    "section": "Scatter plot",
    "text": "Scatter plot\nMake a scatter plot showing the relationship between students’ self reported confidence rating and test score.\nQuestion: Which variable is the Explanatory variable, \\(x\\)?\nAnswer:\nQuestion: Which is the Response variable, \\(y\\)?\nAnswer:\n\n# Base R\nplot(Score ~ ConfidenceRatingMean, data = math)\n\n\n\n\n\n\n\n# ggplot\nggplot(math, aes(x = ConfidenceRatingMean, y = Score )) +\n  geom_point(color = \"darkblue\") +\n  theme_bw() +\n  labs(\n    title = \"Relationship between Student Confidence Rating in Math and Test Score\"\n  ) \n\n\n\n\n\n\n\n\nQuestion: Before calculating the Correlation Coefficient, r, describe in words the direction and strength of the relationship.\nAnswer:\nQuestion: What’s your best guess at, \\(r\\) based on the scatterplot?\nAnswer:\nQuestion: Does it look linear?\nAnswer:\nCalculate the Correlation Coefficient, \\(r\\):\n\ncor(Score ~ ConfidenceRatingMean, data = math)\n\n[1] 0.7278648\n\n\nQuestion: How far off was your guess?\nAnswer:"
  },
  {
    "objectID": "6-Statistical_Tests_Part2/01-Bivariate_Data_Intro.html#scatter-plot-1",
    "href": "6-Statistical_Tests_Part2/01-Bivariate_Data_Intro.html#scatter-plot-1",
    "title": "Introducing the Bivariate Data",
    "section": "Scatter plot",
    "text": "Scatter plot\nMake a scatter plot showing the relationship between wait time and the duration of the next eruption.\nWhich variable is the Explanatory variable? Which is the Response?\nQuestion: Before calculating the Correlation Coefficient, r, describe in words the direction and strength of the relationship.\nAnswer:\nQuestion: What’s your best guess at, \\(r\\) based on the scatterplot?\nAnswer:\nQuestion: Does it look linear?\nAnswer:\nCalculate the Correlation Coefficient, \\(r\\):\nQuestion: How far off was your guess?\nAnswer:"
  },
  {
    "objectID": "6-Statistical_Tests_Part2/03-Regression_Practice.html",
    "href": "6-Statistical_Tests_Part2/03-Regression_Practice.html",
    "title": "Regression Practice",
    "section": "",
    "text": "In this assignment, you will practice regression analysis including:\n\nPlotting bivariate data with a regression line\nCalculating and interpreting the correlation coefficient, r\nFitting a linear regression analysis\nVerifying if a linear model is model is adequate:\n\nChecking for linearity (scatterplot)\nChecking for constant variance (plot(lm_output, which=1))\nChecking for normality of residuals (qqPlot(lm_output$residuals))\n\n\n\nlibrary(tidyverse)\nlibrary(mosaic)\nlibrary(rio)\nlibrary(car)"
  },
  {
    "objectID": "6-Statistical_Tests_Part2/03-Regression_Practice.html#plot-the-data-and-calculate-r",
    "href": "6-Statistical_Tests_Part2/03-Regression_Practice.html#plot-the-data-and-calculate-r",
    "title": "Regression Practice",
    "section": "Plot the Data and calculate r",
    "text": "Plot the Data and calculate r\nDoes the relationship look linear?\nWhat is the correlation coefficient, r?\nWhat does this r show?"
  },
  {
    "objectID": "6-Statistical_Tests_Part2/03-Regression_Practice.html#fit-a-linear-regression-model",
    "href": "6-Statistical_Tests_Part2/03-Regression_Practice.html#fit-a-linear-regression-model",
    "title": "Regression Practice",
    "section": "Fit a Linear Regression Model",
    "text": "Fit a Linear Regression Model\n\n#lm_output &lt;- lm()\n\nAdd the regression line to your chart:\n\n# Sometimes you have to run the whole chunk with plot() and the abline() together:\n\n#plot()\n#abline(lm_output$coefficients)\n\nWhat is the slope of the regression line, and what does it mean?\nWhat is the intercept and what does it mean?\nWhat is your p-value?\nWhat is your conclusion?\nWhat is the confidence interval for the slope?\nInterpret the confidence interval."
  },
  {
    "objectID": "6-Statistical_Tests_Part2/03-Regression_Practice.html#check-model-requirements",
    "href": "6-Statistical_Tests_Part2/03-Regression_Practice.html#check-model-requirements",
    "title": "Regression Practice",
    "section": "Check Model Requirements",
    "text": "Check Model Requirements\nCheck the normality of the residuals:\nCheck for constant variance (Residual by Predicted plot):\n\n#plot(lm_output, which = 1)\n\nLastly, the car you’re interested in buying has around 100,000 miles and costs $11,200. Could this be considered a good deal? Why?"
  },
  {
    "objectID": "6-Statistical_Tests_Part2/03-Regression_Practice.html#plot-the-data-and-calculate-r-1",
    "href": "6-Statistical_Tests_Part2/03-Regression_Practice.html#plot-the-data-and-calculate-r-1",
    "title": "Regression Practice",
    "section": "Plot the Data and calculate r",
    "text": "Plot the Data and calculate r\nDoes the relationship look linear?\nWhat is the correlation coefficient, r?\nWhat does this r show?"
  },
  {
    "objectID": "6-Statistical_Tests_Part2/03-Regression_Practice.html#fit-a-linear-regression-model-1",
    "href": "6-Statistical_Tests_Part2/03-Regression_Practice.html#fit-a-linear-regression-model-1",
    "title": "Regression Practice",
    "section": "Fit a Linear Regression Model",
    "text": "Fit a Linear Regression Model\nAdd the regression line to your chart:\n\n# Sometimes you have to do the plot() and the abline() in one chunk and run the whole thing:\n\nWhat is the slope of the regression line, and what does it mean?\nWhat is the intercept and what does it mean?\nWhat is your p-value?\nWhat is your conclusion?\nWhat is the confidence interval for the slope?\nInterpret the confidence interval:"
  },
  {
    "objectID": "6-Statistical_Tests_Part2/03-Regression_Practice.html#check-model-requirements-1",
    "href": "6-Statistical_Tests_Part2/03-Regression_Practice.html#check-model-requirements-1",
    "title": "Regression Practice",
    "section": "Check Model Requirements",
    "text": "Check Model Requirements\nCheck the normality of the residuals:\nCheck for constant variance (Residual by Predicted plot):"
  },
  {
    "objectID": "6-Statistical_Tests_Part2/03-Regression_Practice.html#plot-the-data-and-calculate-r-2",
    "href": "6-Statistical_Tests_Part2/03-Regression_Practice.html#plot-the-data-and-calculate-r-2",
    "title": "Regression Practice",
    "section": "Plot the Data and calculate r",
    "text": "Plot the Data and calculate r\nDoes the relationship look linear?\nWhat is the correlation coefficient, r?\nWhat does this r show?"
  },
  {
    "objectID": "6-Statistical_Tests_Part2/03-Regression_Practice.html#fit-a-linear-regression-model-2",
    "href": "6-Statistical_Tests_Part2/03-Regression_Practice.html#fit-a-linear-regression-model-2",
    "title": "Regression Practice",
    "section": "Fit a Linear Regression Model",
    "text": "Fit a Linear Regression Model\nAdd the regression line to your chart:\n\n# Sometimes you have to do the plot() and the abline() in one chunk and run the whole thing:\n\nWhat is the slope of the regression line, and what does it mean?\nWhat is the intercept and what does it mean?\nWhat is your p-value?\nWhat is your conclusion?\nWhat is the confidence interval for the slope?\nInterpret the confidence interval:"
  },
  {
    "objectID": "6-Statistical_Tests_Part2/03-Regression_Practice.html#check-model-requirements-2",
    "href": "6-Statistical_Tests_Part2/03-Regression_Practice.html#check-model-requirements-2",
    "title": "Regression Practice",
    "section": "Check Model Requirements",
    "text": "Check Model Requirements\nCheck the normality of the residuals:\nCheck for constant variance (Residual by Predicted plot):"
  },
  {
    "objectID": "6-Statistical_Tests_Part2/06-Distribution_of_Phat_Practice.html",
    "href": "6-Statistical_Tests_Part2/06-Distribution_of_Phat_Practice.html",
    "title": "Distribution of P-hat Practice",
    "section": "",
    "text": "Instructions\nComplete the following questions about the sampling distribution of \\(\\hat{p}\\). When completed, Render the qmd file and submit the html.\n\n\nQuestions\nQuestion: When can we use the normal distribution to approximate the sampling distribution of \\(\\hat{p}\\)?\nAnswer:\nQuestion: What is the mean of the sampling distribution of \\(\\hat{p}\\)?\nAnswer:\nQuestion: What is the standard deviation of the sampling distribution of \\(\\hat{p}\\)?\nAnswer:\nSuppose the true population proportion, \\(p\\), of people who support a candidate for office is 52%. We would like to learn something about a sample proportion, \\(\\hat{p}\\), with a sample size \\(n=1000\\) suggesting that the candidate will lose the election (\\(\\hat{p}&lt;0.50\\)).\nUse the following R code to answer the questions below:\n\nphat &lt;- \np &lt;-\nn &lt;- \n\nsigma_phat &lt;- sqrt(p*(1-p)/n)  \n\nError: object 'p' not found\n\nz &lt;- (phat-p) / sigma_phat\n\nError: object 'phat' not found\n\n# Left Tail:\npnorm()\n\nError in pnorm(): argument \"q\" is missing, with no default\n\n# Right tail\n1-pnorm()\n\nError in pnorm(): argument \"q\" is missing, with no default\n\n\nQuestion: What is the \\(z\\)-score associated with \\(\\hat{p}&lt;0.5\\)?\nAnswer:\nQuestion: What is the standard deviation of \\(\\hat{p}\\)?\nAnswer:\nQuestion: What is the probability of a sample of size \\(n=1000\\) suggesting that the candidate will lose even if the true population support, \\(p=0.52\\)?\nAnswer:"
  },
  {
    "objectID": "6-Statistical_Tests_Part2/08-One_Sample_Proportion_Practice.html",
    "href": "6-Statistical_Tests_Part2/08-One_Sample_Proportion_Practice.html",
    "title": "One-sample Proportion Practice",
    "section": "",
    "text": "Recall that we can use the normal distribution to approximate the distribution of \\(\\hat{p}\\) under certain conditions.\nQuestion: What to we need to check before we assume normality for a hypothesis test?\nAnswer:\nQuestion: What to we need to check before we assume normality for confidence intervals?\nAnswer:"
  },
  {
    "objectID": "6-Statistical_Tests_Part2/08-One_Sample_Proportion_Practice.html#hypothesis-test",
    "href": "6-Statistical_Tests_Part2/08-One_Sample_Proportion_Practice.html#hypothesis-test",
    "title": "One-sample Proportion Practice",
    "section": "Hypothesis Test",
    "text": "Hypothesis Test\nState the null and alternative hypotheses (replace the ??? with the correct information):\n\\[H_0: p=???\\]\n\\[H_a: p???0.15\\] Choose your confidence level:\n\\[\\alpha = 0.\\]\nQuestion: What is the value of the test statistic?\nAnswer:\nQuestion: What is the P-value?\nAnswer:\nQuestion: Based on your selected \\(\\alpha\\) and P-value, what is your conclusion?\nAnswer:"
  },
  {
    "objectID": "6-Statistical_Tests_Part2/08-One_Sample_Proportion_Practice.html#confidence-interval",
    "href": "6-Statistical_Tests_Part2/08-One_Sample_Proportion_Practice.html#confidence-interval",
    "title": "One-sample Proportion Practice",
    "section": "Confidence Interval",
    "text": "Confidence Interval\nCreate a 99% confidence interval for the true population proportion of teenagers who smoke?"
  },
  {
    "objectID": "6-Statistical_Tests_Part2/08-One_Sample_Proportion_Practice.html#perscription-use",
    "href": "6-Statistical_Tests_Part2/08-One_Sample_Proportion_Practice.html#perscription-use",
    "title": "One-sample Proportion Practice",
    "section": "Perscription Use",
    "text": "Perscription Use\nA survey of doctors is planned to see what percentage prescribe a certain medication. Find the sample size required to achieve a 2% margin of error if the confidence level is 95%. Assume there are no prior estimates for p.\nQuestion: How many doctors would we need to survey?\nAnswer:"
  },
  {
    "objectID": "6-Statistical_Tests_Part2/08-One_Sample_Proportion_Practice.html#proportion-of-left-handed-artists",
    "href": "6-Statistical_Tests_Part2/08-One_Sample_Proportion_Practice.html#proportion-of-left-handed-artists",
    "title": "One-sample Proportion Practice",
    "section": "Proportion of Left-handed Artists",
    "text": "Proportion of Left-handed Artists\nSuppose you would like to study creativity of left-handed people. Your favorite generative AI told you that the estimated population of left-handed people in the United States is 11%. We would like to have a Margin of Error less \\(\\pm3\\%\\) (0.03) with 90% confidence.\nQuestion: How many artists would you have to survey?\nAnswer:"
  },
  {
    "objectID": "6-Statistical_Tests_Part2/10-Two_Sample_Proportion_Practice.html",
    "href": "6-Statistical_Tests_Part2/10-Two_Sample_Proportion_Practice.html",
    "title": "2-Sample Proportion Practice",
    "section": "",
    "text": "Complete the following questions about testing differences between 2 proportions. When completed, Render the qmd file and submit the html."
  },
  {
    "objectID": "6-Statistical_Tests_Part2/10-Two_Sample_Proportion_Practice.html#confidence-interval",
    "href": "6-Statistical_Tests_Part2/10-Two_Sample_Proportion_Practice.html#confidence-interval",
    "title": "2-Sample Proportion Practice",
    "section": "Confidence Interval",
    "text": "Confidence Interval\nCreate a 95% confidence interval for the difference in the proportion of females to males who prefer to keep the penny:\nQuestion: Interpret the confidence interval in context of the question:\nAnswer:"
  },
  {
    "objectID": "6-Statistical_Tests_Part2/10-Two_Sample_Proportion_Practice.html#hypothesis-test",
    "href": "6-Statistical_Tests_Part2/10-Two_Sample_Proportion_Practice.html#hypothesis-test",
    "title": "2-Sample Proportion Practice",
    "section": "Hypothesis Test",
    "text": "Hypothesis Test\nYou also want to see if there is a difference between the proportion of women who want to keep the penny and the proportion of men who want to keep the penny. Use a level of significance of \\(\\alpha = 0.05\\).\nState your null and alternative hypotheses:\n\\[H_0: p_{female}???p_{male}\\]\n\\[H_a: p_{female}???p_{male}\\]\nQuestion: What is the test statistic?\nAnswer:\nQuestion: What is the P-value?\nAnswer:\nQuestion: Based on your decision rule, state your conclusion in context of the problem:\nAnswer:"
  },
  {
    "objectID": "6-Statistical_Tests_Part2/10-Two_Sample_Proportion_Practice.html#hypothesis-test-1",
    "href": "6-Statistical_Tests_Part2/10-Two_Sample_Proportion_Practice.html#hypothesis-test-1",
    "title": "2-Sample Proportion Practice",
    "section": "Hypothesis Test",
    "text": "Hypothesis Test\nConstruct a null and alternative hypothesis for the study:\n\\[H_0: \\]\n\\[H_a: \\]\nPerform the appropriate analysis:\nQuestion: What is the P-value?\nAnswer:\nQuestion: Based on your decision rule, state your conclusion in context of the problem:\nAnswer:"
  },
  {
    "objectID": "6-Statistical_Tests_Part2/10-Two_Sample_Proportion_Practice.html#confidence-interval-1",
    "href": "6-Statistical_Tests_Part2/10-Two_Sample_Proportion_Practice.html#confidence-interval-1",
    "title": "2-Sample Proportion Practice",
    "section": "Confidence Interval",
    "text": "Confidence Interval\nCreate a 95% confidence interval for the difference in the proportion of divorces between communicative and non-communicative couples:\nQuestion: Interpret the confidence interval in context of the question:\nAnswer:"
  },
  {
    "objectID": "6-Statistical_Tests_Part2/12-Chi_Square_Practice.html",
    "href": "6-Statistical_Tests_Part2/12-Chi_Square_Practice.html",
    "title": "Chi-Square Practice",
    "section": "",
    "text": "Complete the following questions about testing for independence between 2 categorical variables. When completed, Render the qmd file and submit the html."
  },
  {
    "objectID": "6-Statistical_Tests_Part2/12-Chi_Square_Practice.html#hypothesis-test",
    "href": "6-Statistical_Tests_Part2/12-Chi_Square_Practice.html#hypothesis-test",
    "title": "Chi-Square Practice",
    "section": "Hypothesis Test",
    "text": "Hypothesis Test\nPerform the Chi-square test for independence:\nHINT: The way the data were imported into bp_alcohol, it contains a columns, V1, which is a categorical variable. You only want to include the counts from the table. Run chisq.test() dropping the first column. Also, remember to name the output so you can easily extract information needed to check the requirements.\nQuestion: Are the requirements satisfied for the \\(\\chi\\)-square test for independence?\nAnswer:\nQuestion: How many degrees of freedom does this \\(\\chi\\)-square test have?\nAnswer:\nQuestion: What is the value of the test statistic?\nAnswer:\nQuestion: What is the P-value?\nAnswer:\nState your conclusion in context of thise problem:"
  },
  {
    "objectID": "6-Statistical_Tests_Part2/Introducing_Categorical_Data_Analysis.html",
    "href": "6-Statistical_Tests_Part2/Introducing_Categorical_Data_Analysis.html",
    "title": "Introducing Categorical Data Analysis",
    "section": "",
    "text": "In this lesson, we will:\n\nIntroduce a new dataset that we will use throughout the unit on categorical data analysis\nReview summarizing categorical data numerically and visually\nIntroduce the foundations of statistical inference for categorical data analysis"
  },
  {
    "objectID": "6-Statistical_Tests_Part2/Introducing_Categorical_Data_Analysis.html#digging-into-the-data",
    "href": "6-Statistical_Tests_Part2/Introducing_Categorical_Data_Analysis.html#digging-into-the-data",
    "title": "Introducing Categorical Data Analysis",
    "section": "Digging into the Data",
    "text": "Digging into the Data\n\nLoad the Data and Libraries:\n\nlibrary(tidyverse)\nlibrary(mosaic)\nlibrary(rio)\nlibrary(ggplot2)\n\nsw &lt;- import('https://raw.githubusercontent.com/byuistats/Math221D_Cannon/master/Data/StarWarsData_clean.csv') %&gt;% mutate(Household.Income = factor(Household.Income, levels = c(\"$0 - $24,999\", \"$25,000 - $49,999\", \"$50,000 - $99,999\", \"$100,000 - $149,999\",  \"$150,000+\")))\n\nLook at the data (glimpse(), View()).\nQUESTION: How many respondents are there for the survey?\nANSWER:\n\n\nExplore Columns\nRecall that we can summarize categorical data numerically as counts or percents, and visually as bar charts.\nDig into specific columns:\nQUESTION: What proportion of respondents are fans of Star Wars?\nANSWER:\nUse the prop.table() function to get a table of proportions.\n\nprop.table(table(sw$`Are You a Fan of SW?`))\n\n\n       No       Yes \n0.3381295 0.6618705 \n\n\nQUESTION: What is the Favorability breakdown for R2D2?\nANSWER:\n\nggplot(sw, aes(x=`Favorability_R2 D2`)) +\n  geom_bar()\n\n\n\n\n\n\n\n\nNotice that there are some missing values, and the bars are not in a meaningful order. Let’s clean up the data and sort out the order.\n\nLikert Scale Data\nSurveys will often ask respondents to select a response to a question on a 5 or 7 point scale. For example, “Strongly Disagree” all the way up to “Strongly Agree”, or in the Star Wars survey, “Very Unfavorable” to “Very Favorable”.\nGood visualizations will make sure the bars follow the order that will make it easy for the reader to know if a character is well-liked or despised.\nIn a previous activity, we introduced the idea of a factor() variable in R. Factors make sure that R knows when a categorical variable is a categorical variable even when it is encoded as a number. Factors are also useful for fixing the order of categorical variables.\nOur dataset has many questions about favorability. Let’s first find what the options are. We can use the unique() function to get a list:\n\nunique(sw$`Favorability_Anakin Skywalker`)\n\n[1] \"Very favorably\"                             \n[2] \"Somewhat favorably\"                         \n[3] \"Somewhat unfavorably\"                       \n[4] \"Neither favorably nor unfavorably (neutral)\"\n[5] \"Very unfavorably\"                           \n[6] \"\"                                           \n[7] \"Unfamiliar (N/A)\"                           \n\n\nNotice the blank “” which denotes missing values. I can create a list that has these options in the correct order. You have to decide where to put “Unfamiliar (N/A)”. I will put it at the end.\n\nfavorability_order &lt;- c(\"Very unfavorably\", \"Somewhat unfavorably\", \"Neither favorably nor unfavorably (neutral)\", \"Somewhat favorably\", \"Very favorably\", \"Unfamiliar (N/A)\", \"\")\n\nI can now make a boxplot, specify that my variable is a factor and specify the levels of that factor:\n\nggplot(sw, aes(y=factor(`Favorability_R2 D2`, levels = favorability_order) )) +\n  geom_bar(fill=\"navy\") +\n  theme_bw() +\n  labs(\n    y = \"\",\n    title = \"Favorability Rating of R2D2 \"\n  )\n\n\n\n\n\n\n\n\nI can reuse favorability_order for any other favorability survey question:\n\nggplot(sw, aes(y=factor(`Favorability_Boba Fett` , levels = favorability_order) )) +\n  geom_bar(fill=\"navy\") +\n  theme_bw() +\n  labs(\n    y = \"\",\n    title = \"Favorability Rating of Boba Fett \"\n  )\n\n\n\n\n\n\n\n\n\n\n\nEducation\nWhat is the Education distribution of respondents?\n\nunique(sw$Education)\n\n[1] \"High school degree\"               \"Bachelor degree\"                 \n[3] \"Some college or Associate degree\" \"Graduate degree\"                 \n[5] \"\"                                 \"Less than high school degree\"    \n\nggplot(sw, aes(y = factor(Education, levels = c(\"Less than high school degree\", \"High school degree\", \"Some college or Associate degree\", \"Bachelor degree\", \"Graduate degree\", \"\")) )) +\n  geom_bar(fill=\"navy\") +\n  theme_bw() +\n  labs(\n    y = \"\",\n    title = \"Education Breakdown\"\n  )"
  },
  {
    "objectID": "6-Statistical_Tests_Part2/Introducing_Categorical_Data_Analysis.html#variables",
    "href": "6-Statistical_Tests_Part2/Introducing_Categorical_Data_Analysis.html#variables",
    "title": "Introducing Categorical Data Analysis",
    "section": "2 Variables",
    "text": "2 Variables\nWe can also look at 2 categorical variables simultaneously in a grouped bar chart. How you group the bars will depend on the story you want to tell with the data.\nLet’s look at the relationship between Gender and Are You a Fan of SW? grouping in different ways:\n\n# Remove Missing Values\ngender_fan &lt;- sw %&gt;%\n  filter(\n    Gender != \"\",\n    `Are You a Fan of SW?` != \"\"\n  )\n\n\nggplot(gender_fan, aes(fill =  Gender, x=`Are You a Fan of SW?`)) +\n  geom_bar(position=\"dodge\") +\n  theme_bw() +\n  labs(\n    title = \"Grouped by Fan\"\n  )\n\n\n\n\n\n\n\nggplot(gender_fan, aes(x =  Gender, fill=`Are You a Fan of SW?`)) +\n  geom_bar(position=\"dodge\") +\n  theme_bw() +\n  labs(\n    title = \"Grouped by Gender\"\n  )\n\n\n\n\n\n\n\n\nOur brains will naturally want to make the comparison of the bars that are nearest to each other. If you want the reader to compare Fan status for each gender, then grouping by Gender would be the most appropriate."
  },
  {
    "objectID": "6-Statistical_Tests_Part2/Introducing_Categorical_Data_Analysis.html#your-turn",
    "href": "6-Statistical_Tests_Part2/Introducing_Categorical_Data_Analysis.html#your-turn",
    "title": "Introducing Categorical Data Analysis",
    "section": "Your Turn",
    "text": "Your Turn\nPick a few other variables to summarize from the sw dataset individually.\n\nPick 2 Variables\nPick 2 categorical variables and look at the bar chart for both. Decide on the most important comparison and make sure your graph groups by that variable."
  },
  {
    "objectID": "6-Statistical_Tests_Part2/Introducing_Categorical_Data_Analysis.html#approximately-normal",
    "href": "6-Statistical_Tests_Part2/Introducing_Categorical_Data_Analysis.html#approximately-normal",
    "title": "Introducing Categorical Data Analysis",
    "section": "Approximately Normal",
    "text": "Approximately Normal\nAs you probably guessed, the distribution of \\(\\hat{p}\\) is approximately normal with:\n\\[\\mu_{\\hat{p}} = p\\] \\[\\sigma_{\\hat{p}} = \\sqrt{\\frac{p(1-p)}{N}}\\]\nJust as it was with a sample mean, we have to check certain conditions to assume the distribution is approximately normal.\nFor \\(\\bar{x}\\), we needed 1. The population to be normally distributed 2. The sample size greater than 30.\nThe principle of having a large enough sample for a proportion applies, but we define “large enough sample size” a little differently.\nWe can assume the distribution of \\(\\hat{p}\\) is approximately normal if:\n\\[np \\geq 10\\] \\[n(1-p) \\geq 10\\]\nIn plain English, this means our sample size has to be big enough to have at least 10 “successes” and 10 “failures”. For example, if we’re estimating the proportion of left handed people, we would need a sample size large enough to have at least 10 left handed people and 10 right handed people.\nIf the distribution of sample means is approximately normal according to the conditions above, we can calculate a z-score as we did in Unit 1 and 2:\n\\[z = \\frac{\\hat{p}-p}{\\sqrt{\\frac{p(1-p)}{N}}}\\]\nIn the next chapter, we will discuss how to use the prop.test() function, much like the t.test() function to create confidence intervals and perform hypothesis tests based on the assumption that \\(\\hat{p}\\) is normally distributed."
  },
  {
    "objectID": "6-Statistical_Tests_Part2/Introducing_Categorical_Data_Analysis.html#footnotes",
    "href": "6-Statistical_Tests_Part2/Introducing_Categorical_Data_Analysis.html#footnotes",
    "title": "Introducing Categorical Data Analysis",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFor the definitive answer to the question, see Who Shot First↩︎"
  },
  {
    "objectID": "6-Statistical_Tests_Part2/One_and_Two_Sample_Proportions.html",
    "href": "6-Statistical_Tests_Part2/One_and_Two_Sample_Proportions.html",
    "title": "One and Two Sample Proportion Tests",
    "section": "",
    "text": "In this section, we will learn:\n\nHow to set up and analyze hypothesis tests for 1-sample proportions\nHow to create confidence intervals for 1-sample proportions\nHow to set up and analyze hypothesis tests for 2-sample proportions\nHow to create confidence intervals for the difference between 2-sample proportions"
  },
  {
    "objectID": "6-Statistical_Tests_Part2/One_and_Two_Sample_Proportions.html#hypothesis-tests",
    "href": "6-Statistical_Tests_Part2/One_and_Two_Sample_Proportions.html#hypothesis-tests",
    "title": "One and Two Sample Proportion Tests",
    "section": "Hypothesis Tests",
    "text": "Hypothesis Tests\nBy now, the framework of hypothesis testing should start to feel familiar.\n\nSet up a Null and alternative hypothesis in terms of a population parameter\nUse data to calculate a test statistic\nCalculate a P-value (the probability of getting a test statistic as extreme or more extreme than the one I observed if the null hypothesis were true)\nCompare P-value to \\(\\alpha\\)\nState your conclusion in context of the problem\n\nThe process is no different. We want to use data to make a conclusion about the unknown population proportion, \\(p\\).\n\nLeft Handedness Among Visual Arts Majors\nThere is a belief in a link between creativity and left-handedness. Left-handed people make up about 10% of the population. Let’s test to see if BYU-I Visual Arts majors have a higher proportion of left-handed people than the general population.\nDefine our null and alternative hypotheses:\n\\[H_0:  p = 0.10\\] \\[H_a: p &gt; 0.10\\] \\[\\alpha = 0.05\\]\nNote: In stats we typically use Greek letters to designate population parameters. For proportions we us the Latin letter, \\(p\\), because \\(\\pi\\) was already taken.\nWe collect a random sample of 77 Visual Arts majors and find that 10 of them are left handed. The sample proportion is:\n\\[\\hat{p} = \\frac{X}{N} = \\frac{10}{77} = 0.1298701\\]\nEven though our sample proportion is bigger than 0.10, we understand sampling variability and want to see if it is statistically significantly higher than 0.10.\nThe prop.test() function behaves very much like the t.test() function. We have to input \\(X\\), \\(N\\), the null hypothesized \\(p\\), and the alternative hypothesis:\n\nprop.test(x = 10, n = 77, p=0.10, alternative=\"greater\")\n\n\n    1-sample proportions test with continuity correction\n\ndata:  10 out of 77, null probability 0.1\nX-squared = 0.46753, df = 1, p-value = 0.2471\nalternative hypothesis: true p is greater than 0.1\n95 percent confidence interval:\n 0.07423612 1.00000000\nsample estimates:\n        p \n0.1298701 \n\n\nConclusion: Because the p-value, 0.2471, is greater than \\(\\alpha=0.05\\), we fail to reject the null hypothesis. We have insufficient evidence to suggest that BYU-I Visual Arts majors have a higher proportion of left-handed people than the general population.\n\nChecking Requirements\nRecall that we must check that we have a big enough sample size to trust our p-value. To do this, we check that there are more than 10 expected number of success and failures for a given sample size:\n\\[np = 77*0.10=7.7\\]\n\\[n(1-p) = 77*0.9 = 69.3\\] QUESTION: Are both \\(np\\) and \\(n(1-p)\\) greater than 10?\nANSWER:\nIt looks like we don’t have enough data to assume that the distribution of \\(\\hat{p}\\) is normal. Our p-value may not be appropriate."
  },
  {
    "objectID": "6-Statistical_Tests_Part2/One_and_Two_Sample_Proportions.html#confidence-intervals",
    "href": "6-Statistical_Tests_Part2/One_and_Two_Sample_Proportions.html#confidence-intervals",
    "title": "One and Two Sample Proportion Tests",
    "section": "Confidence Intervals",
    "text": "Confidence Intervals\nWe can use prop.test() to calculate confidence intervals as well.\nRecall: Confidence intervals do not depend on null and alternative hypotheses so we omit that information in the prop.test() function.\n\nprop.test(x=10, n = 77)$conf.int\n\n[1] 0.06738384 0.23042038\nattr(,\"conf.level\")\n[1] 0.95\n\n\nExplanation: I am 95% confident that the true proportion of left-handed Visual Arts majors is between 0.067 and 0.230.\n\nYour Turn: Trump Support in Idaho\nIn the 2024 presidential election, Donald Trump won 67% of the vote in Idaho. After months in office, you would like to see if support for Trump has decreased from his share of the vote. You sample 772 registered voters and get 566 responses. Of the 566 responses, 364 say they approve of President Trump.\nPerform a hypothesis test that tests if the presidential approval is less than his share of the vote:\n\\[H_0: p = \\]\n\\[H_a: p \\]\n\\[\\alpha = \\]\nQuestion: What is the value of the test statistics for this test?\nAnswer:\nQuestion: What is the P-Value?\nAnswer:\nQuestion: State your conclusion in context of this problem:\nAnswer:\nQUESTION: Can we trust the P-value? (Check \\(np\\) and \\(n(1-p)\\))\nANSWER:\nCreate a \\((1-\\alpha)\\) Confidence Interval for Idaho’s Presidential Approval:\nQUESTION: Explain your confidence interval in context of the research question:\nANSWER:"
  },
  {
    "objectID": "6-Statistical_Tests_Part2/One_and_Two_Sample_Proportions.html#example-1-voting-behaviour-by-gender",
    "href": "6-Statistical_Tests_Part2/One_and_Two_Sample_Proportions.html#example-1-voting-behaviour-by-gender",
    "title": "One and Two Sample Proportion Tests",
    "section": "Example 1: Voting Behaviour by Gender",
    "text": "Example 1: Voting Behaviour by Gender\nSuppose we want to test if women are more likely to identify as Democrat than men. We sample 250 men and 250 women and measure their political affiliation. We find that 80 men identify as Democrat and 102 females identify as Democrat.\nJust as with the two-sample t-test for means, we must define a reference group. In this example, we will use females as the reference group so that our alternative will be relative to that group.\n\\[H_0: p_{femaleDem} = p_{maleDem}\\] \\[H_a: p_{femaleDem} &gt; p_{maleDem}\\] We will use \\(\\alpha = 0.05\\)\n\nprop.test(x = c(102, 80), n = c(250, 250), alternative = \"greater\")\n\n\n    2-sample test for equality of proportions with continuity correction\n\ndata:  c(102, 80) out of c(250, 250)\nX-squared = 3.8099, df = 1, p-value = 0.02548\nalternative hypothesis: greater\n95 percent confidence interval:\n 0.01350993 1.00000000\nsample estimates:\nprop 1 prop 2 \n 0.408  0.320 \n\n\nWe can also create a confidence interval for the difference:\n\nprop.test(x = c(102, 80), n = c(250, 250))$conf.int\n\n[1] 5.904086e-06 1.759941e-01\nattr(,\"conf.level\")\n[1] 0.95\n\n\nConfidence intervals for differences can be positive and negative. In this example, a negative number would indicate that Females are less likely to be Democrat and a positive number means they are more likely to be Democrat.\nOur confidence interval is just above zero on the lower end. We are 95% confident that females are between 0.000% and 17.6% more likely to be Democrat than men.\n\nTest Requirments\nJust as with 1-sample proportion tests, we must validate that we have a large enough sample size to ensure that \\(\\hat{p}\\) is approximately normally distributed. When we have 2 samples, however, we must check both \\(\\hat{p}\\)’s. For both hypothesis testing and confidence intervals we check:\nRequirements for Hypothesis Testing and Confidence Intervals\n\\[ n_1\\hat{p}_1 \\ge 10\\] \\[n_1(1-\\hat{p}_1) \\ge 10\\] \\[ n_2\\hat{p}_2 \\ge 10\\] \\[n_2(1-\\hat{p}_2) \\ge 10\\]\nAn easy R calculator to check this is:\n\n# All must be true:\n\nx1 &lt;- 102\nn1 &lt;- 250\nphat1 &lt;- x1/n1\n\nn1*phat1 &gt;= 10\n\n[1] TRUE\n\nn1*(1-phat1) &gt;=10\n\n[1] TRUE\n\nx2 &lt;- 80\nn2 &lt;- 250\nphat2 &lt;- x2 / n2\n\nn2*phat2 &gt;= 10\n\n[1] TRUE\n\nn2*(1-phat2) &gt;=10\n\n[1] TRUE"
  },
  {
    "objectID": "6-Statistical_Tests_Part2/One_and_Two_Sample_Proportions.html#soccer-popularity-on-the-rise",
    "href": "6-Statistical_Tests_Part2/One_and_Two_Sample_Proportions.html#soccer-popularity-on-the-rise",
    "title": "One and Two Sample Proportion Tests",
    "section": "Soccer Popularity on the Rise?",
    "text": "Soccer Popularity on the Rise?\nSoccer is becoming much more popular in the United States. We would like to test if this is being driven by demographic shifts in the population where the younger generation is more likely to favor soccer.\nA researcher samples 524 individuals under 40 and 655 individuals older than 40 and asks what their preferred sport is. Of the 524 respondents under 40, 44 identified soccer as their favorite sport. Of the 655 respondents over 40, 27 identified soccer as their favorite sport.\nPerform a 2-sample proportion test to determine if significantly more younger people identify soccer as their favorite sport.\nQUESTION: State your Null and Alternative Hypotheses:\n\\[Ho:  \\]\n\\[Ha:  \\]\n\\[\\alpha = \\]\nQUESTION: Perform the appropriate analysis:\n\nprop.test()\n\nError in prop.test(): argument \"x\" is missing, with no default\n\n\nQUESTION: Are the requirements for the hypothesis test and confidence interval satisfied?\n\n# All must be true:\n\nx1 &lt;- \nn1 &lt;- \nphat1 &lt;- x1/n1\n\nn1*phat1 &gt;= 10\n\n[1] FALSE\n\nn1*(1-phat1) &gt;=10\n\n[1] FALSE\n\nx2 &lt;- \nn2 &lt;- \nphat2 &lt;- x2 / n2\n\nn2*phat2 &gt;= 10\n\n[1] FALSE\n\nn2*(1-phat2) &gt;=10\n\n[1] FALSE\n\n\nANSWER:\nCreate and interpret the confidence interval for the difference in the proportions.\nQUESTION: Explain your confidence interval in context of the research question:\nANSWER:"
  },
  {
    "objectID": "7-Semester_Project/Importing_Data.html",
    "href": "7-Semester_Project/Importing_Data.html",
    "title": "Downloading Data",
    "section": "",
    "text": "So far in class, we have encountered only fairly clean data examples carefully managed and stored in an easy to access location. We have been able to use the import() function from the rio library with a link to a well-contained data resource.\nWhen we encounter data in the wild, it can be much more complicated to extract and clean up. However, good research should be transparent, with data sources cited and, where possible, published. We can often find links to raw data for graphs.\nIn these notes we will see how we can use tools we’ve already used to import data directly from the internet (or a saved file on your computer) and learn some new skills about how to clean up the data for specific needs."
  },
  {
    "objectID": "7-Semester_Project/Importing_Data.html#mac-instructions",
    "href": "7-Semester_Project/Importing_Data.html#mac-instructions",
    "title": "Downloading Data",
    "section": "Mac Instructions:",
    "text": "Mac Instructions:\n\nOpen Finder, which is the smiley face icon located on the bottom of your screen.\nNavigate to the folder where your file is located by clicking through the folders.\nOnce you’ve found your file, right-click on it (or hold down the Control key while clicking), and select “Get Info”.\nIn the window that pops up, you’ll see a field called “Where:”, which shows you the file path. It looks something like /Users/YourUsername/Documents/YourFile.csv. You can copy this path by selecting it and pressing Command + C."
  },
  {
    "objectID": "7-Semester_Project/Importing_Data.html#pc-instructions",
    "href": "7-Semester_Project/Importing_Data.html#pc-instructions",
    "title": "Downloading Data",
    "section": "PC Instructions:",
    "text": "PC Instructions:\n\nOpen File Explorer, which is usually the folder icon located on your taskbar or in the Start menu.\nNavigate to the folder where your file is located by clicking through the folders.\nOnce you’ve found your file, right-click on it and select “Properties”.\nIn the Properties window, you’ll see a field called “Location” or “Location:” which shows you the file path. It looks something like C:\\Users\\YourUsername\\Documents\\YourFile.csv. You can copy this path by selecting it and pressing Ctrl + C.\n\nIn both cases, the file path tells the computer where to find your file, similar to how a street address tells someone where to find a physical location. You can use this file path to access your file from anywhere on your computer.\nUnfortunately, R doesn’t like single backslashes, \\. If you’re on a PC you can either add a second \\ or switch them to /\n\nlibrary(tidyverse)\nlibrary(mosaic)\nlibrary(rio)\nlibrary(ggplot2)\n\nwar &lt;- import('C:\\\\Users\\\\paulccannon\\\\OneDrive - BYU-Idaho\\\\Math221inR\\\\countries-in-conflict-data.csv')\nwar &lt;- import('C:/Users/paulccannon/OneDrive - BYU-Idaho/Math221inR/countries-in-conflict-data.csv')\n\nThe rio::import() function can handle many different types data types (.xlsx, .xls, .csv, .txt, and many others). If you run into a data file type that rio can’t import, there are other libraries that have similar functions that can do the same thing. But we limit the scope for this class to sources that rio can handle."
  },
  {
    "objectID": "7-Semester_Project/Semester_Project_Template.html",
    "href": "7-Semester_Project/Semester_Project_Template.html",
    "title": "Semester Project",
    "section": "",
    "text": "Introduction\n\n\nDesign the Study\n\n\nCollect the Data\n\n\nDescribe/Summarize the Data\n\nlibrary(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.4.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(mosaic)\n\nWarning: package 'mosaic' was built under R version 4.4.3\n\n\nRegistered S3 method overwritten by 'mosaic':\n  method                           from   \n  fortify.SpatialPolygonsDataFrame ggplot2\n\nThe 'mosaic' package masks several functions from core packages in order to add \nadditional features.  The original behavior of these functions should not be affected by this.\n\nAttaching package: 'mosaic'\n\nThe following object is masked from 'package:Matrix':\n\n    mean\n\nThe following objects are masked from 'package:dplyr':\n\n    count, do, tally\n\nThe following object is masked from 'package:purrr':\n\n    cross\n\nThe following object is masked from 'package:ggplot2':\n\n    stat\n\nThe following objects are masked from 'package:stats':\n\n    binom.test, cor, cor.test, cov, fivenum, IQR, median, prop.test,\n    quantile, sd, t.test, var\n\nThe following objects are masked from 'package:base':\n\n    max, mean, min, prod, range, sample, sum\n\nlibrary(rio)\n\nWarning: package 'rio' was built under R version 4.4.3\n\n\n\nAttaching package: 'rio'\n\nThe following object is masked from 'package:mosaic':\n\n    factorize\n\nlibrary(ggplot2)\n\n# Be sure to make this using ggplot()\n\n\n\nMake Inference\n\n# Perform the appropriate test for the data selected\n\n\n\nConclusion (Take Action)"
  }
]