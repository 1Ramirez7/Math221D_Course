[
  {
    "objectID": "1-Data_Literacy/5-Shape_and_Center.html",
    "href": "1-Data_Literacy/5-Shape_and_Center.html",
    "title": "Describing Quantitative Data (Shape & Center)",
    "section": "",
    "text": "By the end of this lesson, you should be able to:\n\nCreate a histogram from data\nInterpret data presented in a histogram\nIdentify left-skewed, right-skewed, and symmetric distributions from histograms\nCalculate the mean, median, and mode for quantitative data using software\nCompare the centers of distributions using graphical and numerical summaries\nDescribe the effects that skewness or outliers have on the relationship between the mean and median\nDistinguish between a parameter and a statistic"
  },
  {
    "objectID": "1-Data_Literacy/5-Shape_and_Center.html#lesson-outcomes",
    "href": "1-Data_Literacy/5-Shape_and_Center.html#lesson-outcomes",
    "title": "Describing Quantitative Data (Shape & Center)",
    "section": "",
    "text": "By the end of this lesson, you should be able to:\n\nCreate a histogram from data\nInterpret data presented in a histogram\nIdentify left-skewed, right-skewed, and symmetric distributions from histograms\nCalculate the mean, median, and mode for quantitative data using software\nCompare the centers of distributions using graphical and numerical summaries\nDescribe the effects that skewness or outliers have on the relationship between the mean and median\nDistinguish between a parameter and a statistic"
  },
  {
    "objectID": "1-Data_Literacy/5-Shape_and_Center.html#review-of-the-five-steps-of-the-statistical-process",
    "href": "1-Data_Literacy/5-Shape_and_Center.html#review-of-the-five-steps-of-the-statistical-process",
    "title": "Describing Quantitative Data (Shape & Center)",
    "section": "Review of the Five Steps of the Statistical Process",
    "text": "Review of the Five Steps of the Statistical Process\nWe will use the five steps in the Statistical Process throughout the course. Recall the five steps (and the mnemonic “Daniel Can Discern More Truth) before you begin this lesson.\n\n\n\n\n\nStep 1:\n\n\n\n\nDaniel\n\n\n\n\nDesign the study\n\n\n\n\n\n\n\n\nStep 2:\n\n\n\n\nCan\n\n\n\n\nCollect data\n\n\n\n\n\n\nStep 3:\n\n\n\n\nDiscern\n\n\n\n\nDescribe the data\n\n\n\n\n\n\nStep 4:\n\n\n\n\nMore\n\n\n\n\nMake inferences\n\n\n\n\n\n\nStep 5:\n\n\n\n\nTruth\n\n\n\n\nTake action"
  },
  {
    "objectID": "1-Data_Literacy/5-Shape_and_Center.html#shape-of-a-distribution",
    "href": "1-Data_Literacy/5-Shape_and_Center.html#shape-of-a-distribution",
    "title": "Describing Quantitative Data (Shape & Center)",
    "section": "Shape of a Distribution",
    "text": "Shape of a Distribution\nCost to Treat Tuberculosis in India\n\n  Step 1: Design the study.\nTuberculosis (TB) is the deadliest bacterial disease in the world. In 2009, nine million new cases of tuberculosis were diagnosed, leading to almost 2 million deaths worldwide. Currently, the principal vaccine used to prevent tuberculosis is Bacille Calmette Guerin (BCG). Unfortunately, BCG is only moderately effective at preventing tuberculosis. Historically, India has had a high number of tuberculosis cases. The Indian Government wants to reduce the prevalence of this disease.\nIn this activity, you will compare the average costs of treating a person who contracts tuberculosis to the costs of preventing a case of tuberculosis in India.\n  Step 2: Collect data.\nHealth Care records of tuberculosis patients in India were surveyed to estimate the cost to treat patients with tuberculosis. The following data are representative of the total costs (in US dollars) incurred by society in the treatment of 10 randomly selected tuberculosis patients in India.\n\n15,100     19,000     4,800     6,500     14,900     600     23,500     11,500     12,900     32,200\n\nThese costs include health care treatment, time missed from work, and in some cases utility lost due to death.\n  Step 3: Describe the data.\n\nVisualizing Quantitative Data: Histograms\nThe following data are representative of the total costs (in US dollars) incurred by society in the treatment of 10 randomly selected tuberculosis patients in India.\n\n15,100     19,000     4,800     6,500     14,900     600     23,500     11,500     12,900     32,200\n\nTo help us visualize these data, we will create a graph called a histogram. To make a histogram, we will divide the number line from 0 to 35,000 in seven equal parts. We will then count the number of data points in each of these intervals:\n\n\n\n\n\n\nInterval\n\n\n\n\nNumber of Observations\n\n\n\n\n\n\n\n\nAt least 0 and less than 5,000\n\n\n\n\n2\n\n\n\n\n\n\nAt least 5,000 and less than 10,000\n\n\n\n\n1\n\n\n\n\n\n\nAt least 10,000 and less than 15,000\n\n\n\n\n3\n\n\n\n\n\n\nAt least 15,000 and less than 20,000\n\n\n\n\n2\n\n\n\n\n\n\nAt least 20,000 and less than 25,000\n\n\n\n\n1\n\n\n\n\n\n\nAt least 25,000 and less than 30,000\n\n\n\n\n0\n\n\n\n\n\n\nAt least 30,000 and less than 35,000\n\n\n\n\n1\n\n\n\n\n\n\nFor each of these intervals, called bins, we draw a bar on the histogram. The width of the bars is determined by the width of the bin (5000 in this example). The height of the bars is equal to the number of observations that fall in each bin. As we look at the histogram shown below, we see bars ranging from $0 to $35,000. We also see higher bars in the middle between $10,000 to $20,000 show that these values are more commonly occurring than the other values. If we computed the average of the values contained in our histogram, we would compute the number \\[\n  \\frac{15,100 + 19,000 + 4,800 + 6,500 + 14,900 + 600 + 23,500 + 11,500 + 12,900 + 32,200}{10} = 14,100\n\\] showing that the center of the histogram (or average) is at $14,100.\n\nThis is a histogram created in R: \n\n\n\nTo create this histogram in R, you can copy and paste the following code into R:\n\n# Create a dataset with the costs from 10 randomly selected patients:\ndata &lt;- c(15100, 19000, 4800, 6500, 14900, 600, 23500, 11500,12900, 32200)\n\n# Create a histogram. We add x-axis labels using `xlab = \"\"` and a title `main = \"\"`\nhist(data, xlab=\"Treatment Costs\", main = \"Tuberculosis Costs in India\")\n\n\n\n\n\n\n\n\n\n\nMaking Inference About the Population\nAfter summarizing the data from our sample of the populations both numerically and graphically, we can use this information to make inference about the full population. \n  Step 4: Make inferences.\nIn the past, the total average cost to society to treat a case of tuberculosis in India was known to be $13,800. As shown in our Step 3 calculations, the 10 randomly selected patients showed an average cost that was higher than the historic value at $14,100. This might make us believe that the actual total average cost to society is also $14,100. However, in depth statistical calculations (that you will be taught how to do later this semestr) show that there is a 46% chance that our sample had an average of $14,100 just by random chance. This isn’t too hard to believe since we only had a sample size of 10 people, and $14,100 is only $300 above $13,800, so it turns out to be fairly likely (46% chance) that because of random chance our sample had an average that was a little higher than the actual value from the population. So we will conclude that the total average cost to society is still essentially the same as it has been in the past.\n\n\n Step 5: Take action.\nAfter making inferences, you take action. The motivation for conducting a study like this is usually to see if there is inflation in the costs.\n\nAnswer the following question:\n\n\n\nGiven our conclusion in Step 4 (that the results of our random sample being at an average $14,100 had a 46% probability of just being caused by random chance) do you think the Government of India needs to take any special action to stop the increase in the cost to treat tuberculosis?\n\n\n\nShow/Hide Solution\n\n\nAnswers may vary. – However, we could not say that the true mean cost has really changed from $13,800. So, there is not enough evidence of inflation. There is no need for the Government of India to take action.\n\n\n\n\nOne benefit of using a histogram is that it allows you to visualize the distribution of the data. A histogram illustrates the overall shape of the distribution of the data. The height of the bars show how many observations fall in that range.\n\nAnswer the following question:\n\n\n\nWhich bin of the histogram of tuberculosis costs contained the most data points?\n\n\n\nShow/Hide Solution\n\n\nThe bin going from $10,000 to $15,000 contained 3 observations ($11,500, $12,900, and $14,900), which was the most of any of the bins in the histogram. This can be seen visually in the histogram by looking at the height of each bar and the starting and stopping points of the bar along the x-axis of the graph.\n\n\n\n\nWe will describe the shape of the distribution of a data set using the following basic categories: symmetric, bell-shaped, skewed right, and skewed left. Additionally, we can label the shape of a distribution as uniform, unimodal, bimodal, or multimodal.\nA distribution is symmetric if both the left and right side of the distribution appear to be roughly a mirror image of each other. A special symmetric distribution is a bell-shaped distribution. When data follow a bell-shaped distribution, the histogram looks like a bell. Bell-shaped distributions play an important role in Statistics and will play a role in most of the future lessons.\nA distribution is right-skewed if a histogram of the distribution shows a long right tail. This can occur if there are some very large outliers on the right-hand side of the distribution. A distribution is left-skewed if a histogram shows that it has a long tail to the left.\n\n\n\nRight-skewed\n\n\nSymmetric & Bell-shaped\n\n\nLeft-skewed\n\n\n\n\n&lt;img src/Images/Lesson03_shape_wageshist.png” width=130%&gt;\n\n\n&lt;img src/Images/Lesson03_shape_maleheightshist.png” width=130%&gt;\n\n&lt;img src/Images/Lesson03_shape_gpahist.png” width=130%&gt;\n\n\n\n\nMean: $10.45\nMedian: $9.04\nMean is to the right of the median.\n\n\nMean: 71.1 inches\nMedian: 71 inches\nMean and median are roughly equal.\n\n\nMean: 3.42\nMedian: 3.45\nMean is to the left of the median.\n\n\n\n\nIf a distribution has only one peak, it is said to be unimodal. The three distributions illustrated above are all unimodal distributions. Some people might argue that there are several peaks in the GPA data, so it should not be considered unimodal. Even though there are jagged bumps in the histogram, it is important to visualize the overall shape in the data. When interpreting a histogram, it can be helpful to blur your eyes and imagine the overall shape after smoothing out the bumps. If the overall trend indicates that there is more than one bump, then we do not consider the distribution to be unimodal. We will usually only work with unimodal data sets in this course.\nSome distributions have no distinct peak, others have more than one peak. When there is no distinct peak, and the histogram shows a relatively flat shape, we might say the data follow a uniform distribution. If there are two distinct peaks, a distribution is called bimodal. If there are more than two peaks, we refer to the distribution as multimodal."
  },
  {
    "objectID": "1-Data_Literacy/5-Shape_and_Center.html#center-of-a-distribution",
    "href": "1-Data_Literacy/5-Shape_and_Center.html#center-of-a-distribution",
    "title": "Describing Quantitative Data (Shape & Center)",
    "section": "Center of a Distribution",
    "text": "Center of a Distribution\n\nStep 3: Describe the data.\nSometimes people talk about the “typical” BYU-Idaho student or the average waiting time for a bus. But what does it mean for something or someone to be “average?” How can we quantify what it means to be typical or average? In the example below, we will explore one way to define what “average” means.\nWhen we talk about the “typical” or “average” value, we are essentially describing the center of a population. If we want to estimate the “average” costs to treat a tuberculosis patient, there are several ways we can do it.\n\n\nMeasuring the Center of a Distribution\n\nMean\nThe sample mean or sample arithmetic mean is the most common tool to estimate the center of a distribution. It is referred to simply as the mean. It is computed by adding up the observed data and dividing by the number of observations in the data set.\nIn Statistics, important ideas are given a name. Very important ideas are given a symbol. The sample mean has both a name (mean) and a symbol (\\(\\bar x\\), called “x-bar”).\n\\[\n  \\bar{x} \\text{ is used to denote the sample mean}\n\\]\nYou may have heard people refer to the sample mean as the average. Technically, the word average refers to any number that is used to estimate the center of a distribution. The mean, median and mode are all examples of “averages.” To avoid confusion, it is best to use the words mean, median, and mode instead of the word average, so that it is clear which “average” your are referencing.\n\nAnswer the following question:\n\n\n\nPractice finding the mean, \\(\\bar x\\), for the tuberculosis treatment costs of the 10 patients in India by simplifying the following: \\[\\bar x=\\frac{15100 + 19000 + 4800 + 6500 + 14900 + 600 + 23500 + 11500 + 12900 + 32200}{10}=\\]\n\n\n\nShow/Hide Solution\n\n\nThe mean cost to treat the 10 TB patients in India is: \\(\\bar x = \\$14,100\\). To see how to calculate the mean in R, see the “R Instructions” below.\n\n\n\n\n\n\nMedian\nThe median is the middle value in a sorted data set. Half of the observations in the data set are below the median and half are above the median. To find the median, you:\n\nSort the values from smallest to largest\n\nDo one of the following:\n\nIf there are an odd number of values, the median is the middle value in the sorted list.\nIf there are an even number of values, the median is the mean of the two middle values in the sorted list.\n\n\n\n\nAnswer the following questions:\n\n\n\nPractice finding the median of the tuberculosis treatment costs for the 10 patients in India. First, sort the data from smallest to largest.\n\n\n\nShow/Hide Solution\n\n\n600\n4800\n6500\n11500\n12900\n14900\n15100\n19000\n23500\n32200\n\n\n\n\nSince there are an even number of observations (n=10), the median is computed as the mean of the middle two values. Use your answer to the previous question to find the median of the data. What is the median?\n\n\n\nShow/Hide Solution\n\n\n600\n4800\n6500\n11500\n12900\n14900\n15100\n19000\n23500\n32200\n\nThe middle two numbers are 12900 and 14900. The mean of these two numbers is:\n\n\\(\\text{Median } = \\frac{12900 + 14900}{2} = 13900\\)\n\nThe median cost to treat the ten TB patients in India is $13,900.\n\n\n\n\n\nMode\nThe most frequently occurring value is called the mode. Sometimes there is more than one mode. For example, in the data set\n\\[{1,~~2, ~~2, ~~2, ~~3, ~~4, ~~4, ~~5, ~~5, ~~5, ~~6}\\]\nthe modes are 2 and 5. Both of these values occur three times, which is more times than any other value.\nIf no number occurs more than once in the data set, we say that there is no mode. For the data set representing the costs to treat tuberculosis in India, none of the values is repeated. So, there is no mode for these data.\n\nAnswer the following question:\n\n\n\nFor a particular data set, which of the following can occur?\n\n\nThere may be no mode.\nThere may be exactly one mode.\nThere may be several modes.\nOnly A and B can occur.\nA, B, and C can all occur.\n\n\n\nShow/Hide Solution\n\n\nA, B, and C can all occur.\n\n\n\n\n\n\nR Instructions for Mean, Median, and Mode\n\nR Instructions\n\n\nTo calculate most numerical summaries (such as the mean, median, and mode) in Excel, follow these general steps:\n\nOpen R.\nEnter the data using the “assignment operator”, &lt;-, and c() which establishes a “collection” of things, in this case numbers.\n\nCalculate summary statisti\nThen, highlight the data (by clicking on it) to which you want to apply the function. The cell reference range will automatically be added to your formula. Then type a closed parenthesis, “)” and hit enter.\n\nCaculate a Mean\nFor example, to calculate the mean of the sample of tuberculosis patient costs in India:\n\n# Create a dataset called `data` with the costs from 10 randomly selected patients:\ndata &lt;- c(15100, 19000, 4800, 6500, 14900, 600, 23500, 11500,12900, 32200)\n\nmean(data)\n\n[1] 14100\n\n\n\nCalculate a Median\nSimply replace the word “mean” in the formula with the word “median”. Try it with the tuberculosis patient data, you should get the same value that was calculated by hand above.\n\nmedian(data)\n\n[1] 13900\n\n\n\nCalculate a Mode\nR makes it difficult to use the mode because there are fewer situations when it is useful for quantitative data where few values repeat. In the rare occasion we are not interested in a mean or median, we can tabulate the frequency of specific values using the table() function:\n\n# Create a new dataset called data2:\ndata2 &lt;- c(3,4,9,5,2,3,5,4,2,3,1,5,3,1,2,6,2,4,6,2,2,2,9,1,2,7,8)\n\n# The `table()` function counts up all the times specific values show up.  This works for numbers or categories:\ntable(data2)\n\ndata2\n1 2 3 4 5 6 7 8 9 \n3 8 4 3 3 2 1 1 2 \n\n\nThe first row of the table() output is the value being counted. The second row is the frequency of occurrence.\nWhich value is most frequently occurring?\nIf there are lots of occurring values, we can use R to sort() the table output to make it easier to see which is the mode:\n\nsort(table(data2))\n\ndata2\n7 8 6 9 1 4 5 3 2 \n1 1 2 2 3 3 3 4 8 \n\n\nThe rows are the same as before but are sorted in ascending order. We can now easily see that 2 occurred 8 times making 2 the mode.\n\n\n\n\nParameters and Statistics\nWe only have data on the cost to treat ten randomly selected tuberculosis patients. This represents a random sample from the population. The sample obtained by the researchers depends on random chance. If the study was repeated and a new sample of ten patients was randomly drawn from all cases of tuberculosis in India, would we observe the same data values? Certainly not!\nHowever, if we took a second random sample from the population, we would expect the mean of the new sample to be somewhat similar to the mean for our original sample. And if we took a third sample of data, we should expect the mean of this sample to be different than the means of the other two samples. In fact, every sample will give us a different sample mean, but all of these sample means will be fairly similar in value.\nOne of the primary purposes of collecting and analyzing data is to estimate the true mean of a population. Since collecting data on the entire population is usually not feasible, we usually never know what the true mean is. So we estimate the true population mean with the sample mean from a single sample of data from the population.\nThe sample mean is an example of a statistic. A statistic is a number that describes a sample. The true (usually unknown) population mean is an example of a parameter. A parameter is any number that describes a population.\nAn easy way to distinguish between a parameter and a statistic is to note the repetition in the first letters:\n\nPopulation Parameter True (usually unknown) value describing a population\nSample Statistic Estimate of the population parameter obtained from a sample\n\nIn the example above, the sample mean \\(\\bar x\\) = $14,100 is a statistic. Over the last few years, the total mean cost to treat tuberculosis in India has been $13,800. This $13,800 is considered a parameter because it is the “known” value for the full population.\nDifferent symbols are used to distinguish between the sample mean (a statistic) and the population mean (a parameter). The symbol for the sample mean is \\(\\bar x\\). The symbol for the population mean is \\(\\mu\\).\nPerspective\nThe mean cost to treat the ten tuberculosis patients in the sample was \\(\\bar x\\) = $14,100. This number gives us some useful information. However, if this was all we were given, we would not be able to distinguish the data above from a situation where the cost for each of the ten patients was exactly $14,100. Notice that if the cost for each patient was $14,100, the mean would be:\n\\[\\bar x=\\frac{14100 + 14100 + 14100 + 14100 + 14100 + 14100 + 14100 + 14100 + 14100 + 14100}{10} =14,100\\]\nEven though measures of center are important, we need to consider the shape, center and spread of a distribution of data. When evaluating data, it is sometimes tempting to compute a mean but to avoid creating a histogram. This can lead to errant decisions based on a misunderstanding or incorrect transcription of data. If there is a transcription error in the data, it is sometimes easiest to detect it as an outlier in a histogram."
  },
  {
    "objectID": "1-Data_Literacy/5-Shape_and_Center.html#summary",
    "href": "1-Data_Literacy/5-Shape_and_Center.html#summary",
    "title": "Describing Quantitative Data (Shape & Center)",
    "section": "Summary",
    "text": "Summary\n\nRemember…\n\n\n\nHistograms are created by dividing the number line into several equal parts, starting at or below the minimum value occurring in the data and ending at or above the maximum value in the data. The number of data points occurring in each interval (called a bin) are counted. A bar is then drawn for each bin so that the height of the bar shows the number of data points contained in that bin.\nA histogram allows us to visually interpret data to quickly recognize which values are most common and which values are least common in the data.\nHistograms can be left-skewed (the majority of the data is on the right of the histogram, less common values stretch to the left side), right-skewed (majority of the data is on the left side with less common values stretching to the right), or symmetrical and bell-shaped (most data is in the middle with less common values stretching out to either side).\nThe mean, median, and mode are measures of the center of a distribution. The mean is the most common measure of center and is computed by adding up the observed data and dividing by the number of observations in the data set. The median represents the 50th percentile in the data. The mean can be calculated in R using mean(...), the median by using median(...), and the mode by table(...) where the ... in each case consists of the data.\nWhen comparing the centers of distributions using graphical and numerical summaries, the direction of the skew showing in the histogram will generally correspond with the mean being pulled in that direction.\n\n\n\n\nRight-skewed\n\n\nSymmetric & Bell-shaped\n\n\nLeft-skewed\n\n\n\n\n&lt;img src/Images/Lesson03_shape_wageshist.png” width=130%&gt;\n\n\n&lt;img src/Images/Lesson03_shape_maleheightshist.png” width=130%&gt;\n\n&lt;img src/Images/Lesson03_shape_gpahist.png” width=130%&gt;\n\n\n\n\nMean: $10.45\nMedian: $9.04\nMean is to the right of the median.\n\n\nMean: 71.1 inches\nMedian: 71 inches\nMean and median are roughly equal.\n\n\nMean: 3.42\nMedian: 3.45\nMean is to the left of the median.\n\n\n\n\n\nIn a symmetrical and bell-shaped distribution of data, the mean, median, and mode are all roughly the same in value. However, in a skewed distribution, the mean is strongly influenced by outliers and tends to be pulled in the direction of the skew. In a left-skewed distribution, the mean will tend to be to the left of the median. In a right-skewed distribution, the mean will tend to be to the right of the median.\nA parameter is a true (but usually unknown) number that describes a population. A statistic is an estimate of a parameter obtained from a sample of the population."
  },
  {
    "objectID": "2-Tidy_Data/04-Tidyverse_Fundamentals.html",
    "href": "2-Tidy_Data/04-Tidyverse_Fundamentals.html",
    "title": "Introducing the Tidyverse - Part 1",
    "section": "",
    "text": "In statistics classes, you are typically provided simple, clean datasets to load and analyze with ease. This is a terrible disservice to anyone who will deal with data outside of the classroom.\nAnyone who works with data will have to do some data wrangling. Data wrangling is an appropriate description of cleaning, sorting, filtering, summarizing, transforming, and a whole host of other activities to make data usable for a specific purpose.\nIn this document, we introduce a moderately messy dataset and demonstrate basic programming commands to help us get data ready for analysis or visualization."
  },
  {
    "objectID": "2-Tidy_Data/04-Tidyverse_Fundamentals.html#additional-resources",
    "href": "2-Tidy_Data/04-Tidyverse_Fundamentals.html#additional-resources",
    "title": "Introducing the Tidyverse - Part 1",
    "section": "Additional Resources",
    "text": "Additional Resources\nBelow are 2 great resources for digging a little deeper into data manipulation in R.\nTidyverse Cheat Sheet\nR for Data Science\nNext, we will explain a few programming fundamentals that will help make"
  },
  {
    "objectID": "2-Tidy_Data/04-Tidyverse_Fundamentals.html#logical-operators",
    "href": "2-Tidy_Data/04-Tidyverse_Fundamentals.html#logical-operators",
    "title": "Introducing the Tidyverse - Part 1",
    "section": "Logical Operators",
    "text": "Logical Operators\nLogical operators are used extensively in computer programming to determine if a certain condition is met. They always return a “True” or a “False”, but we can treat them like a 0 for false and 1 for true.\nWe can tell a computer to determine a conditional statement (typically “less than”, “greater than” or “not equal to”) for specific variables, and it will return a TRUE if the statement is true and FALSE if not.\n\nQuantitative Variables\nLet’s examine the height_cm column in the survey data.\n\nfavstats(survey$Height_cm)\n\n  min  Q1 median      Q3 max     mean       sd   n missing\n 1.68 161    170 178.125 999 169.2412 53.54382 312       0\n\nhist(survey$Height_cm)\n\n\n\n\n\n\n\n\nThe maximum is 999 cm, which is around 33 Feet! We know this is not a possible value.\nIt is very unlikely that a high school student is taller than 7 feet. We can use a logical operator to see which students are taller than 7 feet (213.36 cm):\n\nsurvey$Height_cm &gt; 213.36\n\n  [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [13] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [25] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [37] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [49] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [61] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [73] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE\n [85] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [97] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[109] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[121] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[133] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[145] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[157] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[169] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[181] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[193] FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[205] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[217] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[229] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[241] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[253] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[265] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[277] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[289] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n[301] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n\n# To illustrate, the below code puts the Survey column, Height_cm, along with the TRUE/FALSE logical\ndata.frame(Height_cm =survey$Height_cm, logical = survey$Height_cm &gt; 213.36)[73:84,]\n\n   Height_cm logical\n73     162.0   FALSE\n74     172.0   FALSE\n75     160.0   FALSE\n76     175.0   FALSE\n77     182.8   FALSE\n78     153.0   FALSE\n79     184.0   FALSE\n80     170.0   FALSE\n81     150.0   FALSE\n82     177.8   FALSE\n83     999.0    TRUE\n84     172.7   FALSE\n\n\nWhat does the above code return?\nA list of TRUE and FALSE for every line of the data. It is as long as the number of rows in the dataset.\n\n\nCategorical Variables\nWe can also use logical operators for categorical data. For example, if we wanted to see how many people are ambidextrous, we can run the following:\n\n# What are unique values in of the respondents?\n\nunique(survey$Handed)\n\n[1] \"Left-Handed\"  \"Right-Handed\" \"Ambidextrous\"\n\n# Use a logical operator to get TRUE and FALSE for students who responded \"Ambidextrous\" on the survey question about handedness\n\nsum(survey$Handed == \"Ambidextrous\")\n\n[1] 9\n\ndata.frame(Handed = survey$Handed, logical = survey$Handed == 'Ambidextrous')\n\n          Handed logical\n1    Left-Handed   FALSE\n2   Right-Handed   FALSE\n3   Right-Handed   FALSE\n4   Right-Handed   FALSE\n5    Left-Handed   FALSE\n6   Right-Handed   FALSE\n7   Ambidextrous    TRUE\n8   Right-Handed   FALSE\n9   Right-Handed   FALSE\n10  Right-Handed   FALSE\n11  Right-Handed   FALSE\n12   Left-Handed   FALSE\n13  Right-Handed   FALSE\n14  Right-Handed   FALSE\n15  Right-Handed   FALSE\n16  Right-Handed   FALSE\n17  Right-Handed   FALSE\n18  Right-Handed   FALSE\n19  Right-Handed   FALSE\n20   Left-Handed   FALSE\n21  Right-Handed   FALSE\n22   Left-Handed   FALSE\n23  Right-Handed   FALSE\n24  Right-Handed   FALSE\n25  Right-Handed   FALSE\n26  Right-Handed   FALSE\n27  Right-Handed   FALSE\n28  Ambidextrous    TRUE\n29  Right-Handed   FALSE\n30  Right-Handed   FALSE\n31  Right-Handed   FALSE\n32  Right-Handed   FALSE\n33  Right-Handed   FALSE\n34  Right-Handed   FALSE\n35   Left-Handed   FALSE\n36  Right-Handed   FALSE\n37  Right-Handed   FALSE\n38   Left-Handed   FALSE\n39  Right-Handed   FALSE\n40  Right-Handed   FALSE\n41  Right-Handed   FALSE\n42  Right-Handed   FALSE\n43  Right-Handed   FALSE\n44  Right-Handed   FALSE\n45  Right-Handed   FALSE\n46  Right-Handed   FALSE\n47  Right-Handed   FALSE\n48  Ambidextrous    TRUE\n49  Right-Handed   FALSE\n50  Right-Handed   FALSE\n51  Right-Handed   FALSE\n52  Right-Handed   FALSE\n53  Right-Handed   FALSE\n54  Right-Handed   FALSE\n55  Right-Handed   FALSE\n56   Left-Handed   FALSE\n57  Right-Handed   FALSE\n58  Right-Handed   FALSE\n59  Right-Handed   FALSE\n60  Right-Handed   FALSE\n61  Right-Handed   FALSE\n62  Right-Handed   FALSE\n63  Right-Handed   FALSE\n64  Right-Handed   FALSE\n65  Right-Handed   FALSE\n66  Right-Handed   FALSE\n67  Right-Handed   FALSE\n68  Right-Handed   FALSE\n69  Right-Handed   FALSE\n70  Right-Handed   FALSE\n71  Right-Handed   FALSE\n72  Right-Handed   FALSE\n73  Right-Handed   FALSE\n74   Left-Handed   FALSE\n75   Left-Handed   FALSE\n76  Right-Handed   FALSE\n77  Right-Handed   FALSE\n78  Right-Handed   FALSE\n79  Right-Handed   FALSE\n80   Left-Handed   FALSE\n81  Right-Handed   FALSE\n82   Left-Handed   FALSE\n83  Ambidextrous    TRUE\n84  Right-Handed   FALSE\n85  Right-Handed   FALSE\n86   Left-Handed   FALSE\n87  Right-Handed   FALSE\n88  Right-Handed   FALSE\n89   Left-Handed   FALSE\n90  Right-Handed   FALSE\n91   Left-Handed   FALSE\n92  Right-Handed   FALSE\n93  Right-Handed   FALSE\n94  Right-Handed   FALSE\n95  Right-Handed   FALSE\n96  Right-Handed   FALSE\n97  Right-Handed   FALSE\n98  Right-Handed   FALSE\n99  Right-Handed   FALSE\n100 Right-Handed   FALSE\n101 Right-Handed   FALSE\n102 Right-Handed   FALSE\n103 Right-Handed   FALSE\n104 Right-Handed   FALSE\n105 Right-Handed   FALSE\n106 Right-Handed   FALSE\n107 Right-Handed   FALSE\n108 Right-Handed   FALSE\n109 Right-Handed   FALSE\n110 Right-Handed   FALSE\n111 Right-Handed   FALSE\n112 Right-Handed   FALSE\n113 Right-Handed   FALSE\n114 Right-Handed   FALSE\n115 Right-Handed   FALSE\n116 Right-Handed   FALSE\n117 Right-Handed   FALSE\n118 Right-Handed   FALSE\n119 Right-Handed   FALSE\n120 Right-Handed   FALSE\n121 Right-Handed   FALSE\n122 Right-Handed   FALSE\n123 Right-Handed   FALSE\n124 Right-Handed   FALSE\n125 Right-Handed   FALSE\n126 Right-Handed   FALSE\n127 Right-Handed   FALSE\n128 Right-Handed   FALSE\n129 Right-Handed   FALSE\n130 Right-Handed   FALSE\n131  Left-Handed   FALSE\n132 Right-Handed   FALSE\n133 Right-Handed   FALSE\n134 Right-Handed   FALSE\n135 Right-Handed   FALSE\n136 Right-Handed   FALSE\n137 Right-Handed   FALSE\n138 Right-Handed   FALSE\n139 Right-Handed   FALSE\n140 Right-Handed   FALSE\n141 Right-Handed   FALSE\n142 Right-Handed   FALSE\n143 Ambidextrous    TRUE\n144 Right-Handed   FALSE\n145 Right-Handed   FALSE\n146 Right-Handed   FALSE\n147 Right-Handed   FALSE\n148 Right-Handed   FALSE\n149  Left-Handed   FALSE\n150 Right-Handed   FALSE\n151 Right-Handed   FALSE\n152 Right-Handed   FALSE\n153 Right-Handed   FALSE\n154 Right-Handed   FALSE\n155 Right-Handed   FALSE\n156 Right-Handed   FALSE\n157 Right-Handed   FALSE\n158 Right-Handed   FALSE\n159 Right-Handed   FALSE\n160 Right-Handed   FALSE\n161 Right-Handed   FALSE\n162 Right-Handed   FALSE\n163  Left-Handed   FALSE\n164 Right-Handed   FALSE\n165 Right-Handed   FALSE\n166 Right-Handed   FALSE\n167  Left-Handed   FALSE\n168 Right-Handed   FALSE\n169  Left-Handed   FALSE\n170  Left-Handed   FALSE\n171 Right-Handed   FALSE\n172 Right-Handed   FALSE\n173 Right-Handed   FALSE\n174 Right-Handed   FALSE\n175 Right-Handed   FALSE\n176 Right-Handed   FALSE\n177 Right-Handed   FALSE\n178 Right-Handed   FALSE\n179 Right-Handed   FALSE\n180 Right-Handed   FALSE\n181 Right-Handed   FALSE\n182 Right-Handed   FALSE\n183 Right-Handed   FALSE\n184 Right-Handed   FALSE\n185 Ambidextrous    TRUE\n186 Right-Handed   FALSE\n187 Right-Handed   FALSE\n188 Right-Handed   FALSE\n189 Right-Handed   FALSE\n190 Right-Handed   FALSE\n191 Right-Handed   FALSE\n192 Right-Handed   FALSE\n193 Right-Handed   FALSE\n194 Right-Handed   FALSE\n195 Right-Handed   FALSE\n196 Right-Handed   FALSE\n197 Right-Handed   FALSE\n198 Right-Handed   FALSE\n199 Right-Handed   FALSE\n200 Right-Handed   FALSE\n201 Right-Handed   FALSE\n202 Right-Handed   FALSE\n203 Right-Handed   FALSE\n204 Right-Handed   FALSE\n205 Right-Handed   FALSE\n206  Left-Handed   FALSE\n207 Ambidextrous    TRUE\n208 Right-Handed   FALSE\n209 Ambidextrous    TRUE\n210 Right-Handed   FALSE\n211 Right-Handed   FALSE\n212 Right-Handed   FALSE\n213 Right-Handed   FALSE\n214 Right-Handed   FALSE\n215 Right-Handed   FALSE\n216 Right-Handed   FALSE\n217 Right-Handed   FALSE\n218 Right-Handed   FALSE\n219 Right-Handed   FALSE\n220 Right-Handed   FALSE\n221 Right-Handed   FALSE\n222 Right-Handed   FALSE\n223 Right-Handed   FALSE\n224  Left-Handed   FALSE\n225  Left-Handed   FALSE\n226 Right-Handed   FALSE\n227 Right-Handed   FALSE\n228 Right-Handed   FALSE\n229 Right-Handed   FALSE\n230 Right-Handed   FALSE\n231 Right-Handed   FALSE\n232 Right-Handed   FALSE\n233 Right-Handed   FALSE\n234 Right-Handed   FALSE\n235 Right-Handed   FALSE\n236  Left-Handed   FALSE\n237 Right-Handed   FALSE\n238  Left-Handed   FALSE\n239 Right-Handed   FALSE\n240  Left-Handed   FALSE\n241 Right-Handed   FALSE\n242 Right-Handed   FALSE\n243 Right-Handed   FALSE\n244  Left-Handed   FALSE\n245 Right-Handed   FALSE\n246 Right-Handed   FALSE\n247 Right-Handed   FALSE\n248 Right-Handed   FALSE\n249 Right-Handed   FALSE\n250 Right-Handed   FALSE\n251  Left-Handed   FALSE\n252 Right-Handed   FALSE\n253 Right-Handed   FALSE\n254 Right-Handed   FALSE\n255 Right-Handed   FALSE\n256 Right-Handed   FALSE\n257 Right-Handed   FALSE\n258 Right-Handed   FALSE\n259 Right-Handed   FALSE\n260 Right-Handed   FALSE\n261 Right-Handed   FALSE\n262 Right-Handed   FALSE\n263 Right-Handed   FALSE\n264 Right-Handed   FALSE\n265 Right-Handed   FALSE\n266 Right-Handed   FALSE\n267 Right-Handed   FALSE\n268 Right-Handed   FALSE\n269 Right-Handed   FALSE\n270 Right-Handed   FALSE\n271 Right-Handed   FALSE\n272 Right-Handed   FALSE\n273 Right-Handed   FALSE\n274 Right-Handed   FALSE\n275 Right-Handed   FALSE\n276  Left-Handed   FALSE\n277 Right-Handed   FALSE\n278 Right-Handed   FALSE\n279 Right-Handed   FALSE\n280  Left-Handed   FALSE\n281 Right-Handed   FALSE\n282 Right-Handed   FALSE\n283 Right-Handed   FALSE\n284 Right-Handed   FALSE\n285  Left-Handed   FALSE\n286 Right-Handed   FALSE\n287 Right-Handed   FALSE\n288 Ambidextrous    TRUE\n289  Left-Handed   FALSE\n290 Right-Handed   FALSE\n291  Left-Handed   FALSE\n292 Right-Handed   FALSE\n293 Right-Handed   FALSE\n294 Right-Handed   FALSE\n295 Right-Handed   FALSE\n296 Right-Handed   FALSE\n297 Right-Handed   FALSE\n298 Right-Handed   FALSE\n299  Left-Handed   FALSE\n300 Right-Handed   FALSE\n301 Right-Handed   FALSE\n302 Right-Handed   FALSE\n303 Right-Handed   FALSE\n304 Right-Handed   FALSE\n305 Right-Handed   FALSE\n306 Right-Handed   FALSE\n307  Left-Handed   FALSE\n308 Right-Handed   FALSE\n309 Right-Handed   FALSE\n310 Right-Handed   FALSE\n311 Right-Handed   FALSE\n312 Right-Handed   FALSE\n\n\nWe will show you why this is useful when we introduce tidyverse functions."
  },
  {
    "objectID": "2-Tidy_Data/04-Tidyverse_Fundamentals.html#ceci-nest-pas-une-pipe",
    "href": "2-Tidy_Data/04-Tidyverse_Fundamentals.html#ceci-nest-pas-une-pipe",
    "title": "Introducing the Tidyverse - Part 1",
    "section": "Ceci n’est pas une pipe",
    "text": "Ceci n’est pas une pipe\nThe tidyverse organizes actions to data sequentially. We separate steps by what is called a “pipe” which is programmed %&gt;%.\nHINT: The shortkey for adding a “pipe” is ctrl+shift+m for Windows, and cmd+shift+m on Mac. Learn this because we use them a lot!"
  },
  {
    "objectID": "2-Tidy_Data/04-Tidyverse_Fundamentals.html#removing-rows---filter",
    "href": "2-Tidy_Data/04-Tidyverse_Fundamentals.html#removing-rows---filter",
    "title": "Introducing the Tidyverse - Part 1",
    "section": "Removing Rows - filter()",
    "text": "Removing Rows - filter()\nLogical operators are useful when removing rows from a dataset. The most common logical operators used to filter rows are:\n\n&lt; and &lt;= means “less than” and “less than or equal to” respectively\n&gt; and &gt;= means “greater than” and “greater than or equal to” respectively\n== means “equal to” (NOTE: we use double equals because in most computer languages, a single = is an assignment operator. This avoids ambiguity)\n!= means “not equal to”; this one is useful if you want to eliminate one level of a variable\n%in% is useful for defining a list of levels that you want to include\n\nWe typically begin with the raw dataset, then “pipe” that dataset into a sequence of functions using the “pipe” operator, %&gt;%.\nLet’s begin by filtering out rows we think have legitimate heights:\n\nsurvey %&gt;% \n  filter(Height_cm &lt; 214)\n\n# A tibble: 310 × 60\n   Country Region DataYear ClassGrade Gender Ageyears Handed       Height_cm\n   &lt;chr&gt;   &lt;chr&gt;     &lt;int&gt;      &lt;int&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;            &lt;dbl&gt;\n 1 USA     FL         2022         12 Male         18 Left-Handed        182\n 2 USA     IN         2022         12 Male         17 Right-Handed       190\n 3 USA     GA         2022         12 Female       17 Right-Handed       172\n 4 USA     NC         2022         11 Female       15 Right-Handed       163\n 5 USA     CO         2022         12 Female       17 Left-Handed         51\n 6 USA     MO         2022         11 Male         17 Right-Handed       181\n 7 USA     NC         2022         11 Male         17 Ambidextrous       175\n 8 USA     SC         2022         11 Female       18 Right-Handed       160\n 9 USA     WA         2022         11 Female       16 Right-Handed       156\n10 USA     WA         2022         12 Female       17 Right-Handed       169\n# ℹ 300 more rows\n# ℹ 52 more variables: Footlength_cm &lt;dbl&gt;, Armspan_cm &lt;dbl&gt;,\n#   Languages_spoken &lt;dbl&gt;, Travel_to_School &lt;chr&gt;,\n#   Travel_time_to_School &lt;int&gt;, Reaction_time &lt;dbl&gt;,\n#   Score_in_memory_game &lt;dbl&gt;, Favourite_physical_activity &lt;chr&gt;,\n#   Imprtance_reducing_pllutin &lt;int&gt;, Imprtance_recycling_rubbish &lt;int&gt;,\n#   Imprtance_cnserving_water &lt;int&gt;, Imprtance_saving_energy &lt;int&gt;, …\n\n\nThe above code will return a new dataset without the outliers.\nHow many rows does the original dataset have?\nHow many rows does the filtered dataset have?\nSuppose for some reason, we only want to include right- or left-handed people (excluding ambidextrous). We can add multiple conditions in the filter() function separated by a comma:\n\nunique(survey$Handed)\n\n[1] \"Left-Handed\"  \"Right-Handed\" \"Ambidextrous\"\n\nsurvey %&gt;%\n  filter(Height_cm &lt; 214,\n         Handed != \"Ambidextrous\")\n\n# A tibble: 302 × 60\n   Country Region DataYear ClassGrade Gender Ageyears Handed       Height_cm\n   &lt;chr&gt;   &lt;chr&gt;     &lt;int&gt;      &lt;int&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;            &lt;dbl&gt;\n 1 USA     FL         2022         12 Male         18 Left-Handed        182\n 2 USA     IN         2022         12 Male         17 Right-Handed       190\n 3 USA     GA         2022         12 Female       17 Right-Handed       172\n 4 USA     NC         2022         11 Female       15 Right-Handed       163\n 5 USA     CO         2022         12 Female       17 Left-Handed         51\n 6 USA     MO         2022         11 Male         17 Right-Handed       181\n 7 USA     SC         2022         11 Female       18 Right-Handed       160\n 8 USA     WA         2022         11 Female       16 Right-Handed       156\n 9 USA     WA         2022         12 Female       17 Right-Handed       169\n10 USA     WA         2022         11 Male         18 Right-Handed       160\n# ℹ 292 more rows\n# ℹ 52 more variables: Footlength_cm &lt;dbl&gt;, Armspan_cm &lt;dbl&gt;,\n#   Languages_spoken &lt;dbl&gt;, Travel_to_School &lt;chr&gt;,\n#   Travel_time_to_School &lt;int&gt;, Reaction_time &lt;dbl&gt;,\n#   Score_in_memory_game &lt;dbl&gt;, Favourite_physical_activity &lt;chr&gt;,\n#   Imprtance_reducing_pllutin &lt;int&gt;, Imprtance_recycling_rubbish &lt;int&gt;,\n#   Imprtance_cnserving_water &lt;int&gt;, Imprtance_saving_energy &lt;int&gt;, …\n\n# Equivalently we can use %in% instead of the !=\n\nsurvey %&gt;%\n  filter(Height_cm &lt; 214,\n         Handed %in% c(\"Left-Handed\", \"Right-Handed\"),\n         Region %in% c(\"MO\", \"FL\"))\n\n# A tibble: 27 × 60\n   Country Region DataYear ClassGrade Gender Ageyears Handed       Height_cm\n   &lt;chr&gt;   &lt;chr&gt;     &lt;int&gt;      &lt;int&gt; &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;            &lt;dbl&gt;\n 1 USA     FL         2022         12 Male         18 Left-Handed       182 \n 2 USA     MO         2022         11 Male         17 Right-Handed      181 \n 3 USA     MO         2022         11 Female       17 Right-Handed      151 \n 4 USA     FL         2022         12 Female       18 Right-Handed      170 \n 5 USA     FL         2022         12 Male         18 Right-Handed      180 \n 6 USA     FL         2022         12 Female       18 Right-Handed      173.\n 7 USA     FL         2022         12 Male         18 Right-Handed      175 \n 8 USA     FL         2022         12 Male         17 Right-Handed      180 \n 9 USA     MO         2022         11 Male         17 Right-Handed      164 \n10 USA     FL         2022         12 Female       17 Right-Handed      166 \n# ℹ 17 more rows\n# ℹ 52 more variables: Footlength_cm &lt;dbl&gt;, Armspan_cm &lt;dbl&gt;,\n#   Languages_spoken &lt;dbl&gt;, Travel_to_School &lt;chr&gt;,\n#   Travel_time_to_School &lt;int&gt;, Reaction_time &lt;dbl&gt;,\n#   Score_in_memory_game &lt;dbl&gt;, Favourite_physical_activity &lt;chr&gt;,\n#   Imprtance_reducing_pllutin &lt;int&gt;, Imprtance_recycling_rubbish &lt;int&gt;,\n#   Imprtance_cnserving_water &lt;int&gt;, Imprtance_saving_energy &lt;int&gt;, …\n\nsurvey$Handed %in% c(\"Left-Handed\", \"Right-Handed\")\n\n  [1]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE\n [13]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [25]  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [37]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE\n [49]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [61]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [73]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE\n [85]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [97]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[109]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[121]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[133]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE  TRUE\n[145]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[157]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[169]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[181]  TRUE  TRUE  TRUE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[193]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[205]  TRUE  TRUE FALSE  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[217]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[229]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[241]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[253]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[265]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[277]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE\n[289]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[301]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n\n\nHow many rows does our latest dataset have?"
  },
  {
    "objectID": "2-Tidy_Data/04-Tidyverse_Fundamentals.html#adding-columns---mutate",
    "href": "2-Tidy_Data/04-Tidyverse_Fundamentals.html#adding-columns---mutate",
    "title": "Introducing the Tidyverse - Part 1",
    "section": "Adding Columns - mutate()",
    "text": "Adding Columns - mutate()\nThe mutate() statement is used to add new columns to a dataset.\nTo create a new column, “pipe” the previous steps into the mutate() statement. Inside the parentheses, give the new column a name and set it equal to what you want that column to be.\nEXAMPLE: Create a column of the ratio of Height to armspan called, ht_to_span, by using a mutate() statement:\n\nclean &lt;- survey %&gt;%\n  filter(Height_cm &lt; 214,\n         Handed != \"Ambidextrous\") %&gt;%\n  mutate(ht_to_span = Height_cm / Armspan_cm,\n         ht_in = Height_cm / 2.54) %&gt;%\n  select(Handed, ht_to_span, ht_in)\n\nView(clean)\n\nNotice we no longer have to use $ to access specific columns! The tidyverse lives up to its name!"
  },
  {
    "objectID": "2-Tidy_Data/04-Tidyverse_Fundamentals.html#selecting-columns---select",
    "href": "2-Tidy_Data/04-Tidyverse_Fundamentals.html#selecting-columns---select",
    "title": "Introducing the Tidyverse - Part 1",
    "section": "Selecting Columns - select()",
    "text": "Selecting Columns - select()\nThere are now over 60 columns in this dataset. Suppose we are only interested in reaction times and height-to-armspan ratio as they related to handedness. To tidy up the data even further, select only the columns we are interested in (Handed, Reaction_time, and ht_to_span):\n\nsurvey %&gt;%\n  filter(Height_cm &lt; 214,\n         Handed != \"Ambidextrous\") %&gt;%\n  mutate(ht_to_span = Height_cm / Armspan_cm) %&gt;%\n  select(Handed, Reaction_time, ht_to_span)\n\n# A tibble: 302 × 3\n   Handed       Reaction_time ht_to_span\n   &lt;chr&gt;                &lt;dbl&gt;      &lt;dbl&gt;\n 1 Left-Handed          0.349      1.28 \n 2 Right-Handed         0.358      0.990\n 3 Right-Handed         0.447      1.03 \n 4 Right-Handed         0.438      1.02 \n 5 Left-Handed          0.542      0.981\n 6 Right-Handed         0.428      0.968\n 7 Right-Handed         0.427      1.01 \n 8 Right-Handed         0.412      1.10 \n 9 Right-Handed         0.346      1.04 \n10 Right-Handed         0.391      1    \n# ℹ 292 more rows\n\n\nSee how much we can do in just a few short, sequential lines of code? Let’s name out clean dataset, clean, and create a boxplot of reaction times comparing left and right handed students:\n\nclean &lt;- survey %&gt;%\n  filter(Height_cm &lt; 214,\n         Handed != \"Ambidextrous\") %&gt;%\n  mutate(ht_to_span = Height_cm / Armspan_cm) %&gt;%\n  select(Handed, Reaction_time, ht_to_span)\n\nboxplot(clean$Reaction_time ~ clean$Handed)\n\n# Modify the code to remove outliers in Reaction_time and remake the boxplot\n\nclean &lt;- survey %&gt;%\n  filter(Height_cm &lt; 214,\n         Handed != \"Ambidextrous\") %&gt;%\n  mutate(ht_to_span = Height_cm / Armspan_cm) %&gt;%\n  select(Handed, Reaction_time, ht_to_span)\n\nboxplot(clean$Reaction_time ~ clean$Handed)"
  },
  {
    "objectID": "2-Tidy_Data/04-Tidyverse_Fundamentals.html#summarising-data---group_by-summarise",
    "href": "2-Tidy_Data/04-Tidyverse_Fundamentals.html#summarising-data---group_by-summarise",
    "title": "Introducing the Tidyverse - Part 1",
    "section": "Summarising Data - group_by() + summarise()",
    "text": "Summarising Data - group_by() + summarise()\nThe above data might be adequate for a visualization or analysis, but we can calculate summary statistics tables like we did with favstats() using the tidyverse.\nThe summarise() (or equivalently, summarize()) function is like the mutate statement. We create a name for the new column and set it equal to what we want.\nLet’s name the new dataset, clean, and see how to make summaries using tidyverse.\n\nboxplot(survey$Reaction_time)\n\n\n\n\n\n\n\nclean &lt;- survey %&gt;%\n  filter(Height_cm &lt; 214, \n         Handed != \"Ambidextrous\", \n         Reaction_time &lt; 1,\n         Armspan_cm &gt; 0,\n         ClassGrade == 12) %&gt;%\n  mutate(ht_to_span = Height_cm / Armspan_cm) %&gt;%\n  select(Handed, Reaction_time, ht_to_span)\n\nboxplot(clean$Reaction_time ~ clean$Handed)\n\n\n\n\n\n\n\nclean %&gt;%\n  summarise(\n    mn_react_time = mean(Reaction_time, na.rm=TRUE),\n    med_react_time = median(Reaction_time, na.rm=TRUE),\n    mn_ratio = mean(ht_to_span, na.rm=TRUE)\n  )\n\n# A tibble: 1 × 3\n  mn_react_time med_react_time mn_ratio\n          &lt;dbl&gt;          &lt;dbl&gt;    &lt;dbl&gt;\n1         0.397          0.366     1.27\n\n\nNotice that the mn_ratio is Inf.\nWhy might that be the case?\nModify the code chunk to exclude rows where arm span is 0:\n\nclean &lt;- survey %&gt;%\n  filter(Height_cm &lt; 214,\n         Handed != \"Ambidextrous\",\n         Reaction_time &lt; 1) %&gt;%\n  mutate(ht_to_span = Height_cm / Armspan_cm) %&gt;%\n  select(Handed, Reaction_time, ht_to_span)\n\n\n\nclean %&gt;%\n  summarise(\n    `Mean Reaction Time` = mean(Reaction_time, na.rm=TRUE),\n    med_react_time = median(Reaction_time, na.rm=TRUE),\n    mn_ratio = mean(ht_to_span, na.rm=TRUE),\n    min_react_time = min(Reaction_time, na.rm=TRUE)\n  ) %&gt;% knitr::kable()\n\n\n\n\nMean Reaction Time\nmed_react_time\nmn_ratio\nmin_react_time\n\n\n\n\n0.4205068\n0.388\nInf\n0.067\n\n\n\n\nfavstats(clean$Reaction_time ~ clean$Handed)\n\n  clean$Handed   min      Q1 median      Q3   max      mean        sd   n\n1  Left-Handed 0.274 0.34825 0.4415 0.53850 0.895 0.4741471 0.1691585  34\n2 Right-Handed 0.067 0.33825 0.3845 0.44775 0.995 0.4134380 0.1302318 258\n  missing\n1       0\n2       0\n\nfavstats(clean$ht_to_span)\n\n    min        Q1   median       Q3 max mean  sd   n missing\n 0.0168 0.9841579 1.006519 1.052681 Inf  Inf NaN 292       0\n\n\nIf we want to get means for separate groups, we can add a group_by() statement to tell which variable(s) we want to group by:\n\nclean &lt;- survey %&gt;%\n  filter(Height_cm &lt; 214,\n         Handed != \"Ambidextrous\",\n         Reaction_time &lt; 1,\n         Armspan_cm &gt; 0) %&gt;%\n  mutate(ht_to_span = Height_cm / Armspan_cm) %&gt;%\n  select(Handed, Reaction_time, ht_to_span)\n\n\n\nclean %&gt;%\n  group_by(Handed) %&gt;%\n  summarise(\n    mn_react_time = mean(Reaction_time, na.rm=TRUE),\n    med_react_time = median(Reaction_time, na.rm=TRUE),\n    mn_ratio = mean(ht_to_span, na.rm=TRUE),\n    max_react = max(Reaction_time),\n    count = n()\n  ) %&gt;%\n  knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\nHanded\nmn_react_time\nmed_react_time\nmn_ratio\nmax_react\ncount\n\n\n\n\nLeft-Handed\n0.4741471\n0.4415\n1.266468\n0.895\n34\n\n\nRight-Handed\n0.4133891\n0.3840\n1.260190\n0.995\n257\n\n\n\n\n\nThe n() is very useful for counting up the number of observations in each group.\nIf we were only interested in the summary statistics table, we can do everything in one series of steps:\n\nsummary_stats_table &lt;-  survey %&gt;%\n  filter(\n    Height_cm &lt; 214,\n    Handed != \"Ambidextrous\",\n    Reaction_time &lt; 1,\n    Armspan_cm &gt; 0) %&gt;%\n  mutate(ht_to_span = Height_cm / Armspan_cm) %&gt;%\n  select(Handed, Reaction_time, ht_to_span) %&gt;%\n  group_by(Handed) %&gt;%\n  summarise(\n    mn_react_time = mean(Reaction_time, na.rm=TRUE),\n    med_react_time = median(Reaction_time, na.rm=TRUE),\n    mn_ratio = mean(ht_to_span, na.rm=TRUE),\n    max_react = max(Reaction_time),\n    count = n()\n  )\n\nsummary_stats_table\n\n# A tibble: 2 × 6\n  Handed       mn_react_time med_react_time mn_ratio max_react count\n  &lt;chr&gt;                &lt;dbl&gt;          &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt; &lt;int&gt;\n1 Left-Handed          0.474          0.442     1.27     0.895    34\n2 Right-Handed         0.413          0.384     1.26     0.995   257"
  },
  {
    "objectID": "2-Tidy_Data/06-GGPlot_Intro.html",
    "href": "2-Tidy_Data/06-GGPlot_Intro.html",
    "title": "Introducing GGPlot!",
    "section": "",
    "text": "GGPlot is a data visualization library that follows Leland Wilkinson’s Grammar of Graphics. The Grammar of Graphics is a systematic approach to how we think about connecting raw data to visual elements.\nThink about a basic sentence in English: The boy threw the ball. This sentence has a subject (the boy), a verb (threw), and a direct object (the ball). While not all sentences include every part of speech, virtually all sentences have at least a subject and a verb.\nThe grammar of graphics has 3 essential components of distinct graphical elements that are needed to make basic “sentences.” They are like the subjects and verbs of English sentences. These elements are:\n\nData layer\nAesthetic mappings\nGeometry layers\n\nThe data layer identifies the data we wish to express visually.\nThe Aesthetic Mapping is a description of how we map specific data elements to specific chart elements. For example, what variable in the data do we want expressed on the X axis or Y axis. We can also map a data variable to the color element.\nLastly, the Geometry Layer tells the computer how to express those Aesthetic Mappings, such as a scatter plot, boxplot, bar chart, etc.\nAs in English, we can make more complex sentences with other graphical elements, but the three mentioned above will be common to all.\nThis sounds more complicated than it is in practice. So let’s look at a familiar example: the Personality data.\n\nlibrary(tidyverse)\nlibrary(mosaic)\nlibrary(rio)\nlibrary(ggplot2)\n\nbig5 &lt;- import('https://raw.githubusercontent.com/byuistats/Math221D_Cannon/master/Data/All_class_combined_personality_data.csv')"
  },
  {
    "objectID": "2-Tidy_Data/06-GGPlot_Intro.html#adding-color",
    "href": "2-Tidy_Data/06-GGPlot_Intro.html#adding-color",
    "title": "Introducing GGPlot!",
    "section": "Adding Color",
    "text": "Adding Color\nTo see the relationship between sepal length and width for each species separately, map the column Species onto the color element in the aesthetic mapping.\nBecause Species is a variable inside the dataset, we put it INSIDE the aes(). This maps Species onto the chart element, color.\n\nggplot(data = iris, mapping = aes(x = Sepal.Length, y = Sepal.Width, color = Species)) +\n  geom_point()\n\n\n\n\n\n\n\n\nTo change ALL the points to a single color, include a “color” statement in the geometry layer:\n\nggplot(data = iris, mapping = aes(x = Sepal.Length, y = Sepal.Width)) +\n  geom_point(color = \"purple\")"
  },
  {
    "objectID": "2-Tidy_Data/06-GGPlot_Intro.html#more-additions",
    "href": "2-Tidy_Data/06-GGPlot_Intro.html#more-additions",
    "title": "Introducing GGPlot!",
    "section": "More Additions",
    "text": "More Additions\nIt is easy to make more interesting graphs that combine multiple geometries or even multiple data layers. For example, if I want to include a trend line on top of the points, simply add a new geometry. The geom_smooth() geometry can add different types of trend lines. We can specify method = 'lm' meaning “linear model” to get a simple line.\n\nggplot(data = iris, mapping = aes(x = Sepal.Length, y = Sepal.Width, color = Species)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")"
  },
  {
    "objectID": "2-Tidy_Data/06-GGPlot_Intro.html#further-customizations",
    "href": "2-Tidy_Data/06-GGPlot_Intro.html#further-customizations",
    "title": "Introducing GGPlot!",
    "section": "Further Customizations",
    "text": "Further Customizations\nWithout changing the underlying “grammar” we can change the “font,” so to speak. To modify the axis labels or add a title and a subtitle, use a labs() layer.\n\nggplot(data = iris, mapping = aes(x = Sepal.Length, y = Sepal.Width, color = Species)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  labs(\n    x = \"Sepal Length\",\n    y = \"Sepal Width\",\n    title = \"Comparing Sepal Length and Sepal Width by Species\"\n  )"
  },
  {
    "objectID": "2-Tidy_Data/06-GGPlot_Intro.html#themes",
    "href": "2-Tidy_Data/06-GGPlot_Intro.html#themes",
    "title": "Introducing GGPlot!",
    "section": "Themes",
    "text": "Themes\nAn easy way to change many visual elements all at once, ggplot() has several pre-packaged themes() you can add to a plot.\nWe typically want high contrast between data points and the background. This makes it easier to perceive differences. Changing the theme of the chart can make lots of changes all at once. theme_bw() is a useful theme which drops the gray default background.\n\nggplot(iris, mapping = aes(x = Sepal.Length, y = Sepal.Width, color = Species)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  labs(\n    x = \"Sepal Length\",\n    y = \"Sepal Width\",\n    title = \"Comparing Sepal Length and Sepal Width by Species\"\n  ) +\n  theme_bw()\n\n\n\n\n\n\n\n\nThere are more themes to try. If you begin typing theme_ you will see a drop down with several other themes.\nExplore some of the themes. Who can come up with the wildest visualization?"
  },
  {
    "objectID": "2-Tidy_Data/06-GGPlot_Intro.html#facets",
    "href": "2-Tidy_Data/06-GGPlot_Intro.html#facets",
    "title": "Introducing GGPlot!",
    "section": "Facets",
    "text": "Facets\nSometimes adding more things to a graph makes it too cluttered. When dealing with multiple groups, you may want to split the graph into several panels, one for each group.\nFacets allow us to split a graph up based on a variable in the data. For example, if we wanted a separate regression plot for each species, we could “add” a facet:\n\nggplot(iris, mapping = aes(x = Sepal.Length, y = Sepal.Width, color = Species)) +\n  geom_point() +\n  facet_grid(~Species) +\n  geom_smooth(method = \"lm\") +\n  labs(\n    x = \"Sepal Length\",\n    y = \"Sepal Width\",\n    title = \"Comparing Sepal Length and Sepal Width by Species\"\n  ) +\n  theme_bw()\n\n\n\n\n\n\n\n\nNotice that the x-axes are the same for each group by default. That is often how we want to visualize data. Sometimes, though, we want to have each graph only cover the range of the data. We can allow the x and y axes to accommodate different ranges of data by setting the “scales” parameter inside the facet_grid to “free”:\n\nggplot(iris, mapping = aes(x = Sepal.Length, y = Sepal.Width, color = Species)) +\n  geom_point() +\n  facet_grid(~Species, scales = \"free\") +\n  geom_smooth(method = \"lm\") +\n  labs(\n    x = \"Sepal Length\",\n    y = \"Sepal Width\",\n    title = \"Comparing Sepal Length and Sepal Width by Species\"\n  ) +\n  theme_bw()\n\n\n\n\n\n\n\n\nNOTE: When we want to “add” something to a graph, we simply include a + and tell it what we want to add. If we want to learn more about any of the graphing elements and their customization, we can always use the question mark help prompts (eg. ?facet_grid)."
  },
  {
    "objectID": "2-Tidy_Data/06-GGPlot_Intro.html#your-turn",
    "href": "2-Tidy_Data/06-GGPlot_Intro.html#your-turn",
    "title": "Introducing GGPlot!",
    "section": "Your Turn",
    "text": "Your Turn\nCreate a side-by-side boxplot using the iris dataset that looks at the distribution of Sepal.Length for each species type.\nBe sure to:\n1. Color the boxes by Species 2. Add theme_bw() to make the chart more high contrast 3. Add a title Sepal Length by Species\nNOTE: Using color = Species with boxplots doesn’t look great. Try fill = Species instead. Using BOTH is not a good idea, but different combinations and see.\n\n\nggplot() + \n\nError: &lt;text&gt;:5:0: unexpected end of input\n3: \n4: \n  ^"
  },
  {
    "objectID": "2-Tidy_Data/06-GGPlot_Intro.html#conclusion",
    "href": "2-Tidy_Data/06-GGPlot_Intro.html#conclusion",
    "title": "Introducing GGPlot!",
    "section": "Conclusion",
    "text": "Conclusion\nGGplot provides many options for easily making complex visualizations. While there is far too much to cover in one lesson, the basic framework is fairly intuitive once you get the hang of it."
  },
  {
    "objectID": "2-Tidy_Data/06-GGPlot_Intro.html#histograms-and-density-plots",
    "href": "2-Tidy_Data/06-GGPlot_Intro.html#histograms-and-density-plots",
    "title": "Introducing GGPlot!",
    "section": "Histograms and Density Plots",
    "text": "Histograms and Density Plots\nWhen we want to look at the distribution of a single variable, we typically use histograms. Because this is a single variable, we only define an x without a y.\n\nggplot(iris, mapping = aes(x = Sepal.Length)) +\n  geom_histogram() +\n  theme_bw() +\n  labs(\n    title = \"Distribution of Sepal Length\",\n    x = \"Sepal Length\"\n  )\n\n\n\n\n\n\n\n# We can modify the number of bins in a histogram:  Play around with the \"bin\" Parameter\n\nggplot(iris, mapping = aes(x = Sepal.Length)) +\n  geom_histogram(bins = 20) +\n  theme_bw() +\n  labs(\n    title = \"Distribution of Sepal Length\",\n    x = \"Sepal Length\"\n  )\n\n\n\n\n\n\n\n\nThe above histogram includes data from all species. We can distinguish species in several ways. One is to color the bars by species. Compare the difference between “color=Species” and “fill=Species” inside the aesthetic.\nWARNING: Would not recommend:\n\nggplot(iris, mapping = aes(x = Sepal.Length, fill=Species)) +\n  geom_histogram(bins = 20) +\n  theme_bw() +\n  labs(\n    title = \"Distribution of Sepal Length\",\n    x = \"Sepal Length\"\n  )\n\n\n\n\n\n\n\n\nIt’s not usually a good idea to layer histograms like this because it can obscure what is happening behind the covered layers. This is a situation where faceting can be useful.\nRecall that by default the x-axis will be fixed to the same values for each facet. We can let the x axis scale be different for each group by including scales = \"free\" into the facet_grid argument as above.\nTry both and see which tells a more compelling story:\n\nggplot(iris, mapping = aes(x = Sepal.Length, fill = Species)) +\n  geom_histogram(bins = 10) +\n  facet_grid(~Species) +\n  #facet_grid(~Species, scales = \"free\") +\n  theme_bw() +\n  labs(\n    title = \"Distribution of Sepal Length\",\n    x = \"Sepal Length\"\n  )\n\n\n\n\n\n\n\n\n\nA Better Histogram\nWhile histograms are a fine way to express the distribution of quantitative variables, it is not the only way. A Density plot is a smooth version of a histogram. Density plots use data to calculate a smooth line that expresses the quantitative variable as a continuous value rather than using crude bins.\nBy making the smooth line, it is much easier to compare between groups on the same plot:\n\nggplot(iris, mapping = aes(x = Sepal.Length, color = Species)) +\n  geom_density(linewidth = 1.2) +\n  theme_bw() +\n  labs(\n    title = \"Distribution of Sepal Length\",\n    x = \"Sepal Length\"\n  )"
  },
  {
    "objectID": "2-Tidy_Data/07-Unit1_Unit2_AA.html",
    "href": "2-Tidy_Data/07-Unit1_Unit2_AA.html",
    "title": "Data Wrangling - Application Activity",
    "section": "",
    "text": "The Big 5 personality test is the most widely accepted tool for modelling personality in academic psychology. It is based on decades of statistical analysis of personality descriptions across languages and cultures. The big 5 traits are:\n\nOpenness\nConscientiousness\nExtroversion\nAgreeableness\nNeuroticism\n\nBrother Cannon collected personality data on students for the past several semesters, including a few metrics that may be associated with personality traits.\nNOTE: Scores for personality traits are given in percentiles relative to the general population.\nIn this activity, you will practice the process for approaching a dataset outlined in class:\n\nLoad the data and libraries\nExplore the data and generate hypotheses\nPrepare the data for analysis\nPerform the appropriate analysis\n\nData preparation will include using the filter() function. For now, analysis means creating good visualizations that tell a story using ggplot() and base R."
  },
  {
    "objectID": "2-Tidy_Data/07-Unit1_Unit2_AA.html#extroversion",
    "href": "2-Tidy_Data/07-Unit1_Unit2_AA.html#extroversion",
    "title": "Data Wrangling - Application Activity",
    "section": "Extroversion",
    "text": "Extroversion\nQUESTION: Create a new dataset called extro that includes only columns for birth month and extroversion scores. Make sure it only has values that are real.\nHINT: Extroversion is measured in percentiles, and you should already know what the months of the year are.\n\n\nextro &lt;- \n\nError: &lt;text&gt;:4:0: unexpected end of input\n2: extro &lt;- \n3: \n  ^\n\n\nQUESTION: USE GGPLOT to create a side-by-side boxplot of Extroversion scores for all birth months.\n\nggplot()\n\n\n\n\n\n\n\n\nQUESTION: Based on the boxplot, which month appears to be the least extroverted? Explain your reasoning.\nANSWER:\nQUESTION: Based on the boxplot, which month appears to be the most extroverted? Explain your reasoning.\nANSWER:"
  },
  {
    "objectID": "2-Tidy_Data/07-Unit1_Unit2_AA.html#neuroticism",
    "href": "2-Tidy_Data/07-Unit1_Unit2_AA.html#neuroticism",
    "title": "Data Wrangling - Application Activity",
    "section": "Neuroticism",
    "text": "Neuroticism\nQUESTION: Create a dataset called neuro that includes only the Section and Neuroticism columns:\n\n\nneuro &lt;- \n  \n\nError: &lt;text&gt;:4:0: unexpected end of input\n2: neuro &lt;- \n3:   \n  ^\n\n\nQUESTION: USE GGPLOT to create a side-by-side boxplot comparing Neuroticism for all the different sections:\nQUESTION: Based on the boxplot, which section appears to be the lowest in trait neuroticism? Explain your reasoning.\nANSWER:"
  },
  {
    "objectID": "3-Stat_Fundamentals/02-The_Normal_Distribution_Textbook.html",
    "href": "3-Stat_Fundamentals/02-The_Normal_Distribution_Textbook.html",
    "title": "The Normal Distribution (Reading)",
    "section": "",
    "text": "By the end of this lesson, you should be able to:\n\nState the properties of a normal distribution\nCalculate the z-score for an individual observation, given the population mean and standard deviation\nInterpret a z-score\nUse the normal distribution to calculate probabilities for one observation\nApproximate probabilities from a normal distribution using the 68-95-99.7 rule\nCalculate a percentile using the normal distribution"
  },
  {
    "objectID": "3-Stat_Fundamentals/02-The_Normal_Distribution_Textbook.html#lesson-outcomes",
    "href": "3-Stat_Fundamentals/02-The_Normal_Distribution_Textbook.html#lesson-outcomes",
    "title": "The Normal Distribution (Reading)",
    "section": "",
    "text": "By the end of this lesson, you should be able to:\n\nState the properties of a normal distribution\nCalculate the z-score for an individual observation, given the population mean and standard deviation\nInterpret a z-score\nUse the normal distribution to calculate probabilities for one observation\nApproximate probabilities from a normal distribution using the 68-95-99.7 rule\nCalculate a percentile using the normal distribution"
  },
  {
    "objectID": "3-Stat_Fundamentals/02-The_Normal_Distribution_Textbook.html#normal-distributions-and-normal-computations",
    "href": "3-Stat_Fundamentals/02-The_Normal_Distribution_Textbook.html#normal-distributions-and-normal-computations",
    "title": "The Normal Distribution (Reading)",
    "section": "Normal Distributions and Normal Computations",
    "text": "Normal Distributions and Normal Computations\n\nBaseball Batting Averages\n\nIn baseball, a player called the “pitcher” throws a ball to a player called the “batter.” The batter swings a wooden or metal bat and tries to hit the ball. A “hit” is made when the batter successfully hits the ball and runs to a point in the field called first base. A player’s batting average is calculated as the ratio of the number of hits a player makes divided by the number of times the player has attempted to hit the ball or in other words, been “at bat.” Sean Lahman reported the batting averages of several professional baseball players in the United States. (Lahman, 2010) The file BattingAverages.xlsx contains his data.\nThe following histogram summarizes the batting averages for these professional baseball players:\n\nNotice the bell-shaped distribution of the data.\nSuppose we want to estimate the probability that a randomly selected player will have a batting average that is greater than 0.280. One way to do this would be to find the proportion of players in the data set who have a batting average above 0.280. We can do this by finding the number of players who fall into each of the red-colored bins below and dividing this number by the total number of players.\n\nIn other words, we could find the proportion of the total area of the bars that is shaded red out of the combined area of all the bars. This gives us the proportion of players whose batting averages are greater than 0.280.\nOut of the 446 players listed, there are a total of 133 players with batting averages over 0.280. This suggests that the proportion of players whose batting average exceeds 0.280 is:\n\\[\\displaystyle{\\frac{133}{446}} = 0.298\\]\nAlternatively, we can use the fact that the data follow a bell-shaped distribution to find the probability that a player has a batting average above 0.280.\n\n\nDensity Curves\nThe bell-shaped curve superimposed on the histogram above is called a density curve. It is essentially a smooth histogram. Notice how closely this curve follows the observed data.\nThe density curve illustrated on the histogram of the batting average data is special. It is called a normal density curve. This density curve is symmetric and has a bell-shape.\nThe normal density curve is also referred to as a normal distribution or a “Gaussian” distribution (after Carl Friedrich Gauss.)\nThe normal density curve appears in many applications in business, nature, medicine, psychology, sociology, and more. We will use the normal density curve extensively in this course.\nAll density curves, including normal density curves, have two basic properties:\n\nThe total area under the curve equals 1.\nThe density curve always lies on or above the horizontal axis.\n\nBecause of these two properties, the area under the curve can be treated as a probability. If we want to find the probability that a randomly selected baseball player will have a batting average between some range of values, we only need to find the area under the curve in that range. This is illustrated by the region shaded in blue in the figure below.\n\nA normal density curve is uniquely determined by its mean, \\(\\mu\\), and its standard deviation, \\(\\sigma\\). So, if random variables follow a normal distribution with a known mean and standard deviation, then we can calculate any probabilities related to that variable by finding the area under the curve.\nWhen the mean of a normal distribution is 0 and its standard deviation is 1, we call it the standard normal distribution.\nWe will return to this example later, and we will find the area shaded in blue.\n\nCharacteristics of the Normal Curve\n\nIntroduction to \\(z\\)-scores\nIn Ghana, the mean height of young adult women is normally distributed with mean \\(159.0\\) cm and standard deviation \\(4.9\\) cm. (Monden & Smits, 2009) Serwa, a female BYU-Idaho student from Ghana, is \\(169.0\\) cm tall. Her height is \\(169.0 - 159.0 = 10\\) cm greater than the mean. When compared to the standard deviation, she is about two standard deviations (\\(\\approx 2 \\times 4.9\\) cm) taller than the mean.\nThe heights of men are also normally distributed. The mean height of young adult men in Brazil is \\(173.0\\) cm (“Oramento,” 2010), and the standard deviation for the population is \\(6.3\\) cm. (Castilho & Lahr, 2001) A Brazilian BYU-Idaho student, Gustavo, is \\(182.5\\) cm tall. Compared to other Brazilians, he is taller than the mean height of Brazilian men.\n\nAnswer the following question:\n\n\n\nApproximately how many standard deviations above the mean is Gustavo’s height?\n\n\n\nShow/Hide Solution\n\n\nGustavo’s height is \\(182.5 - 173.0 = 9.5 \\text{ cm  }\\) above the mean. The standard deviation of the height of Brazilian men is 6.3 cm, so his height is \\(\\displaystyle{ \\frac{9.5}{6.3} =1.508 }\\) standard deviations above the mean.\n\n\n\n\n\n\nComputing \\(z\\)-scores\nWhen we examined the heights of Serwa and Gustavo, we compared their height to the standard deviation. If we look carefully at the steps we did, we subtracted the mean height for people of the same gender and nationality from each individual’s height, respectively.\nThis shows how much taller or shorter the person is than the mean height. In order to compare the height difference to the standard deviation, we divide the difference by the standard deviation. This gives the number of standard deviations the individual is above or below the mean.\nFor example, Serwa’s height is 169.0 cm. If we subtract the mean and divide by the standard deviation, we get \\[z = \\frac{169.0 - 159.0}{4.9} = 2.041\\] We call this number a \\(z\\)-score. The \\(z\\)-score for a data value tells how many standard deviations away from the mean the observation lies. If the \\(z\\)-score is positive, then the observed value lies above the mean. A negative \\(z\\)-score implies that the value was below the mean.\nWe compute the \\(z\\)-score for Gustavo’s height similarly, and obtain \\[z = \\frac{182.5 - 173.0}{6.3} = 1.508\\] Gustavo’s \\(z\\)-score is 1.508. As noted above, this is about one-and-a-half standard deviations above the mean. In general, if an observation \\(x\\) is taken from a random process with mean \\(\\mu\\) and standard deviation \\(\\sigma\\), then the \\(z\\)-score is \\[z = \\frac{x -\\mu }{\\sigma}\\]\nThe \\(z\\)-score can be computed for data from any distribution, but it is most commonly applied to normally distributed data.\n\n\n68-95-99.7% Rule for Bell-shaped Distributions\nHeights of women (or men) in a particular population follow a normal distribution. Most people’s heights are close to the mean. A few are very tall or very short. We would like to make a more precise statement than this.\n\nFor any bell-shaped distribution,\n\n68% of the data will lie within 1 standard deviation of the mean,\n95% of the data will lie within 2 standard deviations of the mean, and\n99.7% of the data will lie within 3 standard deviations of the mean.\n\nThis is called the 68-95-99.7% Rule for Bell-shaped Distributions. Some statistics books refer to this as the Empirical Rule. \n\n\n\nApproximately 68% of the observations from a bell-shaped distribution will be between the values of \\(\\mu -~\\sigma~\\)and \\(\\mu +~\\sigma\\). Consider the heights of young adult women in Ghana. We expect that about 68% of Ghanaian women have a height between the values of \\[\\mu -~\\sigma = 159.0 - 4.9 = 154.1~\\text{cm}\\] and \\[\\mu +~\\sigma = 159.0 + 4.9 = 163.9~\\text{cm}.\\]\nSo, if a female is chosen at random from all the young adult women in Ghana, about 68% of those chosen will have a height between 154.1 and 163.9 cm. Similarly, 95% of the women’s heights will be between the values of \\[\\mu - 2\\sigma = 159.0 - 2(4.9) = 149.2~\\text{cm}\\] and \\[\\mu + 2\\sigma = 159.0 + 2(4.9) = 168.8~\\text{cm}.\\]\nFinally, 99.7% of the women’s heights will be between \\[\\mu - 3\\sigma = 159.0 - 3(4.9) = 144.3~\\text{cm}\\] and \\[\\mu + 3\\sigma = 159.0 + 3(4.9) = 173.7~\\text{cm}.\\]\n\n\n\nUnusual Events\nIf a \\(z\\)-score is extreme (either a large positive number or a large negative number), then that suggests that that observed value is very far from the mean. The 68-95-99.7% rule states that 95% of the observed data values will be within two standard deviations of the mean. This means that 5% of the observations will be more than 2 standard deviations away from the mean (either to the left or to the right).\nWe define an unusual observation to be something that happens less than 5% of the time. For normally distributed data, we determine if an observation is unusual based on its \\(z\\)-score. We call an observation unusual if \\(z &lt; -2\\) or if \\(z &gt; 2\\). In other words, we will call an event unusual if the absolute value of its \\(z\\)-score is greater than 2.\n\nAnswer the following questions:\n\n\n\nOut of Serwa and Gustavo, who is physically taller?\n\n\n\nShow/Hide Solution\n\n\nGustavo is taller. He is 182.5 cm tall, and Serwa is 169.0 cm tall.\n\n\n\nRelative to their own gender and nationality, who is relatively taller?\n\n\n\nShow/Hide Solution\n\n\nRelative to other Ghanaian women, Serwa is very tall. Gustavo is tall relative to Brazilian men, but relative to people of his gender and nationality, he is not relatively taller than Serwa. Serwa has a higher \\(z\\)-score.\n\n\n\nAre either of these heights unusual?\n\n\n\nShow/Hide Solution\n\n\nSerwa’s height is unusual. Her \\(z\\)-score is: \\(z = 2.041\\) This is more than two standard deviations away from the mean. Gustavo’s height is not unusual. His \\(z\\)-score is less than two standard deviations away from the mean.\n\n\n\n\n“Piled Higher and Deeper” by Jorge Cham"
  },
  {
    "objectID": "3-Stat_Fundamentals/02-The_Normal_Distribution_Textbook.html#normal-probability-computations",
    "href": "3-Stat_Fundamentals/02-The_Normal_Distribution_Textbook.html#normal-probability-computations",
    "title": "The Normal Distribution (Reading)",
    "section": "Normal Probability Computations",
    "text": "Normal Probability Computations\nAn important part of the practice of statistics is finding areas under a normal curve. The area under a normal curve, say, to the left of a value, gives the probability of obtaining an observation less than (or equal to) that value. This is an example of converting a value to an area. It is also important to convert an area to a value. For example, if you want to find the 40th percentile for data that follow a normal distribution, you find the value of the observation such that the area (under the curve) to the left of this value is 0.40.\n\nIntroduction to the Normal Probability Applet\nThe Normal Probability Applet is a visualization program offering statistics students insights and computations for the relationship between \\(z\\)-scores and areas under the standard normal curve. You can find a link to this applet here. This app is also compatible to use on your phone, iPad, and other mobile devices. The app is stored at . If you want a copy you can open from your desktop, just right click the link and save it to your computer.\nTo use this applet, follow these instructions:\n\nClick on an area below the curved line to shade/unshade the region.\nClick and drag the red lines to adjust the \\(z\\)-scores and obtain the area.\nType a \\(z\\)-score into one of the bottom input boxes and hit “Enter” to get an area.\nType in an area and hit “Enter” to get a \\(z\\)-score.\n\n\n\n\n\n\n\n\n\nConverting a \\(z\\)-score to a Probability\nUsing this applet we can calculate proportions and probabilities based on the area under the normal curve. For the following examples, please open the Normal Probability Applet and practice using it to find areas under the curve.\nThe Normal Probability Applet is nice for visualizing areas under the curve. However, it has significant limitations. In this class, we will use R to do the heavy lifting. The pnorm(x, \\(\\mu\\), \\(\\sigma\\)) function calculates areas under the curve the normal distribution with mean, \\(\\mu\\), and standard deviation, \\(\\sigma\\), for a given x. The p in pnorm() stands for probability and norm obviously stands for the normal distribution.\nBy default, pnorm() gives the area of the curve to the LEFT of the value, \\(x\\), for a normal distribution with a mean, \\(\\mu\\), and standard deviation, \\(\\sigma\\).\nWe can calculate probabilities directly in the original units of the data, or use the z-score with a \\(\\mu=0\\) and \\(\\sigma=1\\): pnorm(z).\nNOTE: pnorm() has a default value for the mean, \\(\\mu = 0\\) and standard deviation, \\(\\sigma = 1\\) so we don’t have to input those values when we use a z-score with the standard normal distribution.\n\nHeights of Ghanaian Women\nWe will use the example of Serwa’s height to find the proportion of young Ghanaian women who are shorter than Serwa. Recall that for the height of young Ghanaian women, the population mean is 159.0 cm and the population standard deviation is 4.9 cm. Serwa’s height is 169.0 cm. We found the \\(z\\)-score of Serwa’s height as:\n\\[z = \\frac{x -\\mu}{\\sigma} = \\frac{169.0- 159.0}{4.9} = 2.041\\]\nWhat proportion of young Ghanaian women reach a height that is at or below 169 cm? To answer this question, we need to find the area under a normal density curve (i.e. the probability) that is to the left of \\(z = 2.041\\).\nTo find the area under a normal curve corresponding to a \\(z\\)-score of \\(2.041\\), do the following:\n\npnorm(2.041)\n\n[1] 0.9793746\n\n\n\n\n\n\n\n\n\n\nNOTE: The area to the left of our chosen \\(z\\)-score is also the probability that a randomly selected woman will be shorter than Serwa. The probability that a randomly selected Ghanaian woman will be shorter than Serwa is \\(0.979\\), or \\(97.9\\%\\).\n*Remember that in this course, unless otherwise specified, we round to three decimal places.\n\n\nBaseball Averages\nWe now return to the example of the baseball batting averages. We want to find the probability that a randomly selected player will have a batting average that is above 0.280. The population mean is 0.261 and the population standard deviation is 0.034. We can use this information to find a \\(z\\)-score. Then we use the applet to find the area under the normal curve to the right of this \\(z\\)-score.\n\\[z = \\frac{x -\\mu}{\\sigma} = \\frac{0.280 - 0.261}{0.034} = 0.559\\]\n\nx &lt;- 0.280\nmu &lt;- 0.261\nsigma &lt;- 0.034\n\nz &lt;- (x-mu)/sigma\n\n# Area to the LEFT:\npnorm(x, mu, sigma)\n\n[1] 0.7118589\n\n## Equivilantly:\npnorm(z)\n\n[1] 0.7118589\n\n# Area to the RIGHT\n1-pnorm(x, mu, sigma)\n\n[1] 0.2881411\n\n1-pnorm(z)\n\n[1] 0.2881411\n\n\nUsing the Applet, type the \\(z\\)-score of \\(0.559\\) in one of the boxes below the horizontal axis in the applet. Click on the areas under the curve until only the region on the right is highlighted in blue.\n\nThe area under the curve to the right of \\(z = 0.559\\) is \\(0.288\\). This is the probability that a randomly selected player will have a batting average that is greater than 0.280. (Note: It is a coincidence that the area, 0.288, is close to the batting average of 0.280. There is no significance in this.)\nNotice that the area shaded in blue above 0.288 is very close to the area we found when we looked at the area represented by the bars of the histogram 0.298 that was shaded in red above.\n\n\n\nFinding the Probability of Being Between Two Values\nThe normal probability applet allows us to find the probability of being between two values as long as they are on opposite sides of the mean and equally distanced from the mean. By calculating two z-scores, one for each value, and then shading the area between them on the applet, we can find the probability. This is severely limiting.\nBelow, we demonstrate how to find the area between any two values in a normal distribution.\n\nHeights of Ghanaian Women\nWhat is the probability that a randomly selected young Ghanaian women will be between 150.0 cm and 163.0 cm tall? Recall that the average height for young Ghanaian women is \\(\\mu=159.0\\) cm and the population standard deviation is \\(\\sigma=4.9\\) cm.\nWe want to find the probability that a randomly selected woman’s height is between \\(150.0\\) cm and \\(165.0\\) cm. To do this we find the \\(z\\)-score for both values:\n\\[z_1 = \\frac{x- \\mu}{\\sigma} = \\frac{150.0 - 159.0}{4.9} = -1.837\\] \\[z_2 = \\frac{x - \\mu}{\\sigma} = \\frac{165.0 -159.0}{4.9} = 1.22\\]\nWe now answer the question by finding the area under the normal density curve (i.e. the probability) to the left of \\(z = 1.22\\) which is \\(0.8888\\) and also the area under the normal density curve to the left of \\(z = -1.837\\) which is \\(0.033\\). To find the area between \\(z = 1.22\\) and \\(z = -1.837\\), we subtract the smaller area from the larger.\n\\[0.8888 - 0.0331 = 0.8557\\]\nSo the probability that the height of a randomly selected young Ghanaian woman will be between 150.0 cm and 165.0 cm is \\(0.8558\\). This is the same as the proportion of all young Ghanaian women who are between 150.0 and 165.0 cm tall.\nTo find the probability of being between any 2 numbers for a normal distribution with mean, \\(\\mu\\), and standard deviation, \\(\\sigma\\), we can use the following R code:\n\nvalue1 &lt;- 150\nvalue2 &lt;- 165\nmu &lt;- 159\nsigma &lt;- 4.9\n\npnorm(value2, mean = mu, sd = sigma) - pnorm(value1, mean = mu, sd = sigma)\n\n[1] 0.8564917\n\n\nNOTE: Value 1 is the lower of the two values and Value 2 is the higher value. Also, this answer is slightly different than above due to when you round. This answer is more accurate because we rounded at the end of the probability calculations rather than rounding the z-scores, then calculating probabilities.\n\n\n\nCalculating Percentiles using a Normal Distribution\nA percentile is a number such that a specified percentage of the population are at or below this number. For example, the 25th percentile is the number in a data set that is greater than or equal to 25% of all the values in the data set.\nWe can find percentiles for a given dataset by using the quantile() function as described in the chapter on summarizing data\nHowever, to calculate percentiles from a normal distribution, we use the qnorm(percentile, mu, sigma) function.\nNOTE: R typically uses the word quantile when referring to percentiles. So the \\(q\\) in qnorm() stands for quantile.\nTo find the height of a Ghanaian woman corresponding to the 90th percentile:\n\nqnorm(.90, mean = 159, sd = 4.9)\n\n[1] 165.2796\n\n\nThe 90th percentile of the heights averages is 165.2796. That means that 90% of the Ghanaian women are shorter than 165.2796 cm tall."
  },
  {
    "objectID": "3-Stat_Fundamentals/02-The_Normal_Distribution_Textbook.html#roughly-assessing-normality-using-a-histogram",
    "href": "3-Stat_Fundamentals/02-The_Normal_Distribution_Textbook.html#roughly-assessing-normality-using-a-histogram",
    "title": "The Normal Distribution (Reading)",
    "section": "Roughly Assessing Normality Using a Histogram",
    "text": "Roughly Assessing Normality Using a Histogram\n\nBaseball Batting Averages\nConsider the data on the batting averages of Major League Baseball players. The histogram of the batting averages showed a distinct bell-shaped curve.\n\nLater in the course, we will learn a better way to assess normality. For now, we use histograms as a rough way to look for symmetry.\n\nR Instructions\n\n\nHere is a refresher of how to make a Histogram in R\nFor more detailed instructions revisit Summarizing Data\n\nRead in the data\n\n\nlibrary(rio)\nlibrary(mosaic)\nbat_avg &lt;- import(\"https://github.com/byuistats/Math221D_Course/raw/main/Data/BattingAverages.xlsx\")\n\n\nUse histogram(data$column_name) where column_name refers to the column you would like to use to create a histogram.\n\n\nhistogram(bat_avg$BattingAvg)\n\n\n\n\n\n\n\n\n\nCustomize your graph to make it presentation worthy by adding labels and modifying colors:\n\n\nhistogram(bat_avg$BattingAvg, main = \"Distribution of Batting Averages\", xlab = \"Batting Average\", ylab = \"\", col = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\nBody Temperatures\nA group of researchers led by Philip A. Mackowiak, MD, conducted a study to assess the true mean body temperatures of healthy adults. They selected n = 148 subjects between the ages of 18 and 40 years old, representative of the general population.\nEach volunteer was given a physical to assure that they were not ill at the time of the data collection. Their axillary (under the arm) body temperature was measured and reported in a paper published in the Journal of the American Medical Association. [1] These data were extracted and are presented in the file BodyTemp. The body temperatures are given in degrees Fahrenheit.\n\nAnswer the following questions:\n\n\n\nMake a histogram of the body temperature data.\n\n\n\nShow/Hide Solution\n\n\nlibrary(rio)\nlibrary(mosaic)\nbody_temp &lt;- import(\"https://github.com/byuistats/Math221D_Course/raw/main/Data/BodyTemp.xlsx\")\nhistogram(body_temp$BodyTemp)\n\n\n\n\n\n\n\n\n\n\nBased on your histogram, what is the shape of the distribution of these data?\n\n\n\nShow/Hide Solution\n\n\nThe data appear to be normally distributed.\n\n\n\nCalculate the following numerical summaries of the data: sample mean, sample standard deviation, and sample size.\n\n\n\nShow/Hide Solution\n\n\nfavstats(body_temp$BodyTemp)\n\n  min   Q1 median   Q3   max     mean        sd   n missing\n 96.2 97.8   98.3 98.7 100.8 98.23446 0.7375924 148       0\n\n\nThe sample mean is 98.23, sample standard deviation is 0.738, and sample size is n = 148."
  },
  {
    "objectID": "3-Stat_Fundamentals/02-The_Normal_Distribution_Textbook.html#summary",
    "href": "3-Stat_Fundamentals/02-The_Normal_Distribution_Textbook.html#summary",
    "title": "The Normal Distribution (Reading)",
    "section": "Summary",
    "text": "Summary\n\nRemember…\n\n\n\nA normal density curve is symmetric and bell-shaped with a mean of \\(\\mu\\) and a standard deviation of \\(\\sigma\\). The curve lies above the horizontal axis and the total area under the curve is equal to 1. A standard normal distribution has a mean of 0 and a standard deviation of 1.\nA z-score is calculated as:\n\n\\[\\displaystyle{z = \\frac{\\text{value}-\\text{mean}}{\\text{standard deviation}} = \\frac{x-\\mu}{\\sigma}}\\]\n\nA z-score tells us how many standard deviations above (\\(+Z\\)) or below (\\(-Z\\)) the mean (\\(\\mu\\)) a given value (\\(x\\)) is.\nTo calculate probabilities for a normal distribution with mean, \\(\\mu\\) and standard deviation \\(\\sigma\\) for a given observation \\(x\\), use pnorm(x, mu, sigma) or 1-pnorm(x, mu, sigma) to get the desired probability (below, above). Alternatively, calculate the \\(z\\)-score and use pnorm(z) or 1-pnorm(z). In every case, the probability is given by the Area under the curve.\nThe 68-95-99.7% rule states that when data are normally distributed, approximately 68% of the population lies within \\(z=1\\) standard deviation (\\(\\sigma\\)) from the mean, approximately 95% of the data lie within \\(z=2\\) standard deviations from the mean, and approximately 99.7% of the data lie within \\(z=3\\) standard deviations from the mean. For example, this rule approximates that 2.5% of observations will be less than a z-score of \\(z=-2\\).\nPercentiles can be calculated using the qnorm(percentile, mu, sigma) function."
  },
  {
    "objectID": "3-Stat_Fundamentals/10-Assessing_Normality.html",
    "href": "3-Stat_Fundamentals/10-Assessing_Normality.html",
    "title": "Assessing Normality",
    "section": "",
    "text": "Assessing Normality\nIn practice, we must confirm that the distribution of sample means is normally distributed. This is true when:\n\nThe population is normally distributed\n\\(n &gt; 30\\) because of the Central Limit Theorem\n\nBut how do you know if a population is normally distributed? In the real world, there is no teacher to tell you when to assume a population is normal.\nIf our sample size is large enough, we don’t have to worry. We can trust the Central Limit Theorem.\nIf our sample size is &lt; 30, we can assess the normality of our sample to decide if we can still trust output of our hypothesis tests and confidence intervals.\nPreviously, we’ve used histograms to help visualize the distribution of a sample. However, when sample sizes are small, even samples from a standard normal distribution can look skewed.\nAll of the examples below are histograms of random samples from actual standard normal distributions:\n\n\nA New Way to Assess Normality\nStatisticians use something called a QQPlot which works better at assessing normality. QQPlots plot the sorted data of each point in a dataset with the theoretical percentile from a normal distribution. If the data and theoretical percentiles line up, then we can be reasonably sure the population is normally distributed.\nThese are easier to use than to explain. We use the car library and the function qqPlot() to create a chart. (Note the Capital P in the middle.)\nKey Point: If most of the data points line up in the shaded region, we can consider the population as normally distributed.\nBelow are examples of QQPlots for a normal distribution and a right skewed distribution.\n\n\nThese work much better for small sample sizes. Below are several examples of QQPlots for small sample sizes:\n\nWhile not perfect, these are a vastly better tool to assess normality than a histogram.\n\n\n\nPractice\nLet’s try assessing the normality of some data. Below are 3 datasets. Find the response variable(s) from each and determine if the data are sufficiently normally distributed:\n\nlibrary(rio)\nlibrary(tidyverse)\nlibrary(car)\n\nold_faithful &lt;- import('https://github.com/byuistats/Math221D_Cannon/raw/master/Data/OldFaithful.xlsx')\n\nqqPlot(old_faithful$Duration)\n\n\n\n\n\n\n\n\n[1] 19 58\n\nqqPlot(old_faithful$Wait)\n\n\n\n\n\n\n\n\n[1] 265 127\n\nrent &lt;- import('https://github.com/byuistats/Math221D_Cannon/raw/master/Data/Rent.csv')\n\n\nmcat_gpa &lt;- import('https://github.com/byuistats/Math221D_Cannon/raw/master/Data/mcat_gpa.csv')"
  },
  {
    "objectID": "4-Statistical_Tests_Part1/01-Inference_for_mean_sigma_unknown.html",
    "href": "4-Statistical_Tests_Part1/01-Inference_for_mean_sigma_unknown.html",
    "title": "Inference for a Mean",
    "section": "",
    "text": "When we know what the population standard deviation, \\(\\sigma\\), for individuals, we can calculate Z-scores and use the Standard Normal Distribution to calculate probabilities. Z has a standard normal distribution, meaning it has a mean of 0 and a standard deviation of 1.\n\\[ z= \\frac{\\bar{x} - \\mu}{\\frac{\\sigma}{\\sqrt{n}}}\\]\nThough rare, there are situations where we might know the population standard deviation from published research or census data. For example, Standardized test organizations publish population-level summaries which would allow us to test how our sample compares to the general population using the Z formula.\n\n\nWhen testing a null hypothesis, the \\(\\mu\\) in the z-score formula becomes the hypothesized population mean. The \\(z\\)-score is then interpreted as the number of standard deviations away from the hypothesized mean. If we know the population standard deviation, \\(\\sigma\\), then we can calculate the probability of getting a sample mean more extreme than the one we observed if the null hypothesis is true.\nKEY DEFINITION: A P-value is the probability of observing a test statistic as extreme, or more extreme than the one we observed in our sample, if the null hypothesis is true.\nWe use “as or more extreme” because the direction (greater than or less than) depends on our alternative hypothesis. In the above example, if I believe that my students scored higher than the general population average, I can say, there is only a 0.0011 chance of getting a test statistic higher than the one I observed if the null hypothesis is true. Because that probability is very low, I am willing to reject the null hypothesis in favor of the alternative that my students scored higher, on average, than the general population.\nEXAMPLE: A factory claims its light bulbs last, on average, \\(\\mu = 800\\) hours with a standard deviation, \\(\\sigma = 40\\). We randomly select 40 light bulbs to test to see if the life expectancy is actually less than that.\nState the Null and alternative Hypotheses:\n\\[ H_0: \\mu \\]\n\\[ H_a: \\mu \\]\nCalculate the Z-score and find the P-value for the test:\nState your conclusion:\n\n\n\nWe can also calculate a confidence interval for the above example. Recall the formula for a confidence interval is, for a given \\(z^*\\) associated with a desired level of confidence:\n\\[ CI = \\bar{x} \\pm z^*\\frac{\\sigma}{\\sqrt{n}}\\]\nRecall the \\(z^*\\) for common confidence levels:\n\nlibrary(tidyverse)\nlibrary(pander)\ntibble(`Conf. Level` = c(0.99, 0.95, 0.90), `Z*` = c(2.576, 1.96, 1.645)) %&gt;% pander()\n\n\n\n\n\n\n\n\nConf. Level\nZ*\n\n\n\n\n0.99\n2.576\n\n\n0.95\n1.96\n\n\n0.9\n1.645"
  },
  {
    "objectID": "4-Statistical_Tests_Part1/01-Inference_for_mean_sigma_unknown.html#hypothesis-testing",
    "href": "4-Statistical_Tests_Part1/01-Inference_for_mean_sigma_unknown.html#hypothesis-testing",
    "title": "Inference for a Mean",
    "section": "",
    "text": "When testing a null hypothesis, the \\(\\mu\\) in the z-score formula becomes the hypothesized population mean. The \\(z\\)-score is then interpreted as the number of standard deviations away from the hypothesized mean. If we know the population standard deviation, \\(\\sigma\\), then we can calculate the probability of getting a sample mean more extreme than the one we observed if the null hypothesis is true.\nKEY DEFINITION: A P-value is the probability of observing a test statistic as extreme, or more extreme than the one we observed in our sample, if the null hypothesis is true.\nWe use “as or more extreme” because the direction (greater than or less than) depends on our alternative hypothesis. In the above example, if I believe that my students scored higher than the general population average, I can say, there is only a 0.0011 chance of getting a test statistic higher than the one I observed if the null hypothesis is true. Because that probability is very low, I am willing to reject the null hypothesis in favor of the alternative that my students scored higher, on average, than the general population.\nEXAMPLE: A factory claims its light bulbs last, on average, \\(\\mu = 800\\) hours with a standard deviation, \\(\\sigma = 40\\). We randomly select 40 light bulbs to test to see if the life expectancy is actually less than that.\nState the Null and alternative Hypotheses:\n\\[ H_0: \\mu \\]\n\\[ H_a: \\mu \\]\nCalculate the Z-score and find the P-value for the test:\nState your conclusion:"
  },
  {
    "objectID": "4-Statistical_Tests_Part1/01-Inference_for_mean_sigma_unknown.html#confidence-interval",
    "href": "4-Statistical_Tests_Part1/01-Inference_for_mean_sigma_unknown.html#confidence-interval",
    "title": "Inference for a Mean",
    "section": "",
    "text": "We can also calculate a confidence interval for the above example. Recall the formula for a confidence interval is, for a given \\(z^*\\) associated with a desired level of confidence:\n\\[ CI = \\bar{x} \\pm z^*\\frac{\\sigma}{\\sqrt{n}}\\]\nRecall the \\(z^*\\) for common confidence levels:\n\nlibrary(tidyverse)\nlibrary(pander)\ntibble(`Conf. Level` = c(0.99, 0.95, 0.90), `Z*` = c(2.576, 1.96, 1.645)) %&gt;% pander()\n\n\n\n\n\n\n\n\nConf. Level\nZ*\n\n\n\n\n0.99\n2.576\n\n\n0.95\n1.96\n\n\n0.9\n1.645"
  },
  {
    "objectID": "4-Statistical_Tests_Part1/01-Inference_for_mean_sigma_unknown.html#calculating-p-values-by-hand-with-the-t-distribution-one-out-of-ten.-would-not-recommended",
    "href": "4-Statistical_Tests_Part1/01-Inference_for_mean_sigma_unknown.html#calculating-p-values-by-hand-with-the-t-distribution-one-out-of-ten.-would-not-recommended",
    "title": "Inference for a Mean",
    "section": "Calculating P-values by hand with the T Distribution (One out of ten. Would NOT RECOMMENDED)",
    "text": "Calculating P-values by hand with the T Distribution (One out of ten. Would NOT RECOMMENDED)\nSuppose we have 25 test scores defined as “data” in the code chunk below. We can calculate the \\(t\\)-statistic just like we did with the \\(z\\)-score.\nSuppose we believe that the mean score of these 25 students is significantly higher than 50. Our null and alternative hypothesis are as follows:\n\\[ H_0:  \\mu = 50 \\] \\[ H_a: \\mu &gt; 50 \\]\n\n# we can use the t-distribution, pt(), just like pnorm() but must also add the degrees of freedom\n\ndata &lt;- c(88,81,27,92,46,79,67,44,46,88,21,60,71,81,79,52,100,44,42,58,52,48,83,65,98)\n\n# Hypothesized Mean:\nmu_0 &lt;- 50\n\n# Sample size, sample Mean and sample SD\n#  The length function tells us how many data points are in the list.  \nn &lt;- length(data)\nxbar &lt;- mean(data)\ns &lt;- sd(data)\n\ns_xbar = s / sqrt(n)\n\nt &lt;- (xbar - mu_0) / s_xbar\n\n# Probability of getting a test statistic at least as extreme as the one we observed if the null hypothesis is true\n1-pt(t, n-1)\n\n[1] 0.00151866\n\n\nThis means that if the true population mean was 50, then there is only a 0.00152 probability of observing a test statistic, t, as high as the one we got (P-value).\nThe good news is that the more complicated the math becomes, the less of it we have to do! Instead of using R like a calculator to calculate \\(z\\) or \\(t\\)-scores and calculating probabilities “by hand” (using pnorm() or pt()), we can use R functions with the data directly and get much more useful output."
  },
  {
    "objectID": "4-Statistical_Tests_Part1/01-Inference_for_mean_sigma_unknown.html#step-1-read-in-the-data",
    "href": "4-Statistical_Tests_Part1/01-Inference_for_mean_sigma_unknown.html#step-1-read-in-the-data",
    "title": "Inference for a Mean",
    "section": "Step 1: Read in the data",
    "text": "Step 1: Read in the data\n\n# Load Libraries\n\nlibrary(tidyverse)\nlibrary(mosaic)\nlibrary(rio)\nlibrary(car)\n\n\n# Read in data\nbig5 &lt;- import('https://raw.githubusercontent.com/byuistats/Math221D_Cannon/master/Data/All_class_combined_personality_data.csv')"
  },
  {
    "objectID": "4-Statistical_Tests_Part1/01-Inference_for_mean_sigma_unknown.html#step-2-review-the-data",
    "href": "4-Statistical_Tests_Part1/01-Inference_for_mean_sigma_unknown.html#step-2-review-the-data",
    "title": "Inference for a Mean",
    "section": "Step 2: Review the Data",
    "text": "Step 2: Review the Data\nIn this step, we are looking to see if the data are as expected. Are the columns we’re interested in numeric? Categorical? and do these match expectations. We can also start to look for strange data and outliers. Visualizations can help.\nOther common issues to look for include: negative numbers that should only be positive, date values that shouldn’t exist, missing values, character variables inside what should be a number.\nIn the real world, data are messy. Reviewing the data is a critical part of an analysis.\nTake a look through the personality dataset and see if there are any anomalies that might need to be addressed."
  },
  {
    "objectID": "4-Statistical_Tests_Part1/01-Inference_for_mean_sigma_unknown.html#step-3-visulize-the-data",
    "href": "4-Statistical_Tests_Part1/01-Inference_for_mean_sigma_unknown.html#step-3-visulize-the-data",
    "title": "Inference for a Mean",
    "section": "Step 3: Visulize the data",
    "text": "Step 3: Visulize the data\nSo far we have been discussing a single, quantitative variable of interest, like test scores, reaction times, heights, etc. When we start looking at more complicated data, we will expand our repertoire of visualizations, but Histograms are very good when looking at one variable at a time, and boxplots are very good for comparison between groups.\nCreate a histogram of Extroversion. Describe some features of the data. Is it symmetric? Skewed? Are there outliers?\n\n# Basic Graph\nhistogram(big5$Extroversion)\n\n\n\n\n\n\n\n# Improved graph\nhistogram(big5$Extroversion, main = \"Extroversion Scores\", xlab = \"Extroversion\")"
  },
  {
    "objectID": "4-Statistical_Tests_Part1/01-Inference_for_mean_sigma_unknown.html#step-4-perform-the-appropriate-analysis",
    "href": "4-Statistical_Tests_Part1/01-Inference_for_mean_sigma_unknown.html#step-4-perform-the-appropriate-analysis",
    "title": "Inference for a Mean",
    "section": "Step 4: Perform the appropriate analysis",
    "text": "Step 4: Perform the appropriate analysis\nIn this example, we will be testing the hypothesis that Brother Cannon’s students are similar to the general population. We suspect that these youthful BYU-I students are, on average, more extroverted.\n\nHypothesis Test\nWrite out the Null and alternative hypotheses.\n\\[ H_0: \\mu_{extroversion} = 50\\]\n\\[ H_A: \\mu_{extroversion}  &gt; 50\\]\n\\[\\alpha = 0.05\\]\nPerform the one-sample t-test using the “t.test()” function in R. If you ever get stuck remembering how to use a function in R, you can run ?t.test to see documentation. The question mark will open up the help files for any given function in R.\nThe t.test() function takes as input, the data, the hypothesized mean, mu, and the direction of the alternative hypothesis.\nThe default parameters for the t.test() function are: t.test(data, mu = 0, alternative = \"two.sided\").\n\n#?t.test\n\n# One-sided Hypothesis Test\nt.test(big5$Extroversion, mu = 50, alternative = \"greater\")\n\n\n    One Sample t-test\n\ndata:  big5$Extroversion\nt = 6.6529, df = 403, p-value = 4.697e-11\nalternative hypothesis: true mean is greater than 50\n95 percent confidence interval:\n 55.25232      Inf\nsample estimates:\nmean of x \n 56.98267 \n\n\nQuestion: What is the P-value?\nAnswer:\nQuestion: How do we explain our conclusion in context of our research question?\nAnswer:\nTechnical Conclusion:\nContextual Conclusion:\n\n\nConfidence Intervals\nWe can also use the t.test() function to create confidence intervals. Recall that confidence intervals are always 2-tailed. Confidence intervals are typically written in the form: (lower limit, upper limit).\n\n# Confidence Intervals are by definition 2-tailed\n# We can also change the confidence level\n\nt.test(big5$Extroversion, mu = 50, alternative = \"two.sided\", conf.level = .99)\n\n\n    One Sample t-test\n\ndata:  big5$Extroversion\nt = 6.6529, df = 403, p-value = 9.394e-11\nalternative hypothesis: true mean is not equal to 50\n99 percent confidence interval:\n 54.26631 59.69903\nsample estimates:\nmean of x \n 56.98267 \n\n\nTo get only the output for the confidence interval to be shown, we can use a $ to select specific output. Much like when pulling a specific column from a dataset, the $ can pull specific output from an analysis.\nBecause the option for the alternative = in the t.test function is “two.sided”, we don’t actually need to include it when getting confidence intervals.\nAlso, confidence intervals do not require an assumed mu value. So a more efficient way to get a confidence interval for a given set of data is:\n\nt.test(big5$Extroversion, conf.level = .99)$conf.int\n\n[1] 54.26631 59.69903\nattr(,\"conf.level\")\n[1] 0.99\n\n\nQuestion: Describe in words the interpretation of the confidence interval in context of Extroversion.\nAnswer:"
  },
  {
    "objectID": "4-Statistical_Tests_Part1/01-Inference_for_mean_sigma_unknown.html#body-temperature-data",
    "href": "4-Statistical_Tests_Part1/01-Inference_for_mean_sigma_unknown.html#body-temperature-data",
    "title": "Inference for a Mean",
    "section": "Body Temperature Data",
    "text": "Body Temperature Data\nThe dataset below contains information about body temperatures of healthy adults.\n\nLoad the data:\n\n# These lines load the data into the data frame body_temp:\n\nbody_temp &lt;- import(\"https://byuistats.github.io/M221R/Data/body_temp.xlsx\")\n\n\n\nReview the Data\nCreate a table of summary statistics:\n\n\nVisualize the Data\nCreate a histogram to visualize the body temperature data.\nQuestion: Describe the general shape of the distribution.\nAnswer:\n\n\nAnalyze the Data\nIt’s widely accepted that normal body temperature for healthy adults is 98.6 degrees Fahrenheit.\nSuppose we suspect that the average temperature is different than 98.6\nUse a significance level of \\(\\alpha = 0.01\\) to test whether the mean body temperature of healthy adults is equal to 98.6 degrees Fahrenheit.\nQuestion: What is the P-value?\nAnswer:\nExplain our conclusion.\n\nConfidence Interval\nCreate a 99% confidence interval for the true population average temperature of healthy adults."
  },
  {
    "objectID": "4-Statistical_Tests_Part1/02-AA_Hypothesis_Conf_Int.html",
    "href": "4-Statistical_Tests_Part1/02-AA_Hypothesis_Conf_Int.html",
    "title": "Don’t Take it Personally",
    "section": "",
    "text": "Introduction\nIn this activity, you will execute statistical hypothesis tests and generate confidence intervals for each of the Big 5 personality traits using data collected from a random sample of Brother Cannon’s Math 221 students.\nQuestion: What is the population of this analysis?\nAnswer:\nFor each personality trait, include:\n\nA statement of the null and alternative hypotheses and why you chose the alternative you did.\nChoose alpha, $= $\nCheck that you can trust the normality of the mean (n &gt; 30 or qqPlot(respons_variable))\nRun the one-sample t-test and state your conclusion (technical and contextual explanation)\nCalculate a \\(1-\\alpha\\) level confidence interval and describe in words what it means in context of the research question\n\nRecall that we can use favstats() to get summary statistics, boxplot() and histogram() to get visualizations, and the t.test() function to get hypothesis tests and confidence intervals. Be sure to label your plots’ axes and include a title.\n\n# Load Libraries\n\nlibrary(tidyverse)\nlibrary(mosaic)\nlibrary(rio)\nlibrary(car)\n\n\n# Load Data\nbig5 &lt;- import('https://raw.githubusercontent.com/byuistats/Math221D_Cannon/master/Data/All_class_combined_personality_data.csv')\n\n\n\nExtroversion\nState your null and alternative hypotheses:\n\\[H_o:  \\mu = 50\\]\n\\[H_a:  \\mu &gt; 50\\]\n\\[\\alpha = 0.025 \\]\n1. Create a table of summary statistics:\n\n# Extroversion\n\nfavstats(big5$Extroversion) %&gt;% knitr::kable()\n\n\n\n\n\nmin\nQ1\nmedian\nQ3\nmax\nmean\nsd\nn\nmissing\n\n\n\n\n\n1\n42\n58\n73\n100\n56.98267\n21.09599\n404\n1\n\n\n\n\n\n\nCreate a histogram of Extroversion:\n\n\n### I'm making a single variable called \"extrov\" that drops the missing values.  You can do the same thing with the other traits to make analysis a little easier\nextrov &lt;- na.omit(big5$Extroversion) \n\nhistogram(extrov, xlab = \"Extroversion\", main = \"Histogram of Extroversion Percentiles\")\n\n\n\n\n\n\n\n\n\nPerform the one-sample t.test:\n\n\n# For Hypothesis Test:\nt.test(extrov, mu = 50, alternative = \"greater\")\n\n\n    One Sample t-test\n\ndata:  extrov\nt = 6.6529, df = 403, p-value = 4.697e-11\nalternative hypothesis: true mean is greater than 50\n95 percent confidence interval:\n 55.25232      Inf\nsample estimates:\nmean of x \n 56.98267 \n\n\n\nExplain your conclusion:\n\nTechnical: Because the p-value is less than \\(\\alpha\\), I reject the null hypothesis.\nContextual: I have sufficient evidence to suggest that Brother Cannon’s students are, on average, more Extroverted than the general population.\n\nCreate a Confidence Interval for the average extroversion of Brother Cannon’s students:\n\n\n# For a Confidence Interval:\nt.test(extrov, conf.level = 1-.025)$conf.int\n\n[1] 54.62135 59.34399\nattr(,\"conf.level\")\n[1] 0.975\n\n\n\nExplain your confidence interval:\n\nI am 97.5% confident that the true average extroversion of Brother Cannon’s students is somewhere between the 54.62 and 59.34 percentiles.\n\n\nAgreeableness\nState your null and alternative hypotheses. For example, do you think Brother Cannon’s students are more, less, or just different than the general population?\n\\[H_o:  \\mu  \\]\n\\[H_a:  \\mu \\]\n\\[\\alpha =  \\]\n1. Create a table of summary statistics for Agreeableness:\n\nCreate a histogram of Agreeableness:\n\n\nPerform the one-sample t.test:\n\n\nExplain your conclusion:\n\nTechnical:\nContextual:\n\nCreate a Confidence Interval for the average agreeableness of Brother Cannon’s students:\n\n\n# For a Confidence Interval:\nt.test()$conf.int\n\nError in t_test.default(): argument \"x\" is missing, with no default\n\n\n\nExplain your confidence interval:\n\n\n\nOpenness\nState your null and alternative hypotheses:\n\\[H_o:  \\mu \\]\n\\[H_a:  \\mu \\]\n\\[\\alpha = \\]\n\nCreate a table of summary statistics for Openness:\n\n\nCreate a histogram of Openness:\n\n\nPerform the one-sample t.test:\n\n\nExplain your conclusion:\n\nTechnical:\nContextual:\n\nCreate a Confidence Interval for the average openness of Brother Cannon’s students:\n\n\n# For a Confidence Interval:\nt.test()$conf.int\n\nError in t_test.default(): argument \"x\" is missing, with no default\n\n\n\nExplain your confidence interval:\n\n\n\nNeuroticism\nState your null and alternative hypotheses:\n\\[H_o:  \\mu \\]\n\\[H_a:  \\mu \\]\n\\[\\alpha =  \\]\n1. Create a table of summary statistics for Neuroticism:\n\nCreate a histogram of Neuroticism:\n\n\nPerform the one-sample t.test:\n\n\nExplain your conclusion:\n\nTechnical:\nContextual:\n\nCreate a Confidence Interval for the average neuroticism of Brother Cannon’s students:\n\n\n# For a Confidence Interval:\nt.test()$conf.int\n\nError in t_test.default(): argument \"x\" is missing, with no default\n\n\n\nExplain your confidence interval:\n\n\n\nConscientiousness\nState your null and alternative hypotheses:\n\\[H_o:  \\mu \\]\n\\[H_a:  \\mu \\]\n\\[\\alpha = \\]\n1. Create a table of summary statistics:\n\nCreate a histogram of Conscientiousness:\n\n\nPerform the one-sample t.test:\n\n\nExplain your conclusion:\n\nTechnical:\nContextual:\n\nCreate a Confidence Interval for the average conscientiousness of Brother Cannon’s students:\n\n\n# For a Confidence Interval:\nt.test()$conf.int\n\nError in t_test.default(): argument \"x\" is missing, with no default\n\n\n\nExplain your confidence interval:"
  },
  {
    "objectID": "4-Statistical_Tests_Part1/03-Paired_ttest.html",
    "href": "4-Statistical_Tests_Part1/03-Paired_ttest.html",
    "title": "Paired T-test Class Notes",
    "section": "",
    "text": "A matched pairs design is used in statistics to compare 2 treatments or conditions measured on subjects that are logically connected in a meaningful way. In the simplest case, measurements are collected on the same subject as in a before-and-after evaluation.\nMatched pairs designs are often called “dependent samples” because knowing who or what is in the first treatment group determines who will be in the second. In the case of a before-and-after situation, if you are selected to be in the “pre” group, then you will also be in the “post” group. But pairs are not always the same subjects.\n\n\n\nYou want to study the difference in salaries between husbands and wives. If one spouse is selected for the study it automatically determines that the other spouse will be in the other group.\nAn ACT preparation course gives you a test before you take the course and after to see if the course improved test score\nA weight loss program takes your weight at the beginning and after the 12 weeks in the program to see if the program reduced weight.\nComparing prices of a specific set of items between Walmart and Broulims. Because we are comparing the same items, we can take the difference between prices for each item.\n\n\n\n\nAs with the one-sample t-test, we have to make sure that the either the pairs are normally distributed or we have a large enough sample size.\nWith smaller sample sizes, create a qqPlot() using the car library to check for normality.\n\n\n\nWhen we perform a matched pairs analysis, we will be doing a 1-sample t-test on the differences between the connected observations. One challenge is that we can define the difference either way: before - after or after - before.\nA good practice is to define differences so that a negative number means “loss” and a positive number means “gain”.\nFor example, if we believe our weight loss program reduces weight, then defining post_weight - pre_weight should give a negative number, meaning weight lost during the program.\nIf you believe Walmart is cheaper than Broulims, defining the difference Broulims - Walmart gives a positive number, meaning how much you can save, on average, for shopping at Walmart.\nMathematically, it doesn’t matter which way we define the difference as long as we keep track of what a negative number and a positive number mean. This will define which alternative hypothesis we use."
  },
  {
    "objectID": "4-Statistical_Tests_Part1/03-Paired_ttest.html#examples",
    "href": "4-Statistical_Tests_Part1/03-Paired_ttest.html#examples",
    "title": "Paired T-test Class Notes",
    "section": "",
    "text": "You want to study the difference in salaries between husbands and wives. If one spouse is selected for the study it automatically determines that the other spouse will be in the other group.\nAn ACT preparation course gives you a test before you take the course and after to see if the course improved test score\nA weight loss program takes your weight at the beginning and after the 12 weeks in the program to see if the program reduced weight.\nComparing prices of a specific set of items between Walmart and Broulims. Because we are comparing the same items, we can take the difference between prices for each item."
  },
  {
    "objectID": "4-Statistical_Tests_Part1/03-Paired_ttest.html#requirements-for-a-matched-pairs-analysis",
    "href": "4-Statistical_Tests_Part1/03-Paired_ttest.html#requirements-for-a-matched-pairs-analysis",
    "title": "Paired T-test Class Notes",
    "section": "",
    "text": "As with the one-sample t-test, we have to make sure that the either the pairs are normally distributed or we have a large enough sample size.\nWith smaller sample sizes, create a qqPlot() using the car library to check for normality."
  },
  {
    "objectID": "4-Statistical_Tests_Part1/03-Paired_ttest.html#watchouts",
    "href": "4-Statistical_Tests_Part1/03-Paired_ttest.html#watchouts",
    "title": "Paired T-test Class Notes",
    "section": "",
    "text": "When we perform a matched pairs analysis, we will be doing a 1-sample t-test on the differences between the connected observations. One challenge is that we can define the difference either way: before - after or after - before.\nA good practice is to define differences so that a negative number means “loss” and a positive number means “gain”.\nFor example, if we believe our weight loss program reduces weight, then defining post_weight - pre_weight should give a negative number, meaning weight lost during the program.\nIf you believe Walmart is cheaper than Broulims, defining the difference Broulims - Walmart gives a positive number, meaning how much you can save, on average, for shopping at Walmart.\nMathematically, it doesn’t matter which way we define the difference as long as we keep track of what a negative number and a positive number mean. This will define which alternative hypothesis we use."
  },
  {
    "objectID": "4-Statistical_Tests_Part1/03-Paired_ttest.html#step-1-read-in-data",
    "href": "4-Statistical_Tests_Part1/03-Paired_ttest.html#step-1-read-in-data",
    "title": "Paired T-test Class Notes",
    "section": "Step 1: Read in Data",
    "text": "Step 1: Read in Data\n\n# Load Libraries\n\nlibrary(tidyverse)\nlibrary(mosaic)\nlibrary(rio)\nlibrary(car)\n\n\n# Load Data\nweight_loss &lt;- import(\"https://byuistats.github.io/M221R/Data/weight_loss.xlsx\")"
  },
  {
    "objectID": "4-Statistical_Tests_Part1/03-Paired_ttest.html#step-2-explore-the-data-and-generate-hypotheses",
    "href": "4-Statistical_Tests_Part1/03-Paired_ttest.html#step-2-explore-the-data-and-generate-hypotheses",
    "title": "Paired T-test Class Notes",
    "section": "Step 2: Explore the Data and Generate Hypotheses",
    "text": "Step 2: Explore the Data and Generate Hypotheses\nCreate histograms summary statistics for the pre and post weight measurements:\n\nglimpse(weight_loss)\n\nRows: 27\nColumns: 4\n$ subject  &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18…\n$ pre      &lt;dbl&gt; 62.5, 88.8, 74.7, 98.6, 78.1, 73.8, 87.0, 62.1, 60.7, 76.2, 8…\n$ post     &lt;dbl&gt; 56.1, 80.2, 70.8, 97.0, 69.1, 63.8, 81.9, 53.3, 56.5, 71.8, 7…\n$ comments &lt;chr&gt; \"Data collected by Annie Mahon in a\", \"weight loss study. Sub…\n\n# Pre-weight histogram\nhistogram(weight_loss$pre)\n\n\n\n\n\n\n\nfavstats(weight_loss$pre)\n\n  min    Q1 median   Q3  max mean       sd  n missing\n 60.7 72.75     77 80.7 99.6 76.9 10.20671 27       0\n\n# Post-weight histogram\nhistogram(weight_loss$post)\n\n\n\n\n\n\n\nfavstats(weight_loss$post)\n\n  min    Q1 median   Q3 max    mean       sd  n missing\n 53.3 63.45   69.9 75.2  97 70.0963 10.63273 27       0"
  },
  {
    "objectID": "4-Statistical_Tests_Part1/03-Paired_ttest.html#step-3-prepare-the-data-for-analysis",
    "href": "4-Statistical_Tests_Part1/03-Paired_ttest.html#step-3-prepare-the-data-for-analysis",
    "title": "Paired T-test Class Notes",
    "section": "Step 3: Prepare the data for analysis",
    "text": "Step 3: Prepare the data for analysis\nDecide how you’re going to define the difference (\\(post - pre\\) or \\(pre - post\\)).\nQuestion: What does a negative number mean based on your definition?\nAnswer:\n\n# Decide which column to subtract from the other\ndiff &lt;- weight_loss$post - weight_loss$pre\n\nCreate a histogram and a qqPlot of the differences to determine if you will be able to trust the statsitical analyses:\n\n# histogram of the differences\nhistogram(diff)\n\n\n\n\n\n\n\nfavstats(diff)\n\n   min   Q1 median   Q3  max      mean       sd  n missing\n -13.6 -8.9   -6.7 -4.2 -1.6 -6.803704 3.172051 27       0\n\n# Check if the differences are normally distributed:\nqqPlot(diff)\n\n\n\n\n\n\n\n\n[1] 14 18"
  },
  {
    "objectID": "4-Statistical_Tests_Part1/03-Paired_ttest.html#step-4-perform-the-appropriate-analysis",
    "href": "4-Statistical_Tests_Part1/03-Paired_ttest.html#step-4-perform-the-appropriate-analysis",
    "title": "Paired T-test Class Notes",
    "section": "Step 4: Perform the appropriate analysis",
    "text": "Step 4: Perform the appropriate analysis\n\nHypothesis Test\nState your null and alternative hypotheses.\n\\[ H_0: \\mu_{differences} = 0\\]\n\\[H_a:  \\mu_{differences} &lt; 0\\]\n\\[ \\alpha = 0.01\\]\nQuestions to consider:\n\nHow did you define your difference?\nBased on your decision for the difference, what does a negative number mean? a positive number?\nAre you expecting the difference to be greater than, less than, or not equal to 0?\n\nNOTE: One way to verify that you have used the correct alternative is to look at the difference as defined (weight_loss$post - weight_loss$pre) and decide which one you think is supposed to be bigger. Swap out the subtraction sign for the inequality that makes sense (weight_loss$post &lt; weight_loss$pre). Therefore, your alternative should be “less” because \\(&lt;\\) corresponds to “less than”.\nPerform a t-test of the differences:\n\n# Hypothesis t.test()\nt.test(diff, mu = 0, alternative = \"less\")\n\n\n    One Sample t-test\n\ndata:  diff\nt = -11.145, df = 26, p-value = 1.059e-11\nalternative hypothesis: true mean is less than 0\n95 percent confidence interval:\n     -Inf -5.76249\nsample estimates:\nmean of x \n-6.803704 \n\n\nState your conclusion in context of the research question:\nBecause P-value &lt; 0.01 we reject the null hypothesis. We have SUFFICIENT evidence to suggest that participation in the weight loss program led to weight loss.\n\n\nConfidence Interval\n\n# Confidence interval using t.test()\nt.test(diff, conf.level = .99)$conf.int\n\n[1] -8.500002 -5.107405\nattr(,\"conf.level\")\n[1] 0.99\n\n\nI am 99% confident that the true average weight loss during the program was between -8.5 and -5.11 kilograms."
  },
  {
    "objectID": "4-Statistical_Tests_Part1/03-Paired_ttest.html#step-1-read-in-data-1",
    "href": "4-Statistical_Tests_Part1/03-Paired_ttest.html#step-1-read-in-data-1",
    "title": "Paired T-test Class Notes",
    "section": "Step 1: Read in Data",
    "text": "Step 1: Read in Data\n\ncholesterol &lt;- import(\"https://byuistats.github.io/M221R/Data/quiz/R/cholesterol.csv\")"
  },
  {
    "objectID": "4-Statistical_Tests_Part1/03-Paired_ttest.html#step-2-check-data",
    "href": "4-Statistical_Tests_Part1/03-Paired_ttest.html#step-2-check-data",
    "title": "Paired T-test Class Notes",
    "section": "Step 2: Check Data",
    "text": "Step 2: Check Data\nReview the data to see if the variables look correct. Calculate summary statistics and histograms for chol_day2 and chol_day4."
  },
  {
    "objectID": "4-Statistical_Tests_Part1/03-Paired_ttest.html#step-3-prepare-data-for-analysis",
    "href": "4-Statistical_Tests_Part1/03-Paired_ttest.html#step-3-prepare-data-for-analysis",
    "title": "Paired T-test Class Notes",
    "section": "Step 3: Prepare Data for Analysis",
    "text": "Step 3: Prepare Data for Analysis\nDecide how to define your difference. What does a negative number indicate? a positive number?\nCreate a histogram and summary statistics of the differences:\nBecause we \\(n &lt; 30\\) we need to check the qqPlot() to assess the normality of the differences."
  },
  {
    "objectID": "4-Statistical_Tests_Part1/03-Paired_ttest.html#step-4-perform-the-appropriate-analysis-1",
    "href": "4-Statistical_Tests_Part1/03-Paired_ttest.html#step-4-perform-the-appropriate-analysis-1",
    "title": "Paired T-test Class Notes",
    "section": "Step 4: Perform the appropriate analysis",
    "text": "Step 4: Perform the appropriate analysis\n\nHypothesis Test\nState your null and alternative hypotheses.\nWhat is your confidence level, ($1-= $)?\nPerform a matched pairs t-test for the difference in cholesterol at day 2 and day 4:\nState your conclusion:\n\n\nConfidence Interval\nCalculate a confidence interval for the average difference.\nState your conclusions and interpret the confidence interval in context of the research question."
  },
  {
    "objectID": "4-Statistical_Tests_Part1/05-Independent_2_sample_ttest.html",
    "href": "4-Statistical_Tests_Part1/05-Independent_2_sample_ttest.html",
    "title": "Independent Two-sample T-test",
    "section": "",
    "text": "In many situations we would like to compare averages from different populations. In these situations, we take 2 random samples from each population and perform statistical tests to determine if the population means are significantly different. Because these two groups of individuals are sampled independently, we call this analysis Independent 2-Sample t-test.\nAlternatively, in many experimental designs, participants are randomly assigned into a treatment and a control group. The randomization process ensures that there is no association between participants in either group. They are independent.\nWhen 2 random samples are taken from 2 separate populations, or when a group of people are randomly assigned into treatment groups, the samples are independent.\nThis differs from the dependent t-test. Recall that samples are dependent when knowing who or what is in one group determines who or what is in the second group.\nSome examples include:\n\nComparing salaries of men and women (randomly sample men and women separately)\nTesting a new medication compared to a placebo (participants randomly assigned to treatment groups)\nComparing average GPA of Math majors and Economics majors (randomly select from each population)"
  },
  {
    "objectID": "4-Statistical_Tests_Part1/05-Independent_2_sample_ttest.html#independent-t-test-in-r",
    "href": "4-Statistical_Tests_Part1/05-Independent_2_sample_ttest.html#independent-t-test-in-r",
    "title": "Independent Two-sample T-test",
    "section": "Independent t-test in R",
    "text": "Independent t-test in R\nA 2-sample independent t-test in R requires a slight modification to the 1-sample and dependent t-tests already performed. The syntax should look familiar.\nRecall that when we created a boxplot() or did favstats() for one set of data it looked like:\n\nboxplot(data$response_variable)\n\nError in data$response_variable: object of type 'closure' is not subsettable\n\nfavstats(data$response_variable)\n\nError in favstats(data$response_variable): could not find function \"favstats\"\n\n\nwith data$response_variable corresponding to our quantitative variable of interest.\nWhen we wanted to break the analysis down by a grouping factor we used the ~ notation to add a group variable:\n\nboxplot(data$response_variable ~ data$grouping_variable)\n\nError in data$response_variable: object of type 'closure' is not subsettable\n\nfavstats(data$response_variable ~ data$grouping_variable)\n\nError in favstats(data$response_variable ~ data$grouping_variable): could not find function \"favstats\"\n\n\nWe use the exact same modification for a t-test with 2 groups:\n\nt.test(data$response_variable ~ data$grouping_variable, alterntive = \"greater\")\n\nError in data$response_variable: object of type 'closure' is not subsettable\n\n\nRecall that the t-test() function uses mu=0 as a default, we do not need to specify it in the function because the null value when comparing 2 groups is 0.\nNOTE: In R, group 1 and 2 are determined alphabetically according to the labels in the dataset."
  },
  {
    "objectID": "4-Statistical_Tests_Part1/05-Independent_2_sample_ttest.html#confidence-intervals",
    "href": "4-Statistical_Tests_Part1/05-Independent_2_sample_ttest.html#confidence-intervals",
    "title": "Independent Two-sample T-test",
    "section": "Confidence Intervals",
    "text": "Confidence Intervals\nRecall that confidence intervals are necessarily two-sided. So the code for a 99% confidence interval looks like:\n\nt.test(data$response_variable ~ data$grouping_variable, conf.level = .99)$conf.int\n\nError in data$response_variable: object of type 'closure' is not subsettable\n\n\nWe interpret a confidence interval for the difference of means as follows:\n\nI am 99% confident that the true difference of the means is between [lower limit] and [upper limit].\n\nWe can usually do better within the context of a research question:\n\nClass 1 did, on average, between 3.21 and 5.67 percent better than class 2 on the last exam.\n\nStore A outperforms Store B by between $27,022 and $36,977 on average"
  },
  {
    "objectID": "4-Statistical_Tests_Part1/05-Independent_2_sample_ttest.html#step-1-read-in-data",
    "href": "4-Statistical_Tests_Part1/05-Independent_2_sample_ttest.html#step-1-read-in-data",
    "title": "Independent Two-sample T-test",
    "section": "Step 1: Read in Data",
    "text": "Step 1: Read in Data\n\n# Load Libraries\n\nlibrary(tidyverse)\nlibrary(mosaic)\nlibrary(rio)\nlibrary(car)\n\n\n# Load Data\n\nfifa_heart_attacks &lt;- import(\"https://byuistats.github.io/M221R/Data/fifa_heart_attacks.xlsx\")"
  },
  {
    "objectID": "4-Statistical_Tests_Part1/05-Independent_2_sample_ttest.html#step-2-review-data",
    "href": "4-Statistical_Tests_Part1/05-Independent_2_sample_ttest.html#step-2-review-data",
    "title": "Independent Two-sample T-test",
    "section": "Step 2: Review Data",
    "text": "Step 2: Review Data\nLook at the data.\nCreate summary statistics tables of the number of heart attacks for each group.\nCreate a side-by-side boxplot for the during the World Cup and the Control.\nDo you notice any outliers or data that may need to be omitted for analysis?\nCheck to see if the means from both groups are normally distributed:\n\nIs n &gt; 30 for both groups?\nCreate a qqPlot()\n\n\nqqPlot(fifa_heart_attacks$heart_attacks, groups = fifa_heart_attacks$time_period)\n\n\n\n\n\n\n\n\nCan we trust that the central limit theorem applies?"
  },
  {
    "objectID": "4-Statistical_Tests_Part1/05-Independent_2_sample_ttest.html#step-3-prepare-data-for-analysis",
    "href": "4-Statistical_Tests_Part1/05-Independent_2_sample_ttest.html#step-3-prepare-data-for-analysis",
    "title": "Independent Two-sample T-test",
    "section": "Step 3: Prepare Data for Analysis",
    "text": "Step 3: Prepare Data for Analysis\nThese data look ready for analysis."
  },
  {
    "objectID": "4-Statistical_Tests_Part1/05-Independent_2_sample_ttest.html#step-4-perform-the-appropriate-analysis",
    "href": "4-Statistical_Tests_Part1/05-Independent_2_sample_ttest.html#step-4-perform-the-appropriate-analysis",
    "title": "Independent Two-sample T-test",
    "section": "Step 4: Perform the appropriate analysis",
    "text": "Step 4: Perform the appropriate analysis\n\nHypothesis Test\nAre the individuals in each group dependent or independent of each other?\nWrite out your null and alternative hypotheses.\nHo: Ha:\nWhich group is considered group 1 and which is group 2 in R?\nCheck the alphabetical order:\n\nunique(fifa_heart_attacks$time_period)\n\n[1] \"Control\"   \"World Cup\"\n\n\nPerform the appropriate t-test.\nWhat is your test statistic?\nWhat is your p-value?\nState your conclusion:\n\n\n97% Confidence interval\nCalculate the 97% confidence interval for the difference of the means.\nIn context of the research question, interpret the confidence interval."
  },
  {
    "objectID": "4-Statistical_Tests_Part1/05-Independent_2_sample_ttest.html#new-zealand-rugby",
    "href": "4-Statistical_Tests_Part1/05-Independent_2_sample_ttest.html#new-zealand-rugby",
    "title": "Independent Two-sample T-test",
    "section": "New Zealand Rugby",
    "text": "New Zealand Rugby\nRugby is a popular sport in the United Kingdom, France, Australia, New Zealand and South Africa. It is gaining popularity in the US, Canada, Japan and parts of Europe. Some of the rules of the game have recently been changed to make play more exciting. In a study to examine the effects of the rule changes, Hollings and Triggs (1993) collected data on some recent games.\nTypically, a game consists of bursts of activity that terminate when points are scored, if the ball is moved out of the field of play or if a violation of the rules occurs. In 1992, the investigators gathered data on ten international matches which involved the New Zealand national team, the All Blacks. The first five games were the last international games played under the old rules, and the second set of five were the first internationals played under the new rules.\nFor each of the ten games, the data give the successive times (in seconds) of each passage of play in that game.\nYou will investigate whether the mean duration of the passages has dropped under the new rules.\nUse a level of significance of 0.01.\n\nLoad the Data\n\nrugby &lt;- import(\"https://byuistats.github.io/M221R/Data/quiz/R/nz_rugby.csv\")\n\n\n\nExplore the Data\nCreate a side-by-side boxplot for the amount of reported passage of play before and after the rule changes.\nAdd a title and change the colors of the boxes.\nWhat do you observe?\nCreate a table of summary statistics of play time for before and after the rule change. (favstats()):\n\n\nPerform the Appropriate Analysis\n\nHypothesis Test\nState your null and alternative hypotheses:\nNOTE: The default for R is to set group order alphabetically. This means Group 1 = NewRules\nCompare the the time per play under the new and old rules:\n\nqqPlot(rugby$time, groups = rugby$period)\n\n\n\n\n\n\n\n\nDo the data for each group appear normally distributed?\nWhy is it OK to continue with the analysis?\nPerform a t-test.\nWhat is the value of the test statistic?\nHow many degrees of freedom for this test?\nWhat is the p-value?\nWhat do you conclude?\n\n\nConfidence Interval\nCreate a confidence interval for the difference of the average Importance Score between both groups:"
  },
  {
    "objectID": "4-Statistical_Tests_Part1/05-Independent_2_sample_ttest.html#step-1-read-in-the-data",
    "href": "4-Statistical_Tests_Part1/05-Independent_2_sample_ttest.html#step-1-read-in-the-data",
    "title": "Independent Two-sample T-test",
    "section": "Step 1: Read in the data",
    "text": "Step 1: Read in the data\n\ncopd &lt;- import(\"https://byuistats.github.io/M221R/Data/copd_rehab.xlsx\") %&gt;% pivot_longer(cols=c(\"community\", \"hospital\"), names_to = \"Treatment\", values_to = \"Steps\") %&gt;% select(Treatment, Steps) %&gt;% arrange(Treatment)"
  },
  {
    "objectID": "4-Statistical_Tests_Part1/05-Independent_2_sample_ttest.html#step-2-review-the-data",
    "href": "4-Statistical_Tests_Part1/05-Independent_2_sample_ttest.html#step-2-review-the-data",
    "title": "Independent Two-sample T-test",
    "section": "Step 2: Review the data",
    "text": "Step 2: Review the data\nCreate side-by-side boxplots and summary statistics for the community and hospital groups:\nCheck to see if the means are expected to be normally distributed.\nCan trust the CLT for our test statistic and P-value?"
  },
  {
    "objectID": "4-Statistical_Tests_Part1/05-Independent_2_sample_ttest.html#step-3-prepare-data-for-analysis-1",
    "href": "4-Statistical_Tests_Part1/05-Independent_2_sample_ttest.html#step-3-prepare-data-for-analysis-1",
    "title": "Independent Two-sample T-test",
    "section": "Step 3: Prepare Data for Analysis",
    "text": "Step 3: Prepare Data for Analysis\nThe data cleansing has been performed for you. You’re welcome."
  },
  {
    "objectID": "4-Statistical_Tests_Part1/05-Independent_2_sample_ttest.html#step-4-perform-the-appropriate-analysis-1",
    "href": "4-Statistical_Tests_Part1/05-Independent_2_sample_ttest.html#step-4-perform-the-appropriate-analysis-1",
    "title": "Independent Two-sample T-test",
    "section": "Step 4: Perform the appropriate analysis",
    "text": "Step 4: Perform the appropriate analysis\n\nHypothesis Test\nState your null and alternative hypotheses.\nHo:\nHa:\nWhich group is considered group 1 in this data?\nRun the appropriate t-test.\n\n#t.test()\n\nState your conclusion about the hypothesis test.\n\n\nConfidence Interval\nCreate a 95% confidence interval for the difference between the means\nInterpret the 95% confidence interval for the mean difference between the community-based and hospital-based groups."
  },
  {
    "objectID": "4-Statistical_Tests_Part1/06-Independent_2_sample_ttest_practice.html",
    "href": "4-Statistical_Tests_Part1/06-Independent_2_sample_ttest_practice.html",
    "title": "2-Sample Independent T-Test Practice",
    "section": "",
    "text": "Here are several opportunities to practice analyzing 2-sample independent t-tests using R. For each question, you will:\n\nRead in data\nIdentify the Response and Independent variable\nCreate data summaries (numerical and graphical)\nStatistically analyze the data\nCheck for the suitability of the statistical test (CLT, Normality)\nState your hypothesis test conclusions and interpret your confidence intervals\n\nWhen you finish, render this document and submit the .html in Canvas.\n\n# Load the libraries\n\nlibrary(rio)\nlibrary(mosaic)\nlibrary(tidyverse)\nlibrary(car)"
  },
  {
    "objectID": "4-Statistical_Tests_Part1/06-Independent_2_sample_ttest_practice.html#load-the-data",
    "href": "4-Statistical_Tests_Part1/06-Independent_2_sample_ttest_practice.html#load-the-data",
    "title": "2-Sample Independent T-Test Practice",
    "section": "Load the Data",
    "text": "Load the Data\n\ndating &lt;- read_csv('https://github.com/byuistats/Math221D_Course/raw/main/Data/dating_attractive_longformat.csv')"
  },
  {
    "objectID": "4-Statistical_Tests_Part1/06-Independent_2_sample_ttest_practice.html#explore-the-data",
    "href": "4-Statistical_Tests_Part1/06-Independent_2_sample_ttest_practice.html#explore-the-data",
    "title": "2-Sample Independent T-Test Practice",
    "section": "Explore the Data",
    "text": "Explore the Data\nQuestion: What is the response variable?\nAnswer:\nQuestion: What is the explanatory variable?\nAnswer:\nCreate a side-by-side boxplot for the amount of reported importance of attractiveness for each biosex.\nAdd a title and change the colors of the boxes.\nWhat do you observe?\nCreate a table of summary statistics for each group (favstats()):"
  },
  {
    "objectID": "4-Statistical_Tests_Part1/06-Independent_2_sample_ttest_practice.html#hypothesis-test",
    "href": "4-Statistical_Tests_Part1/06-Independent_2_sample_ttest_practice.html#hypothesis-test",
    "title": "2-Sample Independent T-Test Practice",
    "section": "Hypothesis Test",
    "text": "Hypothesis Test\nState your null and alternative hypotheses (replace the ??? with the appropriate symbol):\n\\[H_0:  \\mu_{F}???\\mu_{M}\\]\n\\[H_a:\\mu_{F}???\\mu_{M}\\]\nNOTE: The default for R is to set group order alphabetically. This means Group 1 = Female.\nCheck that the samples for both groups are normally distributed\n\nqqPlot(dating$Importance~dating$Biosex)\n\n\n\n\n\n\n\n\nDo the data for each group appear normally distributed?\nWhy is it OK to continue with the analysis?\nPerform a t-test.\nQuestion: What is the value of the test statistic?\nAnswer:\nQuestion: How many degrees of freedom for this test?\nAnswer:\nQuestion: What is the p-value?\nAnswer:\nQuestion: What do you conclude?\nAnswer:\n\nConfidence Interval\nCreate a confidence interval for the difference of the average Importance Score between both groups:"
  },
  {
    "objectID": "4-Statistical_Tests_Part1/06-Independent_2_sample_ttest_practice.html#review-the-data",
    "href": "4-Statistical_Tests_Part1/06-Independent_2_sample_ttest_practice.html#review-the-data",
    "title": "2-Sample Independent T-Test Practice",
    "section": "Review The Data",
    "text": "Review The Data\nQuestion: What is the response variable?\nAnswer:\nQuestion: What is the explanatory variable?\nAnswer:\nCreate summary statistics tables of dental costs for each office:\nCreate a side-by-side boxplot for dental costs for each office.\nCheck the normality of each group.\nQuestion: Do the samples from both groups appear to be normally distributed? If not, is it a cause for concern for our statistical inference?"
  },
  {
    "objectID": "4-Statistical_Tests_Part1/06-Independent_2_sample_ttest_practice.html#hypothesis-test-1",
    "href": "4-Statistical_Tests_Part1/06-Independent_2_sample_ttest_practice.html#hypothesis-test-1",
    "title": "2-Sample Independent T-Test Practice",
    "section": "Hypothesis Test",
    "text": "Hypothesis Test\nState your null and alternative hypotheses (replace the question marks with the appropriate symbols):\n\\[H_0:  \\mu_{IF}???\\mu_{R}\\]\n\\[H_a:\\mu_{IF}???\\mu_{R}\\]\nPerform the appropriate analysis:\nQuestion: What is the test statistic?\nAnswer:\nQuestion: What is the P-value?\nAnswer:\nState your conclusion:"
  },
  {
    "objectID": "4-Statistical_Tests_Part1/06-Independent_2_sample_ttest_practice.html#confidence-interval-1",
    "href": "4-Statistical_Tests_Part1/06-Independent_2_sample_ttest_practice.html#confidence-interval-1",
    "title": "2-Sample Independent T-Test Practice",
    "section": "Confidence Interval",
    "text": "Confidence Interval\nCreate a confidence interval for the difference in costs between the IF and Rexburg offices:\nExplain the confidence interval in context of the research question:"
  },
  {
    "objectID": "4-Statistical_Tests_Part1/06-Independent_2_sample_ttest_practice.html#review-the-data-1",
    "href": "4-Statistical_Tests_Part1/06-Independent_2_sample_ttest_practice.html#review-the-data-1",
    "title": "2-Sample Independent T-Test Practice",
    "section": "Review The Data",
    "text": "Review The Data\nQuestion: What is the response variable?\nAnswer:\nQuestion: What is the explanatory variable?\nAnswer:\nCreate summary statistics tables of birth weights for each country:\nCreate a side-by-side boxplot for birth weights for each country:\nCheck the normality of each group.\nQuestion: Do the samples from both groups appear to be normally distributed? If not, is it a cause for concern for our statistical inference?"
  },
  {
    "objectID": "4-Statistical_Tests_Part1/06-Independent_2_sample_ttest_practice.html#hypothesis-test-2",
    "href": "4-Statistical_Tests_Part1/06-Independent_2_sample_ttest_practice.html#hypothesis-test-2",
    "title": "2-Sample Independent T-Test Practice",
    "section": "Hypothesis Test",
    "text": "Hypothesis Test\nState your null and alternative hypotheses (replace the question marks with the appropriate symbols):\n\\[H_0:  \\mu_{A}???\\mu_{IL}\\]\n\\[H_a:\\mu_{A}???\\mu_{IL}\\]\nPerform the appropriate analysis:\nQuestion: What is the test statistic?\nAnswer:\nQuestion: What is the P-value?\nAnswer:\nState your conclusion:"
  },
  {
    "objectID": "4-Statistical_Tests_Part1/06-Independent_2_sample_ttest_practice.html#confidence-interval-2",
    "href": "4-Statistical_Tests_Part1/06-Independent_2_sample_ttest_practice.html#confidence-interval-2",
    "title": "2-Sample Independent T-Test Practice",
    "section": "Confidence Interval",
    "text": "Confidence Interval\nCreate a confidence interval for the average difference in weights between babies born to mothers in Africa and Illinois:\nExplain the confidence interval in context of the research question:"
  },
  {
    "objectID": "4-Statistical_Tests_Part1/07-ANOVA_Intro.html",
    "href": "4-Statistical_Tests_Part1/07-ANOVA_Intro.html",
    "title": "Analysis of Variance (ANOVA)",
    "section": "",
    "text": "When we want to compare 3 or more groups, the math get’s more complicated. Analysis of Variance (ANOVA) compares how spread out the means are relative to the average within group variation. The formula for the new test statistic, \\(F\\), is messy, but we can get a sense for what it’s doing visually.\n\nThe test statistic is the ratio of the variation between groups and the average variation within groups. The further spread out the sample means are relative to the noise within the groups, the more significant the result.\n\n\nRecall that the shape of the \\(t\\)-distribution depended on how much data was in the sample. The t-distribution was fatter tailed than the standard normal distribution when n was small. The F-statistic also changes shape. Its shape depends on how many data points are in the sample and how many groups we are comparing. This means the F-distribution has 2 sets of degrees of freedom.\nThe numerator, or between groups degrees of freedom is \\(df_{between}=k-1\\), where k is the number of groups you are comparing.\nThe denominator, or within groups degrees of freedom is \\(df_{within}=n-k\\) where n is the total number of data points and k is the number of groups.\nWe can get these degrees of freedom directly from the R output.\nUnlike the \\(t\\)-distribution, the \\(F\\)-distribution is not centered around zero and can never be negative. \\(F\\) is the ratio of 2 positive numbers and is, therefore, always positive.\nTo summarize:\n\n\\(F\\) is always positive because it is the ratio of 2 positive numbers\n\\(F\\) is always right skewed\n\\(F\\) changes shape depending on the number of groups (numerator degrees of freedom) and the number of total data points (denominator degrees of freedom)\n\n\nThe P-value for an F-statistic is always one-tailed. The probability of observing a test statistic, \\(F\\), if the null hypothesis is true can be visualized:\n\nIn practice, the computer calculates the test statistic, P-value, and degrees of freedom and we interpret the output as with other statistical tests.\n\n\n\nThe null and alternative hypotheses are always the same for an ANOVA:\n\\[H_o: \\mu_1 = \\mu_2 = ...\\mu_k\\] where k is the number of groups in the data.\n\\[H_a: \\text{at least one } \\mu_k \\text{ is different}\\] Note: The \\(F\\)-test does not tell us which group is different or how many are different from each other. The \\(F\\)-test only tells us that something is different.\n\n\n\nJust as with other statistical tests we’ve done so far, the \\(F\\)-test has certain requirements we must check to validate our P-values. Because of the way we calculate F, we are less concerned with the normality of the individual groups as we are with the variation within the groups.\nWhat to check:\n\nAre the standard deviations of each group “equal”?\nAre the residuals normally distributed?\n\n\n\nTo check the first requirement we can compare the biggest standard deviation to the smallest. If the ratio of the biggest to the smallest is less than 2, we conclude that the population standard deviations are “equal”.\nNOTE: Intuitively, this means that if the biggest standard deviation is more than twice as big as the smallest, then we might have cause for concern.\nThis can be checked using the standard deviations from the favstats() output (see Analysis in R below)\n\n\n\nWe will discuss residuals in more depth when we cover regression analysis. For now, think of residuals as the deviations of each observation away from their group mean. If our analysis is to be trusted, these deviations need to be normally distributed.\nIf both requirements are met, then we can trust the P-value.\n\n\n\n\nMuch of the syntax for ANOVA will look familiar, but we will be using a new function, aov() instead of t.test().\nThe aov() function by itself isn’t as useful as t.test(). However, we can use the summary() function to give us everything we need.\nWe typically name our output using the assignment operator &lt;- to make it easier to extract the information we would like. The inside of aov() will look familiar, using the same ~ notation we’ve used all semester.\nThe generic process for performing an ANOVA is:\n\n# Name the ANOVA output:\noutput &lt;- aov(data$response_variable ~ data$categorical_variable)\n\n# Summarise the ANOVA output to get test statistics, DF, P-value, etc:\nsummary(output)\n\n\n\n\n\nWe can use favstats() to extract the standard deviations of each group, then find the ratio of the max/min to see if it is less than 2.\n\n# extract only the standard deviations from favstats using th `$`:\n\nsds &lt;- favstats(data$response_variable ~ data$categorical_variable)$sd\n\nError in favstats(data$response_variable ~ data$categorical_variable): could not find function \"favstats\"\n\n# Compare the max/min to 2\n\nmax(sds) / min(sds)\n\nError in eval(expr, envir, enclos): object 'sds' not found\n\n# if max/min &lt; 2, then we're ok\n\n\n\n\nWe can assess normality of the residuals with a qqPlot(). We first need to extract the residuals from our output:\n\noutput &lt;- aov(data$response_variable ~ data$categorical_variable)\n\nError in data$response_variable: object of type 'closure' is not subsettable\n\nqqPlot(output$residuals)\n\nError in qqPlot(output$residuals): could not find function \"qqPlot\"\n\n\nIf most of the points fall within the blue zone, we can be confident that the residuals are normally distributed."
  },
  {
    "objectID": "4-Statistical_Tests_Part1/07-ANOVA_Intro.html#the-f-distribution",
    "href": "4-Statistical_Tests_Part1/07-ANOVA_Intro.html#the-f-distribution",
    "title": "Analysis of Variance (ANOVA)",
    "section": "",
    "text": "Recall that the shape of the \\(t\\)-distribution depended on how much data was in the sample. The t-distribution was fatter tailed than the standard normal distribution when n was small. The F-statistic also changes shape. Its shape depends on how many data points are in the sample and how many groups we are comparing. This means the F-distribution has 2 sets of degrees of freedom.\nThe numerator, or between groups degrees of freedom is \\(df_{between}=k-1\\), where k is the number of groups you are comparing.\nThe denominator, or within groups degrees of freedom is \\(df_{within}=n-k\\) where n is the total number of data points and k is the number of groups.\nWe can get these degrees of freedom directly from the R output.\nUnlike the \\(t\\)-distribution, the \\(F\\)-distribution is not centered around zero and can never be negative. \\(F\\) is the ratio of 2 positive numbers and is, therefore, always positive.\nTo summarize:\n\n\\(F\\) is always positive because it is the ratio of 2 positive numbers\n\\(F\\) is always right skewed\n\\(F\\) changes shape depending on the number of groups (numerator degrees of freedom) and the number of total data points (denominator degrees of freedom)\n\n\nThe P-value for an F-statistic is always one-tailed. The probability of observing a test statistic, \\(F\\), if the null hypothesis is true can be visualized:\n\nIn practice, the computer calculates the test statistic, P-value, and degrees of freedom and we interpret the output as with other statistical tests."
  },
  {
    "objectID": "4-Statistical_Tests_Part1/07-ANOVA_Intro.html#hypothesis-test",
    "href": "4-Statistical_Tests_Part1/07-ANOVA_Intro.html#hypothesis-test",
    "title": "Analysis of Variance (ANOVA)",
    "section": "",
    "text": "The null and alternative hypotheses are always the same for an ANOVA:\n\\[H_o: \\mu_1 = \\mu_2 = ...\\mu_k\\] where k is the number of groups in the data.\n\\[H_a: \\text{at least one } \\mu_k \\text{ is different}\\] Note: The \\(F\\)-test does not tell us which group is different or how many are different from each other. The \\(F\\)-test only tells us that something is different."
  },
  {
    "objectID": "4-Statistical_Tests_Part1/07-ANOVA_Intro.html#test-requirements",
    "href": "4-Statistical_Tests_Part1/07-ANOVA_Intro.html#test-requirements",
    "title": "Analysis of Variance (ANOVA)",
    "section": "",
    "text": "Just as with other statistical tests we’ve done so far, the \\(F\\)-test has certain requirements we must check to validate our P-values. Because of the way we calculate F, we are less concerned with the normality of the individual groups as we are with the variation within the groups.\nWhat to check:\n\nAre the standard deviations of each group “equal”?\nAre the residuals normally distributed?\n\n\n\nTo check the first requirement we can compare the biggest standard deviation to the smallest. If the ratio of the biggest to the smallest is less than 2, we conclude that the population standard deviations are “equal”.\nNOTE: Intuitively, this means that if the biggest standard deviation is more than twice as big as the smallest, then we might have cause for concern.\nThis can be checked using the standard deviations from the favstats() output (see Analysis in R below)\n\n\n\nWe will discuss residuals in more depth when we cover regression analysis. For now, think of residuals as the deviations of each observation away from their group mean. If our analysis is to be trusted, these deviations need to be normally distributed.\nIf both requirements are met, then we can trust the P-value."
  },
  {
    "objectID": "4-Statistical_Tests_Part1/07-ANOVA_Intro.html#analysis-in-r",
    "href": "4-Statistical_Tests_Part1/07-ANOVA_Intro.html#analysis-in-r",
    "title": "Analysis of Variance (ANOVA)",
    "section": "",
    "text": "Much of the syntax for ANOVA will look familiar, but we will be using a new function, aov() instead of t.test().\nThe aov() function by itself isn’t as useful as t.test(). However, we can use the summary() function to give us everything we need.\nWe typically name our output using the assignment operator &lt;- to make it easier to extract the information we would like. The inside of aov() will look familiar, using the same ~ notation we’ve used all semester.\nThe generic process for performing an ANOVA is:\n\n# Name the ANOVA output:\noutput &lt;- aov(data$response_variable ~ data$categorical_variable)\n\n# Summarise the ANOVA output to get test statistics, DF, P-value, etc:\nsummary(output)\n\n\n\n\n\nWe can use favstats() to extract the standard deviations of each group, then find the ratio of the max/min to see if it is less than 2.\n\n# extract only the standard deviations from favstats using th `$`:\n\nsds &lt;- favstats(data$response_variable ~ data$categorical_variable)$sd\n\nError in favstats(data$response_variable ~ data$categorical_variable): could not find function \"favstats\"\n\n# Compare the max/min to 2\n\nmax(sds) / min(sds)\n\nError in eval(expr, envir, enclos): object 'sds' not found\n\n# if max/min &lt; 2, then we're ok\n\n\n\n\nWe can assess normality of the residuals with a qqPlot(). We first need to extract the residuals from our output:\n\noutput &lt;- aov(data$response_variable ~ data$categorical_variable)\n\nError in data$response_variable: object of type 'closure' is not subsettable\n\nqqPlot(output$residuals)\n\nError in qqPlot(output$residuals): could not find function \"qqPlot\"\n\n\nIf most of the points fall within the blue zone, we can be confident that the residuals are normally distributed."
  },
  {
    "objectID": "4-Statistical_Tests_Part1/07-ANOVA_Intro.html#step-1-read-in-data",
    "href": "4-Statistical_Tests_Part1/07-ANOVA_Intro.html#step-1-read-in-data",
    "title": "Analysis of Variance (ANOVA)",
    "section": "Step 1: Read in data",
    "text": "Step 1: Read in data\nFor this demonstration we will be exploring the iris data. This dataset is built in to base R libraries, so we can access it without reading it in using “import”."
  },
  {
    "objectID": "4-Statistical_Tests_Part1/07-ANOVA_Intro.html#step-2-review-the-data",
    "href": "4-Statistical_Tests_Part1/07-ANOVA_Intro.html#step-2-review-the-data",
    "title": "Analysis of Variance (ANOVA)",
    "section": "Step 2: Review the data",
    "text": "Step 2: Review the data\nThe iris data contains multiple measures on flowers that might be of interest to compare across species.\nLet’s first compare sepal lengths between species."
  },
  {
    "objectID": "4-Statistical_Tests_Part1/07-ANOVA_Intro.html#step-2-explore-the-data",
    "href": "4-Statistical_Tests_Part1/07-ANOVA_Intro.html#step-2-explore-the-data",
    "title": "Analysis of Variance (ANOVA)",
    "section": "Step 2: Explore the Data",
    "text": "Step 2: Explore the Data\nHow many species do we have in our dataset?\n\ntable(iris$Species)\n\n\n    setosa versicolor  virginica \n        50         50         50 \n\n\nCreate a side-by-side boxplot of species and Sepal Length.\n\nboxplot(Sepal.Length ~ Species, data=iris, col = c(2,3,4), ylab=\"Sepal Length\", main = \"Sepal Length by Species\")\n\n\n\n\n\n\n\n\nUsing GGPlot:\n\nggplot(iris, aes(x=Species, y = Sepal.Length)) +\n  geom_boxplot(fill=c(2,3,4)) +\n  theme_bw() +\n  labs(\n    y = \"Sepal Length\",\n    title = \"Sepal Length by Species\"\n  )"
  },
  {
    "objectID": "4-Statistical_Tests_Part1/07-ANOVA_Intro.html#step-4-perform-the-appropriate-analysis",
    "href": "4-Statistical_Tests_Part1/07-ANOVA_Intro.html#step-4-perform-the-appropriate-analysis",
    "title": "Analysis of Variance (ANOVA)",
    "section": "Step 4: Perform the appropriate analysis",
    "text": "Step 4: Perform the appropriate analysis\n\naov_sepal &lt;- aov(Sepal.Length ~ Species, data=iris)\nsummary(aov_sepal)\n\n             Df Sum Sq Mean Sq F value Pr(&gt;F)    \nSpecies       2  63.21  31.606   119.3 &lt;2e-16 ***\nResiduals   147  38.96   0.265                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nBefore we make a conclusion, we want to check that we can trust our output. Every statistical test has requirements that must be satisfied if we are to trust our conclusions. For ANOVA, we need to check the normality and that the variation within groups is roughly the same.\nWe use a QQ-plot to check for normality and the ratio of the largest to the smallest standard deviation to check “equal” variation.\n\n# QQ plots show how closely the the residuals are to a normal distribution\n\nqqPlot(aov_sepal$residuals)\n\n\n\n\n\n\n\n\n[1] 107 132\n\n# Check that there is less than a 2X difference between the largest and smallest standard deviations\n\n# We can assign favstats()$sd to a variable to make it easier to use. Recall the \"$\" can also be used to extract specific output from functions\n\nsds &lt;- favstats(Sepal.Length ~ Species, data=iris)$sd\n\nmax(sds) / min(sds)\n\n[1] 1.803967\n\n\nQuestion: What is the F-statistics?\nAnswer:\nQuestion: What are the between-groups degrees of freedom?\nAnswer:\nQuestion: What are the within-groups degrees of freedom?\nAnswer:\nQuestion: What is the P-value?\nAnswer:\nQuestion: What is your conclusion?\nAnswer:"
  },
  {
    "objectID": "4-Statistical_Tests_Part1/08-ANOVA_Practice.html",
    "href": "4-Statistical_Tests_Part1/08-ANOVA_Practice.html",
    "title": "ANOVA Practice",
    "section": "",
    "text": "You are curious to compare life expectancy between female poets, novelists, and non-fiction writers.\nYou take a sample of female authors from each of the three groups to test if the average age at death is different between any of the three types of authors using a level of significance of, \\(\\alpha = 0.05\\).\n\n\n\n\nlibrary(rio)\nlibrary(tidyverse)\nlibrary(mosaic)\nlibrary(car)\n\nwomenpoet &lt;- rio::import(\"https://byuistats.github.io/BYUI_M221_Book/Data/womenpoet.xls\")\n\n\n\n\nCreate a side-by-side boxplot of the age at death of each of the different author styles.\nModify the colors of each of the boxes for each group.\nCreate a summary statistics table for age at death for each author type:\nList the mean and standard deviations of age at death for:\n\nNovelists:\nPoets:\nNon-fiction:\n\n\n\n\nState your null and alternative hypotheses:\nPerform an Analysis of Variance test including checking for the appropriateness of our analysis.\nQuestion: What is the test statistic?\nAnswer:\nQuestion: What are the degrees of freedom for your analysis?\na. Numerator (between Groups) Degrees of Freedom\nb. Denominator (within groups) Degrees of Freedom\nAnswer:\nQuestion: What is the P-value?\nAnswer:\nQuestion: Do you reject the null hypothesis? Why?\nAnswer:\nQuestion: State your conclusion in context of the problem.\nAnswer:\nQuestion: Can we trust the p-value? a. Check for equal standard deviation (is the ratio of the largest SD / smallest SD greater than 2?) b. Check Normality of the residuals (qqPlot())\nAnswer:"
  },
  {
    "objectID": "4-Statistical_Tests_Part1/08-ANOVA_Practice.html#introduction",
    "href": "4-Statistical_Tests_Part1/08-ANOVA_Practice.html#introduction",
    "title": "ANOVA Practice",
    "section": "",
    "text": "You are curious to compare life expectancy between female poets, novelists, and non-fiction writers.\nYou take a sample of female authors from each of the three groups to test if the average age at death is different between any of the three types of authors using a level of significance of, \\(\\alpha = 0.05\\)."
  },
  {
    "objectID": "4-Statistical_Tests_Part1/08-ANOVA_Practice.html#load-the-data-and-libraries",
    "href": "4-Statistical_Tests_Part1/08-ANOVA_Practice.html#load-the-data-and-libraries",
    "title": "ANOVA Practice",
    "section": "",
    "text": "library(rio)\nlibrary(tidyverse)\nlibrary(mosaic)\nlibrary(car)\n\nwomenpoet &lt;- rio::import(\"https://byuistats.github.io/BYUI_M221_Book/Data/womenpoet.xls\")"
  },
  {
    "objectID": "4-Statistical_Tests_Part1/08-ANOVA_Practice.html#explore-the-data",
    "href": "4-Statistical_Tests_Part1/08-ANOVA_Practice.html#explore-the-data",
    "title": "ANOVA Practice",
    "section": "",
    "text": "Create a side-by-side boxplot of the age at death of each of the different author styles.\nModify the colors of each of the boxes for each group.\nCreate a summary statistics table for age at death for each author type:\nList the mean and standard deviations of age at death for:\n\nNovelists:\nPoets:\nNon-fiction:"
  },
  {
    "objectID": "4-Statistical_Tests_Part1/08-ANOVA_Practice.html#perform-the-appropriate-analysis",
    "href": "4-Statistical_Tests_Part1/08-ANOVA_Practice.html#perform-the-appropriate-analysis",
    "title": "ANOVA Practice",
    "section": "",
    "text": "State your null and alternative hypotheses:\nPerform an Analysis of Variance test including checking for the appropriateness of our analysis.\nQuestion: What is the test statistic?\nAnswer:\nQuestion: What are the degrees of freedom for your analysis?\na. Numerator (between Groups) Degrees of Freedom\nb. Denominator (within groups) Degrees of Freedom\nAnswer:\nQuestion: What is the P-value?\nAnswer:\nQuestion: Do you reject the null hypothesis? Why?\nAnswer:\nQuestion: State your conclusion in context of the problem.\nAnswer:\nQuestion: Can we trust the p-value? a. Check for equal standard deviation (is the ratio of the largest SD / smallest SD greater than 2?) b. Check Normality of the residuals (qqPlot())\nAnswer:"
  },
  {
    "objectID": "4-Statistical_Tests_Part1/08-ANOVA_Practice.html#introduction-1",
    "href": "4-Statistical_Tests_Part1/08-ANOVA_Practice.html#introduction-1",
    "title": "ANOVA Practice",
    "section": "Introduction",
    "text": "Introduction\nA study was conducted to determine if different types of material can reduce the amount of mosquito human contact. The researchers evaluated five different types of patches 1=Odomos, 2=Deltamethrin, 3=Cyfluthrin, 4=D+O, 5=C+O.\nThe amount of mosquito human contact was measured to assess any differences between the five different types of material. Use a level of significance of 0.05."
  },
  {
    "objectID": "4-Statistical_Tests_Part1/08-ANOVA_Practice.html#load-the-data",
    "href": "4-Statistical_Tests_Part1/08-ANOVA_Practice.html#load-the-data",
    "title": "ANOVA Practice",
    "section": "Load the Data",
    "text": "Load the Data\n\nMosquitoPatch &lt;- rio::import(\"https://raw.githubusercontent.com/rdcromar/Math221D/main/MosquitoPatch.csv\") %&gt;% mutate(Treatment = factor(Treatment))"
  },
  {
    "objectID": "4-Statistical_Tests_Part1/08-ANOVA_Practice.html#review-the-data",
    "href": "4-Statistical_Tests_Part1/08-ANOVA_Practice.html#review-the-data",
    "title": "ANOVA Practice",
    "section": "Review the Data",
    "text": "Review the Data\nCreate a side-by-side boxplot for human contact for each of the treatment groups.\nAdd a title and change the colors of the boxes.\nCreate a summary statistics table for human contact for each of the treatment groups:\nQuestion: What do you observe?\nAnswer:\nQuestion: What is the maximum standard deviation?\nAnswer:\nQuestion: What is the minimum standard deviation?\nAnswer:"
  },
  {
    "objectID": "4-Statistical_Tests_Part1/08-ANOVA_Practice.html#perform-the-appropriate-analysis-1",
    "href": "4-Statistical_Tests_Part1/08-ANOVA_Practice.html#perform-the-appropriate-analysis-1",
    "title": "ANOVA Practice",
    "section": "Perform the Appropriate Analysis",
    "text": "Perform the Appropriate Analysis\nState your null and alternative hypotheses:\nPerform an Analysis of Variance test including checking for the appropriateness of our analysis.\nQuestion: What is the test statistic (F-value)?\nAnswer:\nQuestion: What are the degrees of freedom for your analysis?\n\nNumerator (between Groups) Degrees of Freedom\n\nDenominator (within groups) Degrees of Freedom\nAnswer:\n\nQuestion: What is the P-value?\nAnswer:\nQuestion: Do you reject the null hypothesis? Why?\nAnswer:\nQuestion: State your conclusion in context of the problem.\nAnswer:\nQuestion: Can we trust the p-value? a. Check for equal standard deviation (is the ratio of the largest SD / smallest SD greater than 2?) b. Check Normality of the residuals (qqPlot())\nAnswer:"
  },
  {
    "objectID": "5-Statistical_Tests_Part2/01-Bivariate_Data_Intro.html",
    "href": "5-Statistical_Tests_Part2/01-Bivariate_Data_Intro.html",
    "title": "Introducing the Bivariate Data",
    "section": "",
    "text": "Bivariate data refers to situations where you have one quantitative response variable and one quantitative explanatory variable. This requires a different approach to analysis and visualization.\nBy the end of this lesson, you should be able to:\n\nCreate scatterplots for 2 quantitative variables in base R and GGPlot\n\nDescribe what the correlation coefficient, \\(r\\), quantifies\n\nCalculate \\(r\\) using the cor() function\n\nTwo datasets will be used to illustrate these concepts. The first contains self-reported confidence in mathematics and test scores. The second contains eruption duration and time between eruptions of Old Faithful geyser in Yellowstone National Park.\n\n# Load the libraries and data\nlibrary(tidyverse)\nlibrary(mosaic)\nlibrary(rio)\n\ngeyser &lt;- import('https://byuistats.github.io/BYUI_M221_Book/Data/OldFaithful.xlsx')\nnames(geyser)\n\n[1] \"Duration\" \"Wait\"     \"Source\"  \n\nmath &lt;- import('https://byuistats.github.io/BYUI_M221_Book/Data/MathSelfEfficacy.xlsx')\nnames(math)\n\n[1] \"Gender\"               \"Score\"                \"ConfidenceRatingMean\"\n[4] \"Comments\""
  },
  {
    "objectID": "5-Statistical_Tests_Part2/01-Bivariate_Data_Intro.html#scatter-plot",
    "href": "5-Statistical_Tests_Part2/01-Bivariate_Data_Intro.html#scatter-plot",
    "title": "Introducing the Bivariate Data",
    "section": "Scatter plot",
    "text": "Scatter plot\nMake a scatter plot showing the relationship between students’ self reported confidence rating and test score.\nQuestion: Which variable is the Explanatory variable, \\(x\\)?\nAnswer:\nQuestion: Which is the Response variable, \\(y\\)?\nAnswer:\n\n# Base R\nplot(Score ~ ConfidenceRatingMean, data = math)\n\n\n\n\n\n\n\n# ggplot\nggplot(math, aes(x = ConfidenceRatingMean, y = Score )) +\n  geom_point(color = \"darkblue\") +\n  theme_bw() +\n  labs(\n    title = \"Relationship between Student Confidence Rating in Math and Test Score\"\n  ) \n\n\n\n\n\n\n\n\nQuestion: Before calculating the Correlation Coefficient, r, describe in words the direction and strength of the relationship.\nAnswer:\nQuestion: What’s your best guess at, \\(r\\) based on the scatterplot?\nAnswer:\nQuestion: Does it look linear?\nAnswer:\nCalculate the Correlation Coefficient, \\(r\\):\n\ncor(Score ~ ConfidenceRatingMean, data = math)\n\n[1] 0.7278648\n\n\nQuestion: How far off was your guess?\nAnswer:"
  },
  {
    "objectID": "5-Statistical_Tests_Part2/01-Bivariate_Data_Intro.html#scatter-plot-1",
    "href": "5-Statistical_Tests_Part2/01-Bivariate_Data_Intro.html#scatter-plot-1",
    "title": "Introducing the Bivariate Data",
    "section": "Scatter plot",
    "text": "Scatter plot\nMake a scatter plot showing the relationship between wait time and the duration of the next eruption.\nWhich variable is the Explanatory variable? Which is the Response?\nQuestion: Before calculating the Correlation Coefficient, r, describe in words the direction and strength of the relationship.\nAnswer:\nQuestion: What’s your best guess at, \\(r\\) based on the scatterplot?\nAnswer:\nQuestion: Does it look linear?\nAnswer:\nCalculate the Correlation Coefficient, \\(r\\):\nQuestion: How far off was your guess?\nAnswer:"
  },
  {
    "objectID": "5-Statistical_Tests_Part2/02-Linear_Regression_Intro.html",
    "href": "5-Statistical_Tests_Part2/02-Linear_Regression_Intro.html",
    "title": "Simple Linear Regression",
    "section": "",
    "text": "Consider the relationship between Score on a math exam and a student’s self-reported Confidence Rating.\n\nlibrary(tidyverse)\nlibrary(mosaic)\nlibrary(rio)\nlibrary(car)\n\nmath &lt;- import('https://byuistats.github.io/BYUI_M221_Book/Data/MathSelfEfficacy.xlsx')\n\nQuestion: What is the explanatory (aka independent) variable, \\(x\\)?\nAnswer:\nQuestion: What is the response (aka the dependent) variable, \\(y\\)?\nAnswer:\nPlot the relationship:\n\nplot(Score ~ ConfidenceRatingMean, data = math)\n\n\n\n\n\n\n\nggplot(math, aes(x = ConfidenceRatingMean, y = Score )) +\n  geom_point(color = \"darkblue\") +\n  theme_bw() +\n  labs(\n    title = \"Relationship between Student Confidence Rating in Math and Test Score\"\n  ) \n\n\n\n\n\n\n\n\nQuestion: Does the relationship appear linear?\nAnswer:\nQuestion: What is the direction of the relationship? Answer:\nQuestion: What do you think is the strength of the relationship? (Strong/Moderate/Weak) Answer:\nQuestion: What is the correlation coefficient, r? Answer:\n\ncor(Score ~ ConfidenceRatingMean, data = math)\n\n[1] 0.7278648"
  },
  {
    "objectID": "5-Statistical_Tests_Part2/02-Linear_Regression_Intro.html#plotting-the-regression-line",
    "href": "5-Statistical_Tests_Part2/02-Linear_Regression_Intro.html#plotting-the-regression-line",
    "title": "Simple Linear Regression",
    "section": "Plotting the Regression Line",
    "text": "Plotting the Regression Line\n\nBase R\nScatter plots by themselves are nice, but we would also like to see the regression line. Simple graphics in R can be augmented by using some functions. The abline() function in base R, when executed right after a graphing function can add lines. We’ve used this to add vertical lines and horizontal line already in class. We can also use this function to add a regression line. We simply insert our linear model output into the abline() function as follows:\n\nplot(Score ~ ConfidenceRatingMean, data = math)\nabline(math_lm)\n\n\n\n\n\n\n\n\nJust as with the other plotting functions we’ve used, we can change the color, type and width of the line:\n\nplot(Score ~ ConfidenceRatingMean, data = math, pch = 16, main = \"Title\")\nabline(math_lm, col = \"purple\", lwd = 3, lty = 3)"
  },
  {
    "objectID": "5-Statistical_Tests_Part2/02-Linear_Regression_Intro.html#ggplot",
    "href": "5-Statistical_Tests_Part2/02-Linear_Regression_Intro.html#ggplot",
    "title": "Simple Linear Regression",
    "section": "GGPlot",
    "text": "GGPlot\nUsing ggplot(), we can simply add a geom_smooth() geometry and specify the method of “smoothing” as a linear model:\n\nggplot(math, aes(x = ConfidenceRatingMean, y = Score )) +\n  geom_point(color = \"darkblue\") +\n  theme_bw() +\n  labs(\n    title = \"Relationship between Student Confidence Rating in Math and Test Score\"\n  ) +\n  geom_smooth(method=\"lm\")\n\n\n\n\n\n\n\n\nBy default, this gives us a confidence interval for the slope of the regression line. We can turn that off by forcing the “standard error” to be FALSE:\n\nggplot(math, aes(x = ConfidenceRatingMean, y = Score )) +\n  geom_point(color = \"darkblue\") +\n  theme_bw() +\n  labs(\n    title = \"Relationship between Student Confidence Rating in Math and Test Score\"\n  ) +\n  geom_smooth(method=\"lm\", se = FALSE)"
  },
  {
    "objectID": "5-Statistical_Tests_Part2/02-Linear_Regression_Intro.html#hypothesis-testing-for-regression",
    "href": "5-Statistical_Tests_Part2/02-Linear_Regression_Intro.html#hypothesis-testing-for-regression",
    "title": "Simple Linear Regression",
    "section": "Hypothesis Testing for Regression",
    "text": "Hypothesis Testing for Regression\nA linear equation has 2 parameters: Slope and Intercept. In most situations, the intercept isn’t very interesting by itself and is often absurd. We are most often only interested in the slope\n\\[H_o: \\beta_1 = 0\\] \\[H_a: \\beta_1 \\neq 0\\]\nThese are the same for all simple linear regression questions.\nTo get the p-value and test statistics, we use the summary() function as we did with aov:\n\nsummary(math_lm)\n\n\nCall:\nlm(formula = Score ~ ConfidenceRatingMean, data = math)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-38.200  -6.163   1.292   7.567  23.422 \n\nCoefficients:\n                     Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)            18.690      4.610   4.054  8.4e-05 ***\nConfidenceRatingMean   12.695      1.022  12.424  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.27 on 137 degrees of freedom\nMultiple R-squared:  0.5298,    Adjusted R-squared:  0.5264 \nF-statistic: 154.4 on 1 and 137 DF,  p-value: &lt; 2.2e-16\n\n\nWe can also calculate confidence intervals for the slope by using the confint() function. This function requires you to tell it which model to extract a confidence intervals from. You can specify which parameter you’re interested in, and the level of confidence:\n\n# input the model into the following function:\nconfint(math_lm, level = .95)\n\n                         2.5 %   97.5 %\n(Intercept)           9.573588 27.80654\nConfidenceRatingMean 10.674297 14.71535\n\n\nHow do we interpret this confidence interval for a slope?\nTechnically correct: 95% Confident that the true population slope is within (10.674297, 14.71535)\nContextual Explanation: For every 1 unit increase in Confidence Rating, test scores go up by between (10.674297, 14.71535) on average."
  },
  {
    "objectID": "5-Statistical_Tests_Part2/02-Linear_Regression_Intro.html#regression-requirements-is-our-p-value-sus",
    "href": "5-Statistical_Tests_Part2/02-Linear_Regression_Intro.html#regression-requirements-is-our-p-value-sus",
    "title": "Simple Linear Regression",
    "section": "Regression Requirements (Is our P-value sus?)",
    "text": "Regression Requirements (Is our P-value sus?)\nThere are certain requirements for all statistical tests to be valid. For means, we needed to make sure that the Central Limit Theorem applied. This meant that we had a large enough sample size (N&gt;30) or that the population itself was normally distributed.\nFor ANOVA, we had to check that the residuals were normally distributed and that the population standard deviations were the same.\nRegression analysis has 5 requirements to be valid. While this sounds daunting, in practice we can check most of them very quickly.\n\nRelationship between X and Y is Linear\nThe residuals, \\(\\epsilon\\), are normally distributed\nThe Variance of the error terms is constant for all values of X\nThe X’s are fixed and measured without error (i.e. X’s can be considered as known constants)\nThe observations are independent\n\nThe linear relationship is assessed visually with the scatter plot. If there is obvious curvature or non-linearity then fitting a line isn’t the best model.\nWe check the normality of the residuals with a qqPlot() exactly as with the aov() output.\nRecall that with ANOVA, we had to check that the variation in each group was roughly the same (largest standard deviation was less than twice as large as the smallest standard deviation). For a quantitative explanatory variable, we can’t calculate the standard deviation for a specified level of \\(x\\) because \\(x\\) can be any number.\nConstant variance in regression is checked with a new plot that looks at how the predicted values relate to the residuals. This is important because we want our predictions to be “wrong” about the same regardless of the value of the prediction. We’re looking for random scatter.\nRequirement 4 cannot be analyzed directly. It is important because because \\(x\\) is the independent variable. If there is uncertainty about the input, then the simple linear regression might not be the most appropriate model.\nRequirement 5 also cannot be analyzed, but random sampling usually satisfies this requirement.\n\n# Requirement 1:  Check for linear relationship\nggplot(math, aes(x=ConfidenceRatingMean, y = Score)) + \n  geom_point()\n\n\n\n\n\n\n\n# Req 2: Normality of residuals:\nqqPlot(math_lm$residuals)\n\n\n\n\n\n\n\n\n[1] 37 89\n\n# Req 3: Constant variance (look odd patterns). When you put lm() output into the plot function it gives you several different plots. The residual plots we're most interested in are 1 and 2\n\nplot(math_lm, which = 1)"
  },
  {
    "objectID": "5-Statistical_Tests_Part2/03-Regression_Practice.html",
    "href": "5-Statistical_Tests_Part2/03-Regression_Practice.html",
    "title": "Regression Practice",
    "section": "",
    "text": "In this assignment, you will practice regression analysis including:\n\nPlotting bivariate data with a regression line\nCalculating and interpreting the correlation coefficient, r\nFitting a linear regression analysis\nVerifying if a linear model is model is adequate:\n\nChecking for linearity (scatterplot)\nChecking for constant variance (plot(lm_output, which=1))\nChecking for normality of residuals (qqPlot(lm_output$residuals))\n\n\n\nlibrary(tidyverse)\nlibrary(mosaic)\nlibrary(rio)\nlibrary(car)"
  },
  {
    "objectID": "5-Statistical_Tests_Part2/03-Regression_Practice.html#plot-the-data-and-calculate-r",
    "href": "5-Statistical_Tests_Part2/03-Regression_Practice.html#plot-the-data-and-calculate-r",
    "title": "Regression Practice",
    "section": "Plot the Data and calculate r",
    "text": "Plot the Data and calculate r\nDoes the relationship look linear?\nWhat is the correlation coefficient, r?\nWhat does this r show?"
  },
  {
    "objectID": "5-Statistical_Tests_Part2/03-Regression_Practice.html#fit-a-linear-regression-model",
    "href": "5-Statistical_Tests_Part2/03-Regression_Practice.html#fit-a-linear-regression-model",
    "title": "Regression Practice",
    "section": "Fit a Linear Regression Model",
    "text": "Fit a Linear Regression Model\n\n#lm_output &lt;- lm()\n\nAdd the regression line to your chart:\n\n# Sometimes you have to run the whole chunk with plot() and the abline() together:\n\n#plot()\n#abline(lm_output$coefficients)\n\nWhat is the slope of the regression line, and what does it mean?\nWhat is the intercept and what does it mean?\nWhat is your p-value?\nWhat is your conclusion?\nWhat is the confidence interval for the slope?\nInterpret the confidence interval."
  },
  {
    "objectID": "5-Statistical_Tests_Part2/03-Regression_Practice.html#check-model-requirements",
    "href": "5-Statistical_Tests_Part2/03-Regression_Practice.html#check-model-requirements",
    "title": "Regression Practice",
    "section": "Check Model Requirements",
    "text": "Check Model Requirements\nCheck the normality of the residuals:\nCheck for constant variance (Residual by Predicted plot):\n\n#plot(lm_output, which = 1)\n\nLastly, the car you’re interested in buying has around 100,000 miles and costs $11,200. Could this be considered a good deal? Why?"
  },
  {
    "objectID": "5-Statistical_Tests_Part2/03-Regression_Practice.html#plot-the-data-and-calculate-r-1",
    "href": "5-Statistical_Tests_Part2/03-Regression_Practice.html#plot-the-data-and-calculate-r-1",
    "title": "Regression Practice",
    "section": "Plot the Data and calculate r",
    "text": "Plot the Data and calculate r\nDoes the relationship look linear?\nWhat is the correlation coefficient, r?\nWhat does this r show?"
  },
  {
    "objectID": "5-Statistical_Tests_Part2/03-Regression_Practice.html#fit-a-linear-regression-model-1",
    "href": "5-Statistical_Tests_Part2/03-Regression_Practice.html#fit-a-linear-regression-model-1",
    "title": "Regression Practice",
    "section": "Fit a Linear Regression Model",
    "text": "Fit a Linear Regression Model\nAdd the regression line to your chart:\n\n# Sometimes you have to do the plot() and the abline() in one chunk and run the whole thing:\n\nWhat is the slope of the regression line, and what does it mean?\nWhat is the intercept and what does it mean?\nWhat is your p-value?\nWhat is your conclusion?\nWhat is the confidence interval for the slope?\nInterpret the confidence interval:"
  },
  {
    "objectID": "5-Statistical_Tests_Part2/03-Regression_Practice.html#check-model-requirements-1",
    "href": "5-Statistical_Tests_Part2/03-Regression_Practice.html#check-model-requirements-1",
    "title": "Regression Practice",
    "section": "Check Model Requirements",
    "text": "Check Model Requirements\nCheck the normality of the residuals:\nCheck for constant variance (Residual by Predicted plot):"
  },
  {
    "objectID": "5-Statistical_Tests_Part2/03-Regression_Practice.html#plot-the-data-and-calculate-r-2",
    "href": "5-Statistical_Tests_Part2/03-Regression_Practice.html#plot-the-data-and-calculate-r-2",
    "title": "Regression Practice",
    "section": "Plot the Data and calculate r",
    "text": "Plot the Data and calculate r\nDoes the relationship look linear?\nWhat is the correlation coefficient, r?\nWhat does this r show?"
  },
  {
    "objectID": "5-Statistical_Tests_Part2/03-Regression_Practice.html#fit-a-linear-regression-model-2",
    "href": "5-Statistical_Tests_Part2/03-Regression_Practice.html#fit-a-linear-regression-model-2",
    "title": "Regression Practice",
    "section": "Fit a Linear Regression Model",
    "text": "Fit a Linear Regression Model\nAdd the regression line to your chart:\n\n# Sometimes you have to do the plot() and the abline() in one chunk and run the whole thing:\n\nWhat is the slope of the regression line, and what does it mean?\nWhat is the intercept and what does it mean?\nWhat is your p-value?\nWhat is your conclusion?\nWhat is the confidence interval for the slope?\nInterpret the confidence interval:"
  },
  {
    "objectID": "5-Statistical_Tests_Part2/03-Regression_Practice.html#check-model-requirements-2",
    "href": "5-Statistical_Tests_Part2/03-Regression_Practice.html#check-model-requirements-2",
    "title": "Regression Practice",
    "section": "Check Model Requirements",
    "text": "Check Model Requirements\nCheck the normality of the residuals:\nCheck for constant variance (Residual by Predicted plot):"
  },
  {
    "objectID": "5-Statistical_Tests_Part2/Summarizing_Categorical_Data.html",
    "href": "5-Statistical_Tests_Part2/Summarizing_Categorical_Data.html",
    "title": "Summarizing Categorical Data",
    "section": "",
    "text": "In this section we will show how to summarize data numerically and visually.\nWe typically summarize categorical variables with counts and proportions. Visually, an ordered bar chart is the optimal way to express categorical data. Pie charts, while very common, are problematic because of weaknesses in basic human perception.\nLet’s look at survey carried out by FiveThirtyEight about the first 6 Star Wars films.\n\nlibrary(tidyverse)\nlibrary(mosaic)\nlibrary(rio)\nlibrary(ggplot2)\n\nsw &lt;- read_csv('https://raw.githubusercontent.com/byuistats/Math221D_Cannon/master/Data/StarWarsData_clean.csv')\n\n\n\nUse the table() function to tabulate counts for a categorical variable. For example, if we want to tabulate the favorability of Han Solo\n\ntable(sw$`Favorability_Han Solo`)\n\n\nNeither favorably nor unfavorably (neutral) \n                                         44 \n                         Somewhat favorably \n                                        151 \n                       Somewhat unfavorably \n                                          8 \n                           Unfamiliar (N/A) \n                                         15 \n                             Very favorably \n                                        610 \n                           Very unfavorably \n                                          1 \n\n\nYou can also get proportions by inputting a table into the prop.table() function:\n\nprop.table(table(sw$`Favorability_Han Solo`))\n\n\nNeither favorably nor unfavorably (neutral) \n                                0.053075995 \n                         Somewhat favorably \n                                0.182147165 \n                       Somewhat unfavorably \n                                0.009650181 \n                           Unfamiliar (N/A) \n                                0.018094089 \n                             Very favorably \n                                0.735826297 \n                           Very unfavorably \n                                0.001206273 \n\n\nQuestion: What percent of respondents are “Very favorable” towards Han Solo?\nAnswer:\n\n\nThe table() function can make a “cross table” of 2 categorical variables. The resulting table will have rows and columns which correspond to the order of input table(row, column).\nLet’s contrast gender with whether or not a respondent is a fan of Star Wars (Are You a Fan of SW):\n\ntable(sw$Gender, sw$`Are You a Fan of SW?`)\n\n        \n          No Yes\n  Female 158 238\n  Male   119 303\n\n\nWe can include row and column totals by wrapping our table in the addmargins() function as follows:\n\naddmargins(table(sw$Gender, sw$`Are You a Fan of SW?`))\n\n        \n          No Yes Sum\n  Female 158 238 396\n  Male   119 303 422\n  Sum    277 541 818\n\n\nThis can be used to get row or column percentages. Alternatively we can use the prop.table() function to get proportions.\n\nprop.table(table(sw$Gender, sw$`Are You a Fan of SW?`))\n\n        \n                No       Yes\n  Female 0.1931540 0.2909535\n  Male   0.1454768 0.3704156\n\n\nThe default for prop.table() is to give the overall percentages (counts / table total). So the proportions add to 1 across the whole table.\nWe can specify row or column percentages by specifying a “margin.” In R, margin=1 corresponds to rows and margin = 2 corresponds to columns.\nCompare the difference:\n\nprop.table(table(sw$Gender, sw$`Are You a Fan of SW?`), margin = 1)\n\n        \n                No       Yes\n  Female 0.3989899 0.6010101\n  Male   0.2819905 0.7180095\n\n\nThis table sums to 1 across the rows, meaning that about 60% of Females are fans of Star Wars and about 72% of Males are fans.\nNow look at margin = 2\n\nprop.table(table(sw$Gender, sw$`Are You a Fan of SW?`), margin = 2)\n\n        \n                No       Yes\n  Female 0.5703971 0.4399261\n  Male   0.4296029 0.5600739\n\n\nQuestion: What does this table show?\nAnswer:\nNOTE: Which margin we choose to evaluate depends on the order we input columns into the table() function. Be sure to double check that you calculate the correct percentages.\n\n\n\n\nWe can use ggplot() with categorical variables to get summaries of counts using the geom_bar() geometry.\n\nggplot(sw, aes(x = `Are You a Fan of SW?`)) + geom_bar()\n\n\n\n\n\n\n\n\nWe can add another variable to the mix to look at things by gender using the fill= argument inside the aesthetics:\n\nggplot(sw, aes(x = who_shot_first, fill = Gender)) + geom_bar()\n\n\n\n\n\n\n\n\nThe default for geom_bar() is to stack bars. If we want side-by-side bars we can add a “position = ‘dodge’” to the geom_bar() function:\n\nggplot(sw, aes(x = who_shot_first, fill = Gender)) + geom_bar(position = \"dodge\")\n\n\n\n\n\n\n\n\n\n\nThe graphs above include missing values as its own category. The easiest way to deal with missing values is to create a subset of the data that is prepared for the graph we are interested in creating.\nWe first select only the columns that we will use in the visualization, then drop out all the missing values using drop_na() in the tidy fashion.\nNOTE: The drop_na() function drops all rows with ANY missing values. If we use this function on the dataset with all the columns, we may end up losing information on the analysis of interest. This is why we do a select() first. that way we only delete rows missing relevant information.\n\nshot_first &lt;- sw %&gt;%\n  select(who_shot_first, Gender) %&gt;%\n  drop_na()\n\nggplot(shot_first, aes(x = who_shot_first, fill = Gender)) + \n  geom_bar(position = \"dodge\")\n\n\n\n\n\n\n\n\n\n\n\nThe default visualization elements in ggplot() can always be improved. Here are some options for making the chart more readable:\n\nggplot(shot_first, aes(x = who_shot_first, fill = Gender)) + \n  geom_bar(position = \"dodge\") +\n  theme_bw() +\n  labs(\n    x = \"Which Character Shot First?\",\n    y = \"Count\",\n    title = \"Comparing response to the Question 'Who Shot First' by Gender\" \n  )\n\n\n\n\n\n\n\n\n\n\n\nWith categorical variables, we can group differently depending on which comparisons we would like to emphasize. Above, we grouped by responses to “who shot first” and colored by gender. If we swap the x variable and the color, we get the same bars, but arranged differently.\n\nggplot(shot_first, aes(x = Gender, fill = who_shot_first)) + \n  geom_bar(position = \"dodge\") +\n  theme_bw() +\n  labs(\n    x = \"\",\n    y = \"Count\",\n    title = \"Comparing response to the Question 'Who Shot First' by Gender\" \n  )\n\n\n\n\n\n\n\n\nThis different point of view makes it easier to see the breakdown of responses for each gender separately. We can see more clearly that the frequency of Females who do not understand the question is much more pronounced than on the Male side. Males, it seems largely agree that Han shot first."
  },
  {
    "objectID": "5-Statistical_Tests_Part2/Summarizing_Categorical_Data.html#numerical-summaries",
    "href": "5-Statistical_Tests_Part2/Summarizing_Categorical_Data.html#numerical-summaries",
    "title": "Summarizing Categorical Data",
    "section": "",
    "text": "Use the table() function to tabulate counts for a categorical variable. For example, if we want to tabulate the favorability of Han Solo\n\ntable(sw$`Favorability_Han Solo`)\n\n\nNeither favorably nor unfavorably (neutral) \n                                         44 \n                         Somewhat favorably \n                                        151 \n                       Somewhat unfavorably \n                                          8 \n                           Unfamiliar (N/A) \n                                         15 \n                             Very favorably \n                                        610 \n                           Very unfavorably \n                                          1 \n\n\nYou can also get proportions by inputting a table into the prop.table() function:\n\nprop.table(table(sw$`Favorability_Han Solo`))\n\n\nNeither favorably nor unfavorably (neutral) \n                                0.053075995 \n                         Somewhat favorably \n                                0.182147165 \n                       Somewhat unfavorably \n                                0.009650181 \n                           Unfamiliar (N/A) \n                                0.018094089 \n                             Very favorably \n                                0.735826297 \n                           Very unfavorably \n                                0.001206273 \n\n\nQuestion: What percent of respondents are “Very favorable” towards Han Solo?\nAnswer:\n\n\nThe table() function can make a “cross table” of 2 categorical variables. The resulting table will have rows and columns which correspond to the order of input table(row, column).\nLet’s contrast gender with whether or not a respondent is a fan of Star Wars (Are You a Fan of SW):\n\ntable(sw$Gender, sw$`Are You a Fan of SW?`)\n\n        \n          No Yes\n  Female 158 238\n  Male   119 303\n\n\nWe can include row and column totals by wrapping our table in the addmargins() function as follows:\n\naddmargins(table(sw$Gender, sw$`Are You a Fan of SW?`))\n\n        \n          No Yes Sum\n  Female 158 238 396\n  Male   119 303 422\n  Sum    277 541 818\n\n\nThis can be used to get row or column percentages. Alternatively we can use the prop.table() function to get proportions.\n\nprop.table(table(sw$Gender, sw$`Are You a Fan of SW?`))\n\n        \n                No       Yes\n  Female 0.1931540 0.2909535\n  Male   0.1454768 0.3704156\n\n\nThe default for prop.table() is to give the overall percentages (counts / table total). So the proportions add to 1 across the whole table.\nWe can specify row or column percentages by specifying a “margin.” In R, margin=1 corresponds to rows and margin = 2 corresponds to columns.\nCompare the difference:\n\nprop.table(table(sw$Gender, sw$`Are You a Fan of SW?`), margin = 1)\n\n        \n                No       Yes\n  Female 0.3989899 0.6010101\n  Male   0.2819905 0.7180095\n\n\nThis table sums to 1 across the rows, meaning that about 60% of Females are fans of Star Wars and about 72% of Males are fans.\nNow look at margin = 2\n\nprop.table(table(sw$Gender, sw$`Are You a Fan of SW?`), margin = 2)\n\n        \n                No       Yes\n  Female 0.5703971 0.4399261\n  Male   0.4296029 0.5600739\n\n\nQuestion: What does this table show?\nAnswer:\nNOTE: Which margin we choose to evaluate depends on the order we input columns into the table() function. Be sure to double check that you calculate the correct percentages."
  },
  {
    "objectID": "5-Statistical_Tests_Part2/Summarizing_Categorical_Data.html#visual-summaries",
    "href": "5-Statistical_Tests_Part2/Summarizing_Categorical_Data.html#visual-summaries",
    "title": "Summarizing Categorical Data",
    "section": "",
    "text": "We can use ggplot() with categorical variables to get summaries of counts using the geom_bar() geometry.\n\nggplot(sw, aes(x = `Are You a Fan of SW?`)) + geom_bar()\n\n\n\n\n\n\n\n\nWe can add another variable to the mix to look at things by gender using the fill= argument inside the aesthetics:\n\nggplot(sw, aes(x = who_shot_first, fill = Gender)) + geom_bar()\n\n\n\n\n\n\n\n\nThe default for geom_bar() is to stack bars. If we want side-by-side bars we can add a “position = ‘dodge’” to the geom_bar() function:\n\nggplot(sw, aes(x = who_shot_first, fill = Gender)) + geom_bar(position = \"dodge\")\n\n\n\n\n\n\n\n\n\n\nThe graphs above include missing values as its own category. The easiest way to deal with missing values is to create a subset of the data that is prepared for the graph we are interested in creating.\nWe first select only the columns that we will use in the visualization, then drop out all the missing values using drop_na() in the tidy fashion.\nNOTE: The drop_na() function drops all rows with ANY missing values. If we use this function on the dataset with all the columns, we may end up losing information on the analysis of interest. This is why we do a select() first. that way we only delete rows missing relevant information.\n\nshot_first &lt;- sw %&gt;%\n  select(who_shot_first, Gender) %&gt;%\n  drop_na()\n\nggplot(shot_first, aes(x = who_shot_first, fill = Gender)) + \n  geom_bar(position = \"dodge\")\n\n\n\n\n\n\n\n\n\n\n\nThe default visualization elements in ggplot() can always be improved. Here are some options for making the chart more readable:\n\nggplot(shot_first, aes(x = who_shot_first, fill = Gender)) + \n  geom_bar(position = \"dodge\") +\n  theme_bw() +\n  labs(\n    x = \"Which Character Shot First?\",\n    y = \"Count\",\n    title = \"Comparing response to the Question 'Who Shot First' by Gender\" \n  )\n\n\n\n\n\n\n\n\n\n\n\nWith categorical variables, we can group differently depending on which comparisons we would like to emphasize. Above, we grouped by responses to “who shot first” and colored by gender. If we swap the x variable and the color, we get the same bars, but arranged differently.\n\nggplot(shot_first, aes(x = Gender, fill = who_shot_first)) + \n  geom_bar(position = \"dodge\") +\n  theme_bw() +\n  labs(\n    x = \"\",\n    y = \"Count\",\n    title = \"Comparing response to the Question 'Who Shot First' by Gender\" \n  )\n\n\n\n\n\n\n\n\nThis different point of view makes it easier to see the breakdown of responses for each gender separately. We can see more clearly that the frequency of Females who do not understand the question is much more pronounced than on the Male side. Males, it seems largely agree that Han shot first."
  },
  {
    "objectID": "5-Statistical_Tests_Part2/Summarizing_Categorical_Data.html#visualization",
    "href": "5-Statistical_Tests_Part2/Summarizing_Categorical_Data.html#visualization",
    "title": "Summarizing Categorical Data",
    "section": "Visualization",
    "text": "Visualization\nCreate a bar chart for favorability of Han Solo by whether or not they are fans of Star Trek (fan_of_star_trek).\nStart by making a new dataset called trekky that only includes the 2 relevant columns and drops the missing values.\n\n\ntrekky &lt;- sw %&gt;%\n\nError: &lt;text&gt;:4:0: unexpected end of input\n2: trekky &lt;- sw %&gt;%\n3: \n  ^\n\n\nQuestion: What observations can you make based on the visualization?\nAnswer:"
  },
  {
    "objectID": "5-Statistical_Tests_Part2/Summarizing_Categorical_Data.html#proportion-table",
    "href": "5-Statistical_Tests_Part2/Summarizing_Categorical_Data.html#proportion-table",
    "title": "Summarizing Categorical Data",
    "section": "Proportion Table",
    "text": "Proportion Table\nWe would like to compare what percent of female respondents do not understand the question compared to the percent of males who do not understand the question.\nCreate a proportion table that can answer this question:\nQuestion: What percent of female respondents do not understand the question?\nAnswer:\nQuestion: What percent of male respondents do not understand the question?\nAnswer:"
  },
  {
    "objectID": "5-Statistical_Tests_Part2/05-Distribution_of_Phat.html",
    "href": "5-Statistical_Tests_Part2/05-Distribution_of_Phat.html",
    "title": "Sampling Distribution of P_hat",
    "section": "",
    "text": "Categorical data is often summarized as a percent. If we randomly select 500 students and find 276 have brown hair, we can estimate the population proportion using \\(\\hat{p} = \\frac{X}{N}\\) where \\(X\\) is the number with brown hair and N is the number in our sample. We often interchange percents and proportions, but strictly speaking, a proportion is a number between 0 and 1. This can be interpreted as a probability as well.\nIf another researcher were to collect a different sample of 500 from the same population, they would almost certainly get a different number of people with brown hair than the first study. We can imagine taking many samples of 500 students and imagine the theoretical distribution of all possible \\(\\hat{p}\\). If we are taking good samples, most of these \\(\\hat{p}\\)’s should be near the population proportion, \\(p\\).\nIn fact, under certain conditions we can know the distribution of \\(\\hat{p}\\). As you probably guessed, the distribution of \\(\\hat{p}\\) is approximately normal with:\n\\[\\mu_{\\hat{p}} = p\\] \\[\\sigma_{\\hat{p}} = \\sqrt{\\frac{p(1-p)}{N}}\\]\nThat is to say, the mean of all sample proportions is the population proportion.\nJust as it was with a sample mean, \\(\\bar{x}\\), we have to check certain conditions to assume the distribution is approximately normal. For \\(\\bar{x}\\), we needed the population to be normally distributed or to have a sample size greater than 30. The principle of having a large enough sample applies, but it’s different for a proportion.\nWe can assume the distribution is approximately normal if:\n\\[np \\geq 10\\] \\[n(1-p) \\geq 10\\]\nIn plain English, this means our sample size has to be big enough to have at least 10 “successes” and 10 “failures”. For example, if we’re estimating the proportion of left handed people, we would need a sample size large enough to have at least 10 left handed people and 10 right handed people.\nIf the distribution of sample means is approximately normal according to the conditions above, we can calculate a z-score as we did in Unit 1 and 2:\n\\[z = \\frac{\\hat{p}-p}{\\sqrt{\\frac{p(1-p)}{N}}}\\]\nthen use pnorm() as before."
  },
  {
    "objectID": "5-Statistical_Tests_Part2/05-Distribution_of_Phat.html#example",
    "href": "5-Statistical_Tests_Part2/05-Distribution_of_Phat.html#example",
    "title": "Sampling Distribution of P_hat",
    "section": "Example",
    "text": "Example\nSuppose we have a population where the true proportion of success is \\(p = 0.6\\). We take a random sample of size \\(n = 100\\) from this population. We want to find the probability that the sample proportion \\(\\hat{p}\\) is less than \\(0.55\\).\n\n# Given data\nx &lt;- 55\nn &lt;- 100\np_hat &lt;- x/n\n\n# population proportion\np &lt;- 0.6\n\n# Calculate standard deviation\nsigma_phat &lt;- sqrt(p * (1 - p) / n)\n\n# Calculate z-score\nz &lt;- (p_hat - p) / sigma_phat\n\n# Left Tail (lower than p)\npnorm(z)\n\n[1] 0.1537171\n\n# Right Tail (greater than p)\n1-pnorm(z)\n\n[1] 0.8462829"
  },
  {
    "objectID": "5-Statistical_Tests_Part2/05-Distribution_of_Phat.html#your-turn",
    "href": "5-Statistical_Tests_Part2/05-Distribution_of_Phat.html#your-turn",
    "title": "Sampling Distribution of P_hat",
    "section": "Your Turn",
    "text": "Your Turn\nThe nationwide, fully-vaccinated rate if November 2021 was 58%. Based on survey responses of 150, 81% of all BYU-I students on campus during Fall 2021 semester had received at least one vaccination dose against COVID-19 with 74% being fully vaccinated.\nWhat is the probability that we get a random sample of 150 individuals with a \\(\\hat{p}\\) higher than the fully vaccinated rate that we observed?\n\n# given data\n\n# Population Proportion\n\n# Calculate Z\n\n# P-value"
  },
  {
    "objectID": "5-Statistical_Tests_Part2/06-Distribution_of_Phat_Practice.html",
    "href": "5-Statistical_Tests_Part2/06-Distribution_of_Phat_Practice.html",
    "title": "Distribution of P-hat Practice",
    "section": "",
    "text": "Instructions\nComplete the following questions about the sampling distribution of \\(\\hat{p}\\). When completed, Render the qmd file and submit the html.\n\n\nQuestions\nQuestion: When can we use the normal distribution to approximate the sampling distribution of \\(\\hat{p}\\)?\nAnswer:\nQuestion: What is the mean of the sampling distribution of \\(\\hat{p}\\)?\nAnswer:\nQuestion: What is the standard deviation of the sampling distribution of \\(\\hat{p}\\)?\nAnswer:\nSuppose the true population proportion, \\(p\\), of people who support a candidate for office is 52%. We would like to learn something about a sample proportion, \\(\\hat{p}\\), with a sample size \\(n=1000\\) suggesting that the candidate will lose the election (\\(\\hat{p}&lt;0.50\\)).\nUse the following R code to answer the questions below:\n\nphat &lt;- \np &lt;-\nn &lt;- \n\nsigma_phat &lt;- sqrt(p*(1-p)/n)  \n\nError in eval(expr, envir, enclos): object 'p' not found\n\nz &lt;- (phat-p) / sigma_phat\n\nError in eval(expr, envir, enclos): object 'phat' not found\n\n# Left Tail:\npnorm()\n\nError in pnorm(): argument \"q\" is missing, with no default\n\n# Right tail\n1-pnorm()\n\nError in pnorm(): argument \"q\" is missing, with no default\n\n\nQuestion: What is the \\(z\\)-score associated with \\(\\hat{p}&lt;0.5\\)?\nAnswer:\nQuestion: What is the standard deviation of \\(\\hat{p}\\)?\nAnswer:\nQuestion: What is the probability of a sample of size \\(n=1000\\) suggesting that the candidate will lose even if the true population support, \\(p=0.52\\)?\nAnswer:"
  },
  {
    "objectID": "5-Statistical_Tests_Part2/07-One_Sample_Proportion_Ztest.html",
    "href": "5-Statistical_Tests_Part2/07-One_Sample_Proportion_Ztest.html",
    "title": "One-sample Proportion Tests",
    "section": "",
    "text": "In statistics, one-sample proportion tests are used to compare proportions or percentages to a hypothesized value. These tests are useful when dealing with categorical data. We here discuss these tests and provide examples of their application in R.\nThe hypothesis test should look very familiar:\n\\[H_0: p = p_0\\] \\[H_a: p\\; (&lt;,&gt;,\\neq) \\: p_0\\] where \\(p_0\\) is some hypothesized value for the population proportion.\nSo far, we have been using Greek letters to represent population parameters. We deviate from that now due to the fact that the Greek letter for p, \\(\\pi\\), already has a long-established meaning in mathematics. In the hypothesis definition above, \\(p\\) represents the population proportion."
  },
  {
    "objectID": "5-Statistical_Tests_Part2/07-One_Sample_Proportion_Ztest.html#example-1-one-sample-proportion-test",
    "href": "5-Statistical_Tests_Part2/07-One_Sample_Proportion_Ztest.html#example-1-one-sample-proportion-test",
    "title": "One-sample Proportion Tests",
    "section": "Example 1: One Sample Proportion Test",
    "text": "Example 1: One Sample Proportion Test\nSuppose we want to test whether the proportion of students who passed an exam is significantly less than 0.75. We have a sample of 100 students, of which 72 passed.\n\\[\\hat{p} = \\frac{X}{N} = \\frac{72}{100} = .72\\]\nIf we want to test if this is significantly less than 75%, we can use the prop.test() which is very similar to t.test(). Instead of putting in a sample mean, \\(\\bar{x}\\), with a hypothesized \\(\\mu\\), we put in \\(X\\) and \\(N\\) and a hypothesized \\(p\\). Setting the alternative and confidence level operates the same as t.test().\nConfidence intervals for proportions for proportions can also be obtained just as with t.test().\n\n# One Sample Proportion Test Example\n# Hypothesized proportion: 0.75\n# Sample size: 100\n# Number of successes: 72\n\nprop.test(x = 72, n = 100, p = 0.75, alternative = \"less\", conf.level = .9)\n\n\n    1-sample proportions test with continuity correction\n\ndata:  72 out of 100, null probability 0.75\nX-squared = 0.33333, df = 1, p-value = 0.2819\nalternative hypothesis: true p is less than 0.75\n90 percent confidence interval:\n 0.0000000 0.7782396\nsample estimates:\n   p \n0.72 \n\nprop.test(x = 72, n = 100, conf.level = .9)$conf.int\n\n[1] 0.6358512 0.7917861\nattr(,\"conf.level\")\n[1] 0.9\n\n\n\nThe distribution of \\(\\hat{p}\\)\nRecall that the sampling distribution for \\(\\hat{p}\\) is approximately normally distributed when our sample has more than 10 expeted “successes” and more than 10 expected “failures”. We test this by looking at\nHypothesis Test Requirements:\n\\[np \\geq 10\\] \\[n(1-p) \\geq 10\\]\nWe use p, not \\(\\hat{p}\\) for hypothesis testing because hypothesis testing always assumes the null hypothesis is true. Confidence intervals, on the other hand, make no such assumption.\nTo see if the calculated confidence interval is appropriate, we use \\(\\hat{p}\\).\nConfidence Interval Requirements: \\[n\\hat{p} \\geq 10\\] \\[n(1-\\hat{p}) \\geq 10\\] Can we trust the p-value and confidence interval?\nYou can use a calculator for this, or simply use R as a calculator:\n\nx &lt;- 72\nn &lt;- 100\np_hat &lt;- x/n\np &lt;- .75\n\n# For Hypothesis Testing:\nn*p &gt;= 10\n\n[1] TRUE\n\nn*(1-p) &gt;= 10\n\n[1] TRUE\n\n# For Confidence Intervals:\nn*p_hat &gt;= 10\n\n[1] TRUE\n\nn*(1-p_hat) &gt;=10\n\n[1] TRUE"
  },
  {
    "objectID": "5-Statistical_Tests_Part2/07-One_Sample_Proportion_Ztest.html#example-2-handedness",
    "href": "5-Statistical_Tests_Part2/07-One_Sample_Proportion_Ztest.html#example-2-handedness",
    "title": "One-sample Proportion Tests",
    "section": "Example 2: Handedness",
    "text": "Example 2: Handedness\nSuppose the United States national average percent of left-handed people is 11%. A researcher wants to know if visual arts majors are significantly more likely to be left handed. She samples 250 visual arts majors and finds that 36 are left handed.\nPerform a one-sample proportion to see if visual arts majors are significantly more left-handed than the general population.\nState the null and alternative hypotheses and your significance level.\n\\[H_0: p = \\] \\[H_a: p \\] \\[\\alpha = \\]\nQuestion: What is the value of the test statistics for this test?\nAnswer:\nQuestion: What is the P-Value?\nAnswer:\nQuestion: State your conclusion in context of this problem:\nAnswer:\nMake a \\((1-\\alpha)\\) level confidence interval for the true population proportion.\nQuestion: Interpret the confidence interval in context of the question:\nAnswer:\nQuestion: Are the test requirements for the normality of \\(\\hat{p}\\) satisfied?\nAnswer:\nQuestion: Are the requirements for a confidence interval for \\(p\\) satisfied?\nAnswer:"
  },
  {
    "objectID": "5-Statistical_Tests_Part2/11-Chi_Square_Intro.html",
    "href": "5-Statistical_Tests_Part2/11-Chi_Square_Intro.html",
    "title": "Chi-square Test of Independence",
    "section": "",
    "text": "When working through statistical inference for means, we progressed from learning 1-sample t-tests to 2-sample t-tests. When we wanted to compare a quantitative variable between multiple groups, we introduced ANOVA. The hypothesis test changed and we introduced the F-statistic.\nRecall that the F-statistics was based on a ratio of squared quantities and was therefore always positive and skewed right.\nSimilarly, when we want to compare a categorical variable across multiple groups, we must modify the hypothesis test from the 2-sample proportion test and introduce a new test statistic: \\(\\chi^2\\). The Greek letter, \\(\\chi\\), is pronounced like “ki” in “kite”, not like “chi” in “tai chi”.\nAs can be seen from its name, \\(\\chi^2\\) is a squared value and is thus always positive and right skewed like the F-statistic.\n\n\nThe \\(\\chi^2\\) distribution also has degrees of freedom that determine its shape.\n\\[df = (r-1)(c-1)\\] Where \\(r\\) is the number of rows and \\(c\\) is the number of columns in a summary table.\n\n\n\nThe null and alternative hypotheses test for \\(\\chi^2\\) test for independence are always the same.\n\\[H_0: \\text{The row variable is independent of the column variable}\\] \\[H_A: \\text{The row variable is not independent of the column variable}\\] While not a fan of the double negative, it serves a technical purpose. Mathematically, we get the same test statistic and p-value if we swap rows and columns. We cannot say the row variable depends on the column variable without also saying that the column variable depends on the row variable.\nThink of Alice at the Mad Hatter’s tea party:\n\n“Then you should say what you mean,” the March Hare went on. “I do,” Alice hastily replied; “at least-at least I mean what I say-that’s the same thing, you know.”\n“Not the same thing a bit!” said the Hatter. “Why, you might just as well say that ‘I see what I eat’ is the same thing as ‘I eat what I see’!”\n“You might just as well say,” added the March Hare, “that ‘I like what I get’ is the same thing as ‘I get what I like’!”\n“You might just as well say,” added the Dormouse, which seemed to be talking in its sleep, “that ‘I breathe when I sleep’ is the same thing as ‘I sleep when I breathe’!”\n“It is the same thing with you.” said the Hatter,”\n\nSo we are resigned to conclude that we have sufficient/insufficient evidence that they are not independent."
  },
  {
    "objectID": "5-Statistical_Tests_Part2/11-Chi_Square_Intro.html#degrees-of-freedom",
    "href": "5-Statistical_Tests_Part2/11-Chi_Square_Intro.html#degrees-of-freedom",
    "title": "Chi-square Test of Independence",
    "section": "",
    "text": "The \\(\\chi^2\\) distribution also has degrees of freedom that determine its shape.\n\\[df = (r-1)(c-1)\\] Where \\(r\\) is the number of rows and \\(c\\) is the number of columns in a summary table."
  },
  {
    "objectID": "5-Statistical_Tests_Part2/11-Chi_Square_Intro.html#hypothesis-test",
    "href": "5-Statistical_Tests_Part2/11-Chi_Square_Intro.html#hypothesis-test",
    "title": "Chi-square Test of Independence",
    "section": "",
    "text": "The null and alternative hypotheses test for \\(\\chi^2\\) test for independence are always the same.\n\\[H_0: \\text{The row variable is independent of the column variable}\\] \\[H_A: \\text{The row variable is not independent of the column variable}\\] While not a fan of the double negative, it serves a technical purpose. Mathematically, we get the same test statistic and p-value if we swap rows and columns. We cannot say the row variable depends on the column variable without also saying that the column variable depends on the row variable.\nThink of Alice at the Mad Hatter’s tea party:\n\n“Then you should say what you mean,” the March Hare went on. “I do,” Alice hastily replied; “at least-at least I mean what I say-that’s the same thing, you know.”\n“Not the same thing a bit!” said the Hatter. “Why, you might just as well say that ‘I see what I eat’ is the same thing as ‘I eat what I see’!”\n“You might just as well say,” added the March Hare, “that ‘I like what I get’ is the same thing as ‘I get what I like’!”\n“You might just as well say,” added the Dormouse, which seemed to be talking in its sleep, “that ‘I breathe when I sleep’ is the same thing as ‘I sleep when I breathe’!”\n“It is the same thing with you.” said the Hatter,”\n\nSo we are resigned to conclude that we have sufficient/insufficient evidence that they are not independent."
  },
  {
    "objectID": "5-Statistical_Tests_Part2/11-Chi_Square_Intro.html#test-requirements",
    "href": "5-Statistical_Tests_Part2/11-Chi_Square_Intro.html#test-requirements",
    "title": "Chi-square Test of Independence",
    "section": "Test Requirements",
    "text": "Test Requirements\nRecall that 2-sample tests for proportions needed an at least 10 expected successes and at least 10 expected failures (\\(np \\ge 10\\) and \\(n(1-p)\\ge10\\)) for the test statistic to be valid.\nFor a \\(\\chi^2\\) test we need to check the expected counts for all the different combinations.\nWe don’t need to fret about the math behind the expected count calculation. Intuitively, if there was no relationship between the two variables you would expect all the row totals to be proportionally distributed across the column groups.\nWe only need to check that all expected counts are greater than 5.\n\nchisq.test(chiro_table)$expected &gt;= 5\n\n               \n                At Risk Prevention Self Care Sick Role Wellness\n  Australia        TRUE       TRUE      TRUE      TRUE     TRUE\n  Europe           TRUE       TRUE      TRUE      TRUE     TRUE\n  United States    TRUE       TRUE      TRUE      TRUE     TRUE"
  },
  {
    "objectID": "5-Statistical_Tests_Part2/11-Chi_Square_Intro.html#visualization",
    "href": "5-Statistical_Tests_Part2/11-Chi_Square_Intro.html#visualization",
    "title": "Chi-square Test of Independence",
    "section": "Visualization",
    "text": "Visualization\nWe can use ggplot() to create nice bar charts to help interpret the results.\n\nggplot(chiropractic, aes(x = location, fill = motivation)) +\n  geom_bar(position = \"dodge\") +\n  theme_minimal()"
  },
  {
    "objectID": "5-Statistical_Tests_Part2/11-Chi_Square_Intro.html#homework-grades-and-classroom-type",
    "href": "5-Statistical_Tests_Part2/11-Chi_Square_Intro.html#homework-grades-and-classroom-type",
    "title": "Chi-square Test of Independence",
    "section": "Homework: Grades and Classroom Type",
    "text": "Homework: Grades and Classroom Type\nThis is the raw data for the Homework Quiz. Use it to answer the homework questions, but also create a Bar Chart using ggplot().\n\ncourse &lt;- import('https://raw.githubusercontent.com/byuistats/Math221D_Cannon/master/Data/course_type_by_grade.csv')\n\nCreate a Bar Chart using ggplot()\nPerform the \\(\\chi^2\\) test of independence.\nQuestion: Are the requirements for a \\(\\chi^2\\) test satisfied?\nAnswer:\nQuestion: What are the null and alternative hypotheses?\nAnswer:\nQuestion: What is the test statistic, \\(\\chi^2\\)?\nAnswer:\nQuestion: How many degrees of freedom does this test have?\nAnswer:\nQuestion: What is the P-value?\nAnswer:"
  },
  {
    "objectID": "5-Statistical_Tests_Part2/11-Chi_Square_Intro.html#heart-disease-among-australian-women-and-men",
    "href": "5-Statistical_Tests_Part2/11-Chi_Square_Intro.html#heart-disease-among-australian-women-and-men",
    "title": "Chi-square Test of Independence",
    "section": "Heart Disease among Australian Women and Men",
    "text": "Heart Disease among Australian Women and Men\nIn 1982 in Western Australia, 1317 males and 854 females died of ischemic heart disease, 1119 males and 828 females died of cancer, 371 males and 460 females died of cerebral vascular disease, and 346 males and 147 females died of accidents. A medical researcher wanted to see if gender and cause of death are independent using a level of significance of 0.05.\nThe data read in below are a summary table of counts. One way to create a bar chart to compare heart disease deaths between men and women is to take this data and make it “longer”. This stacks the columns with count data in them and makes 2 new columns, one for the counts and one for the category of cardiovascular death.\nI will comment the code below to walk through each step.\nThe %&gt;% “pipe” below comes from the tidyverse library. You can think of this like making a series of steps where everything before the %&gt;% is pushed to the next step. For example, we assign aussie_death the original data table then move the table into the pivot_long() function which is the function that stacks the data. The output of that function becomes a new data shape, aussie_death.\n\n# Import the table data\naussie_death_table &lt;- import(\"https://byuistats.github.io/M221R/Data/quiz/R/aussie_death.csv\") \naussie_death_table\n\n      V1 heart_disease cancer vascular_disease accident\n1 female           854    828              460      147\n2   male          1371   1119              371      346\n\n# Pipe the original table into the pivot_longer() function\n# Pivot_longer needs to know which columns to \"stack\" which is input by the 'cols = ' argument.\n# We also need to give a name to the new column containing the count information.  The 'values_to=' argument names the column that will have the values, in this case we use \"count\".  \n# The 'names_to = ' argument names the column that will contain the labels of each category\n\n# Run the code and see if you can follow what happened\n\naussie_death &lt;- aussie_death_table %&gt;% \n  pivot_longer(cols = c('heart_disease', 'cancer', 'vascular_disease', 'accident'), values_to = 'count', names_to = \"reason\")\n\naussie_death\n\n# A tibble: 8 × 3\n  V1     reason           count\n  &lt;chr&gt;  &lt;chr&gt;            &lt;int&gt;\n1 female heart_disease      854\n2 female cancer             828\n3 female vascular_disease   460\n4 female accident           147\n5 male   heart_disease     1371\n6 male   cancer            1119\n7 male   vascular_disease   371\n8 male   accident           346\n\n# V1 was the default label and really represents \"Gender\" in this study.  We could change the name or leave it as is and fix it in the graphs\n\n\n# When using raw data, 'geom_bar()' creates the counts automatically from the categorical columns in a raw dataset.  The data in aussie_death is still a summary.  If we have the counts already, then we use the `geom_col()` function and have to specify a y variable to define how high to make the bars (in our case \"count\")\n\nggplot(aussie_death, aes(x = reason, y = count, fill = V1)) +\n  geom_col(position=\"dodge\")"
  }
]