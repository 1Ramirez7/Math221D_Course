---
title: "Simple Linear Regression"
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, error = TRUE)
```


# Review

Consider the relationship between Score on a math exam and a student's self-reported Confidence Rating.


```{r}

library(tidyverse)
library(mosaic)
library(rio)
library(car)

math <- import('https://byuistats.github.io/BYUI_M221_Book/Data/MathSelfEfficacy.xlsx')
```

__Question__: What is the explanatory (aka independent) variable, $x$?  
__Answer__:  

__Question__: What is the response (aka the dependent) variable, $y$?  
__Answer__:  


Plot the relationship:

```{r}

plot(Score ~ ConfidenceRatingMean, data = math)

ggplot(math, aes(x = ConfidenceRatingMean, y = Score )) +
  geom_point(color = "darkblue") +
  theme_bw() +
  labs(
    title = "Relationship between Student Confidence Rating in Math and Test Score"
  ) 
```

__Question__:  Does the relationship appear linear?  
__Answer__:  

__Question__: What is the direction of the relationship?
__Answer__:  

__Question__: What do you think is the strength of the relationship?  (Strong/Moderate/Weak)
__Answer__:  

__Question__: What is the correlation coefficient, r?
__Answer__:  

```{r}

cor(Score ~ ConfidenceRatingMean, data = math)

```


# Linear Regression Equation

Recall that the correlation coefficient, $r$, tells us something about the direction and strength of a __linear__ relationship.


Linear Regression allows us to find a functional relationship between X and Y:    

$$y = \beta_0 + \beta_1x_1 +\epsilon$$

This is a reframing of the familiar $y=mx+b$ but we change the intercept and the slope to be Greek letters signifying *population parameters*.  We also add a very important term, $\epsilon$, which signifies the "error" around the line.  Our estimate of error, $\epsilon$, will be critical to assessing the usefulness of our model.

We can estimate the slope and intercept of the line that "best fits" the data and use that line to make predictions.  

__NOTE__:  As before, Greek letters signify population parameters in statistics while Latin letters are often used for sample statistics.  We estimate $\beta_0$ and $\beta_1$ with $b_0$ and $b_1$ respectively. This leads to an equation for predicted values:  

$$\hat{y}=b_0+b_1x_1$$
where $\hat{y}$ signifies a predicted value for $y$ for a given $x$.

# In R

Recall that when we had a quantitative response variable and a categorical explanatory variable we could use the `favstats()` and `boxplot()`with the general formula:  

<br>
`favstats(data$response_variable ~ data$explanatory_variable)` 
`boxplot(data$response_variable ~ data$explanatory_variable)` 
<br>

The same paradigm works for estimating the linear relationship between 2 quantitative variables, but we need to introduce a new function:  `lm()`.  The 'lm' stands for "linear model".  The relationship notation remains the same:  
<br>

`lm(data$response_variable ~ data$explanatory_variable)`

or equivalently:  

`lm(response_variable ~ explanatory_variable, data = dat)`

<br>

except $x$ and $y$ are both quantitative.  The `lm()` output by itself doesn't give us test statistics and p-values.  We will use an additional function, `summarize()`, to get the relevant information for the hypothesis test.  

Let's fit a linear model estimating the slope and intercept of the line that best fits the relationship between Confidence Rating and Test Score. 

Start by naming the `lm()` output, then do a summary on our named object:  

```{r}

math_lm <- lm(Score ~ ConfidenceRatingMean, data = math)
summary(math_lm)

confint(math_lm, level = .92)

```

The `lm()` output includes only the slope and the intercept. 

__Question__:  How do we interpret the intercept of 18.69?  
__Answer__:  

__Question__:  How do we interpret the slope of 12.69 in context?  
__Answer__:  

Now that we have an estimate of the slope and the intercept, we can plug in values of $x$ into our formula, $\hat y = b_o + b_1x = 18.69 + 12.69*x$.  

__Question__:  What is the expected score on a test of someone who ranks themselves 5 in confidence?  
__Answer__:  

```{r}

18.69 + 12.69*5

```



## Plotting the Regression Line

### Base R

Scatter plots by themselves are nice, but we would also like to see the regression line.  Simple graphics in R can be augmented by using some functions.  The `abline()` function in base R, when executed right after a graphing function can add lines.  We've used this to add vertical lines and horizontal line already in class.  We can also use this function to add a regression line.  We simply insert our linear model output into the `abline()` function as follows:


## GGPlot

Using `ggplot()`, we can simply add a `geom_smooth()` geometry and specify the method of "smoothing" as a linear model:  

```{r}

ggplot(math, aes(x = ConfidenceRatingMean, y = Score )) +
  geom_point(color = "darkblue") +
  theme_bw() +
  labs(
    title = "Relationship between Student Confidence Rating in Math and Test Score"
  ) +
  geom_smooth(method="lm")

```

By default, this gives us a confidence interval for the slope of the regression line.  We can turn that off by forcing the "standard error" to be `FALSE`:

```{r}

ggplot(math, aes(x = ConfidenceRatingMean, y = Score )) +
  geom_point(color = "darkblue") +
  theme_bw() +
  labs(
    title = "Relationship between Student Confidence Rating in Math and Test Score"
  ) +
  geom_smooth(method="lm", se = FALSE)

```

## Hypothesis Testing for Regression

A linear equation has 2 parameters: Slope and Intercept.  In most situations, the intercept isn't very interesting by itself and is often absurd. We are most often only interested in the slope:  

$$H_o: \beta_1 = 0$$
$$H_a: \beta_1 \neq 0$$

These hypotheses are the same for all simple linear regression questions.  

To get the p-value and test statistics, we use the `summary()` function as we did with aov:

```{r}
summary(math_lm)
```


We can also calculate confidence intervals for the slope by using the `confint()` function.  This function requires you to tell it which model to extract a confidence intervals from.  You can specify which parameter you're interested in, and the level of confidence:  

```{r}
# input the model into the following function:
confint(math_lm, level = .95)
```


How do we interpret this confidence interval for a slope?

Technically correct:  95% Confident that the true population slope is within (10.674297, 14.71535)

Contextual Explanation:  For every 1 unit increase in Confidence Rating, test scores go up by between (10.674297, 14.71535) on average.  


## Regression Requirements (Is our P-value sus?)

There are certain requirements for all statistical tests to be valid.  For regression analysis, we have to have 5 requirements for our statistics to be valid.  While this sounds daunting, in practice we can check most of them very quickly.  

  1. Relationship between X and Y is Linear
  2. The residuals, $\epsilon$, are normally distributed
  3. The Variance of the error terms is constant for all values of X
  4. The X’s are fixed and measured without error (i.e. X’s can be considered as known constants)
  5. The observations are independent

The linear relationship is assessed visually with the scatter plot.  If there is obvious curvature or non-linearity then fitting a line isn't the best model.  

We check the normality of the residuals with a `qqPlot(lm_output)`.  

Constant variance in regression is checked with a new plot that looks at how the predicted values relate to the residuals.  This is done by:  `plot(lm_output, which=1)`.  This is important because we want our predictions to be "wrong" about the same regardless of the value of the prediction.  We're looking for random scatter.  

Requirement 4 cannot be analyzed directly.  It is important because because $x$ is the independent variable.  If there is uncertainty about the input, then the simple linear regression might not be the most appropriate model.  

Requirement 5 also cannot be analyzed, but random sampling usually satisfies this requirement.  

```{r}

# Requirement 1:  Check for linear relationship
ggplot(math, aes(x=ConfidenceRatingMean, y = Score)) + 
  geom_point()

# Req 2: Normality of residuals:
qqPlot(math_lm$residuals)

# Req 3: Constant variance (look odd patterns). When you put lm() output into the plot function it gives you several different plots. The residual plots we're most interested in are 1 and 2

plot(math_lm, which = 1)

```

# Your Turn

Look at the relationship between Alcohol Consumption compared to life expectancy for selected countries.  


```{r}
alcohol <- import('https://raw.githubusercontent.com/byuistats/Math221D_Cannon/master/Data/life_expectancy_vs_alcohol_consumption.csv')
```

__Question__:  What is the response variable?  
__Answer__:  

__Question__:  What is the explanatory variable?  
__Answer__:  


Make a presentation-worthy scatterplot using `ggplot()` including a regression line:

```{r}

```


__Question__:  Describe in words the nature of the relationship between the 2 variables (direction/strength/linearity)
__Answer__:  

Calculate the correlation coefficient:

```{r}

```

__Question__:  What is the the correlation coefficient, $r$?  
__Answer__:  

Perform a linear regression analysis for the relationship between alcohol consumption and life expectancy. 

```{r}

# Save your linear model:  

# Check for constant variance:  

# Check the normality of the residuals  


```


__Question__:  What is the slope of the regression line?  
__Answer__:  

__Question__:  Interpret the slope of the regression line in context of the problem:  
__Answer__:  

__Question__:  What is the intercept of the regression line?  
__Answer__:  

__Question__:  Interpret the intercept of the regression line in context of the problem:  
__Answer__:  


__Question__:  What do you conclude about the slope of the regression line?    
__Answer__:  

__Question__:  What is the confidence interval for the slope? 

__Question__:  Give a 1-sentence interpretation of the confidence interval in context of your research question:  
__Answer__:  

