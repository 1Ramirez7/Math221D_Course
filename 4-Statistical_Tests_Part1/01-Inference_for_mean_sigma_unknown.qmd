---
title: "Inference for a Mean"
subtitle: "Hypothesis Testing and Confidence Intervals"
---
```{r setup, include=FALSE}
# Ignore this bit of code.  This makes it so that you can render the qmd file even when there are errors in your code.
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, error = TRUE)
```


# Introduction

__Statistical Inference__ is the practice of using data sampled from a population to make conclusions about *population parameters*.  

The two primary methods of statistical inference are:  

  1. Hypothesis Testing
  2. Confidence Intervals
  
Hypothesis testing is the practice of gathering evidence in an attempt to disprove a hypothesis.  We either **reject** the conventional wisdom or **fail to reject**.  Proper scientific questions are falsifiable, meaning they can be disproven.  

Confidence intervals are a way to use data to estimate the unknown population parameter, often the population mean, $\mu$.  

Both of these methods rely on the validity of the assuming that the distribution of the sample mean is normally distributed.  

Recall that, according to the Central Limit Theorem, the distribution of sample means is normal when:  

  1. The underlying population is normally distributed
  2. The sample size, n, is sufficiently large ($n<30$ for this class)

This chapter will introduce the concept of hypothesis testing and confidence intervals.  

# Review

When we know what the population standard deviation, $\sigma$, for individuals, we can calculate Z-scores and use the Standard Normal Distribution to calculate probabilities. Z has a standard normal distribution, meaning it has a mean of 0 and a standard deviation of 1.  

$$ z= \frac{\bar{x} - \mu}{\frac{\sigma}{\sqrt{n}}}$$

Though rare, there are situations where we might know the population standard deviation from published research or census data. For example,  Standardized test organizations publish population-level summaries which would allow us to test how our sample compares to the general population using the Z formula.



# Hypothesis Testing

## Testing for the True Mean Body Temperature

### Null and Alternative Hypotheses

It is commonly believed that the true mean body temperature is 98.6&deg; F (37&deg; C).  In science, a statement or claim such as this is called a **hypothesis**.  We can use data to test a hypothesis.  If there is enough evidence against a hypothesis, we reject it in favor of something else.

The claim representing the "status quo" or the commonly held belief or the usual value is called the **null hypothesis**.  In the case of body temperatures, our null hypothesis is defined by the belief that the true mean body temperature of healthy adults is 98.6&deg; F.

We present the null hypothesis in the following way:

$$H_0: \mu = 98.6$$

If we gather enough evidence against the null hypothesis and determine that it should be rejected, we need another hypothesis to propose in its place.  This is called the **alternative hypothesis**.  If 98.6&deg; F is not the correct body temperature, then it is logical to propose the following alternative hypothesis:

$$H_a: \mu \ne 98.6$$

To be brief in writing, we label the null hypothesis $H_0$. The zero in the subscript represents "null," "baseline," "default," "no effect," etc.  Similarly, we label the alternative hypothesis $H_a$.

Our alternative hypothesis could have been written as:

- $H_a: \mu \ne 98.6$  (two-sided hypothesis; two-tailed)
- $H_a: \mu < 98.6$    (one-sided hypothesis; left-tailed)
- $H_a: \mu > 98.6$    (one-sided hypothesis; right-tailed)

Notice that each of these is a viable alternative to the requirement that the mean is 98.6 degrees.  We would only use $<$ or $>$ if we had a belief in advance that the mean was less than or greater than 98.6.  If we do not have a strong reason to believe the mean is either smaller or larger than the stated value--before we collect our data--then, we use $\ne$ in our alternative hypothesis.

It is important that the null and alternative hypotheses be determined prior to collecting the data.  It is not appropriate to use the data from your study to choose the alternative hypothesis that will be used to test the same data!  This is an example of using data twice, once to choose the test and again to conduct the test.  It is okay to use data from a previous study to determine your null and alternative hypotheses, but it is an improper use of the statistical procedures to use the data to define *and* conduct a hypothesis test.

Notice that the null and alternative hypotheses are statements about a population parameter (e.g. $\mu$.)  They will never involve a sample statistic (e.g. $\bar{x}$.) Population parameters are unknown, and we are trying to make a judgement about whether or not they are equal to a particular value.  Sample statistics are calculated from our data, so there is no reason to do any test to assess what their value is.


The null hypothesis will always be a statement involving equality.  This gives us a starting point in our analysis.  In the reading for [Normal Distributions](Lesson05.html){target="_blank"}, we assumed that the true mean body temperature was 98.6&deg; F.  This becomes the assumed value of the mean of the distribution of the random variable $\bar{x}$.

The scientific method demands that we make a hypothesis (a claim or an educated guess).  We assume that the null hypothesis is true and gather evidence against it.  In other words, we "do research" (e.g., collect data) to gather evidence against that hypothesis.  If we can gather enough evidence to discredit our initial hypothesis, we conclude that it was false, and begin the process again.
If we are unable to reject the original hypothesis, we do not conclude that it is correct.  For example, we do not know that Einstein's Theory of Relativity is correct.  We have simply not been able to disprove it$\ldots$yet.

In the same way, we never can prove a null hypothesis is true.  We can only gather evidence against it.  If we get enough evidence, we reject the null hypothesis.

### Test Statistic

Assuming the null hypothesis is true, we assume that the true mean body temperature of healthy adults actually is $\mu = 98.6^{\circ}$ F.  Wanting to test this claim, researchers collected data on the mean body temperatures of $n=148$ healthy adults.  The mean of the observed values was $\bar{x} = 98.23^{\circ}$ F.

A histogram of the body temperature data shows a nice bell-shaped distribution.  Also, the sample mean, $\bar{x}=98.23^\circ$ F, appears to be reasonably close to the assumed population mean, $\mu=98.6^\circ$ F.
However, it is important to remember that the standard deviation of $\bar{x}$ is $\frac{\sigma}{\sqrt{n}} = \frac{0.675}{\sqrt{148}}=0.055$.  We need to determine how far $\bar{x}$ is from $\mu$.  Since we know $\sigma$ in this case, we can use the $z$-score to determine if $\bar{x}$ is far from the assumed value.  This is called the **test statistic**:

$$
z = \frac{\bar{x} - \mu}{\sigma / \sqrt{n}} = \frac{98.23 - 98.6}{0.675 / \sqrt{148}} = -6.6685
$$

Where $z$ is the test statistic, $\bar{x}$ is the sample mean, $\mu$ is the population mean, $\sigma$ is the standard deviation and $n$ is the sample size.
Based on the results from the formula above, $\bar{x}$ is over 6 standard deviations below the mean!

### $P$-value

#### Definition

The **$P$-value** is the probability of obtaining a test statistic (such as $z$) at least as extreme as the one you calculated, assuming the null hypothesis is true. In other words, our $P$-value is the probability that we would get a $z$-score that is as extreme or more extreme than $z=-6.6685$, assuming the true mean is 98.6&deg; F.

In advance of collecting the data, we may have had no idea whether the true mean would be greater than 98.6 or less than 98.6, if 98.6&deg; F was not the right value.  This is why we used a two-sided alternative hypothesis ($H_a: \mu \ne 98.6$.) When computing the $P$-value, we need to continue this logic.  So, we would have considered a value of $z=-6.6685$ to be equally as extreme as $z=6.6685$.

In the case of a two-sided test for one mean with $\sigma$ known, the $P$-value is the area in the tails beyond $\pm z$, in both the left and right tails of the standard normal distribution.

![](https://github.com/byuistats/Math221D_Course/raw/main/Images/Applet2584E-11-twosided-LoR.png)


When testing a null hypothesis, the $\mu$ in the z-score formula becomes the **hypothesized population mean**.  The $z$-score is then interpreted as the number of standard deviations away from the hypothesized mean.  If we know the population standard deviation, $\sigma$, then we can calculate the probability of getting a sample mean more extreme than the one we observed _if the null hypothesis is true_.  

__KEY DEFINITION__:  A __P-value__ is the probability of observing a test statistic as extreme, or more extreme than the one we observed in our sample, _if the null hypothesis is true_.  

We use "as or more extreme" because the direction (greater than or less than) depends on our alternative hypothesis.  In the above example, if I believe that my students scored higher than the general population average, I can say, there is only a 0.0011 chance of getting a test statistic higher than the one I observed if the null hypothesis is true.  Because that probability is very low, I am willing to reject the null hypothesis in favor of the alternative that my students scored higher, on average, than the general population.  


__EXAMPLE__:  A factory claims its light bulbs last, on average, $\mu = 800$ hours with a standard deviation, $\sigma = 40$.  

We randomly select 40 light bulbs and get an sample mean of 786.  Test to see if the life expectancy is actually less than that.  

State the Null and alternative Hypotheses:

$$ H_0: \mu = 800 $$

$$ H_a: \mu < 800 $$

Calculate the Z-score and find the P-value for the test:


```{r, eval=FALSE, echo=FALSE}

#hypothesized mean and population SD
mu_0 <- 800
sigma <- 40

# Sample: size, mean, z-score
xbar <- 786 
n <-  40
sigma_xbar <- sigma / sqrt(n)

z <- (xbar - mu_0) / sigma_xbar
z
# Prob sample average is lower than our xbar:
# More generally, the probability of getting a TEST STATISTIC less than the one we observed
pnorm(z)

# Prob that a sample average is greater than our xbar:
# More generally, the probability of getting a TEST STATISTIC greater than the one we observed
1-pnorm(z)



```

__Conclusion__:


## Confidence Interval

We can also calculate a confidence interval for the above example.  


Recall the formula for a confidence interval is, for a given $z^*$ associated with a desired level of confidence:

$$ CI = \bar{x} \pm z^*\frac{\sigma}{\sqrt{n}}$$

Recall the $z^*$ for common confidence levels:

```{r, fig.width = 9}
library(tidyverse)
library(pander)
tibble(`Conf. Level` = c(0.99, 0.95, 0.90), `Z*` = c(2.576, 1.96, 1.645)) %>% pander()
```




# Student's T-Distribution (sigma unkown)

In most cases, we perform statistical analyses on samples from a population where we don't know the population standard deviation, $\sigma$.  In this situation, we can no longer use the Standard Normal Distribution.  We use a distribution that is similar to the Normal distribution but whose shape depends on how much data we have. The Student's t-distribution requires a t-value (like a z-score) and something called degrees of freedom.  Degrees of Freedom for a t-distribution are defined:

$$df  = n-1 $$

We calculate test statistics very similarly, but we replace the population standard deviation with the sample standard deviation.  

$$ t = \frac{\bar{x} - \mu}{\frac{s}{\sqrt{n}}}$$

The T-distribution is very similar to the standard normal.  It is symmetrical around zero. However, the shape depends on how much data you have.  The more data you have, the closer the $t$ is to the standard normal.  This is illustrated in the graph below:

![](../Images/T_dist.png)

Because $t$ is not normally distributed, we can't use `pnorm()` to calculate probabilities.  However, R is fully capable of calculating probabilities for many probability distributions including the t-distribution.  We have to tell R the t-value *and* the degrees of freedom.


## Calculating P-values by hand with the T Distribution (One out of ten. Would NOT RECOMMENDED)

Suppose we have 25 test scores defined as "data" in the code chunk below.  We can calculate the $t$-statistic just like we did with the $z$-score.  

Suppose we believe that the mean score of these 25 students is significantly higher than 50.  Our null and alternative hypothesis are as follows: 

$$ H_0:  \mu = 50 $$
$$ H_a: \mu > 50 $$

```{r}
# we can use the t-distribution, pt(), just like pnorm() but must also add the degrees of freedom

data <- c(88,81,27,92,46,79,67,44,46,88,21,60,71,81,79,52,100,44,42,58,52,48,83,65,98)

# Hypothesized Mean:
mu_0 <- 50

# Sample size, sample Mean and sample SD
#  The length function tells us how many data points are in the list.  
n <- length(data)
xbar <- mean(data)
s <- sd(data)

s_xbar = s / sqrt(n)

t <- (xbar - mu_0) / s_xbar

# Probability of getting a test statistic at least as extreme as the one we observed if the null hypothesis is true
1-pt(t, n-1)


```

This means that if the true population mean was 50, then there is only a 0.00152 probability of observing a test statistic, t, as high as the one we got (__P-value__).  

The good news is that the more complicated the math becomes, the less of it we have to do! Instead of using R like a calculator to calculate $z$ or $t$-scores and calculating probabilities "by hand" (using `pnorm()` or `pt()`), we can use R functions with the data directly and get much more useful output.  

# The New Hotness

For situations where we want to make inference about a population from our sample data, we will be following these basic steps:

1. Read in the data
2. Review the data
3. Visualize the data
4. Perform the appropriate analysis

This will be true regardless of the type of data and analysis.  Let's begin by looking at a familiar dataset.  In the following example, we will focus on the Extroversion scores from Brother Cannon's Math 221 classes.  

## Step 1:  Read in the data

```{r}
# Load Libraries

library(tidyverse)
library(mosaic)
library(rio)
library(car)


# Read in data
big5 <- import('https://raw.githubusercontent.com/byuistats/Math221D_Cannon/master/Data/All_class_combined_personality_data.csv')


```

## Step 2:  Review the Data

In this step, we are looking to see if the data are as expected.  Are the columns we're interested in numeric? Categorical? and do these match expectations.  We can also start to look for strange data and outliers.  Visualizations can help.  

Other common issues to look for include:  negative numbers that should only be positive, date values that shouldn't exist, missing values, character variables inside what should be a number.  

In the real world, data are messy.  Reviewing the data is a critical part of an analysis.  

Take a look through the personality dataset and see if there are any anomalies that might need to be addressed.

```{r, echo = FALSE, include=FALSE}

#View(big5)
glimpse(big5)
#head(big5)

favstats(big5$Extroversion)

```


## Step 3:  Visulize the data

So far we have been discussing a single, quantitative variable of interest, like test scores, reaction times, heights, etc.  When we start looking at more complicated data, we will expand our repertoire of visualizations, but Histograms are very good when looking at one variable at a time, and boxplots are very good for comparison between groups.  

Create a histogram of Extroversion.  Describe some features of the data.  Is it symmetric? Skewed? Are there outliers?

```{r}

# Basic Graph
histogram(big5$Extroversion)

# Improved graph
histogram(big5$Extroversion, main = "Extroversion Scores", xlab = "Extroversion")


```


## Step 4:  Perform the appropriate analysis

In this example, we will be testing the hypothesis that Brother Cannon's students are similar to the general population.  We suspect that these youthful BYU-I students are, on average, more extroverted.

### Hypothesis Test

Write out the Null and alternative hypotheses.

$$ H_0: \mu_{extroversion} = 50$$ 

$$ H_A: \mu_{extroversion}  > 50$$

$$\alpha = 0.05$$

Perform the one-sample t-test using the "t.test()" function in R.  If you ever get stuck remembering how to use a function in R, you can run `?t.test` to see documentation. The question mark will open up the help files for any given function in R.  

The `t.test()` function takes as input, the data, the hypothesized mean, mu, and the direction of the alternative hypothesis.  

The default parameters for the `t.test()` function are:  `t.test(data, mu = 0, alternative = "two.sided")`.  

```{r}

#?t.test

# One-sided Hypothesis Test
t.test(big5$Extroversion, mu = 50, alternative = "greater")

```


__Question__:  What is the P-value?  
__Answer__:  


__Question__:  How do we explain our conclusion in context of our research question?  
__Answer__:  

__Technical Conclusion__: 

__Contextual Conclusion__: 

### Confidence Intervals

We can also use the `t.test()` function to create confidence intervals.  Recall that __confidence intervals are always 2-tailed__.  Confidence intervals are typically written in the form:  (lower limit, upper limit).  


```{r}
# Confidence Intervals are by definition 2-tailed
# We can also change the confidence level

t.test(big5$Extroversion, mu = 50, alternative = "two.sided", conf.level = .99)

```


To get only the output for the confidence interval to be shown, we can use a `$` to select specific output.  Much like when pulling a specific column from a dataset, the `$` can pull specific output from an analysis.  

Because the option for the `alternative =`  in the t.test function is "two.sided", we don't actually need to include it when getting confidence intervals.  

Also, confidence intervals do not require an assumed mu value.  So a more efficient way to get a confidence interval for a given set of data is:  


```{r}

t.test(big5$Extroversion, conf.level = .99)$conf.int


```

__Question__:  Describe in words the interpretation of the confidence interval in context of Extroversion.  
__Answer__:  


# Your Turn

## Body Temperature Data

The dataset below contains information about body temperatures of healthy adults. 

### Load the data:

```{r}

# These lines load the data into the data frame body_temp:

body_temp <- import("https://byuistats.github.io/M221R/Data/body_temp.xlsx")

```


### Review the Data

Create a table of summary statistics:  

```{r}

```


### Visualize the Data

Create a histogram to visualize the body temperature data. 

```{r}

```

__Question__:  Describe the general shape of the distribution.  
__Answer__:  


### Analyze the Data

It's widely accepted that normal body temperature for healthy adults is 98.6 degrees Fahrenheit.  

Suppose we suspect that the average temperature is different than 98.6

Use a significance level of $\alpha = 0.01$ to test whether the mean body temperature of healthy adults is equal to 98.6 degrees Fahrenheit.


```{r}

```

__Question__:  What is the P-value?  
__Answer__:  

Explain our conclusion.  

#### Confidence Interval

Create a 99% confidence interval for the true population average temperature of healthy adults.

```{r}

```

